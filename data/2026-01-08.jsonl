{"id": "2601.03390", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.03390", "abs": "https://arxiv.org/abs/2601.03390", "authors": ["Daniel Qian", "Xiyu Hao", "Jinkun Geng", "Yuncheng Yao", "Aurojit Panda", "Jinyang Li", "Anirudh Sivaraman"], "title": "Revisiting Speculative Leaderless Protocols for Low-Latency BFT Replication", "comment": null, "summary": "As Byzantine Fault Tolerant (BFT) protocols begin to be used in permissioned blockchains for user-facing applications such as payments, it is crucial that they provide low latency. In pursuit of low latency, some recently proposed BFT consensus protocols employ a leaderless optimistic fast path, in which clients broadcast their requests directly to replicas without first serializing requests at a leader, resulting in an end-to-end commit latency of 2 message delays ($2Δ$) during fault-free, synchronous periods. However, such a fast path only works if there is no contention: concurrent contending requests can cause replicas to diverge if they receive conflicting requests in different orders, triggering costly recovery procedures.\n  In this work, we present Aspen, a leaderless BFT protocol that achieves a near-optimal latency of $2Δ+ \\varepsilon$, where $\\varepsilon$ indicates a short waiting delay. Aspen removes the no-contention condition by utilizing a best-effort sequencing layer based on loosely synchronized clocks and network delay estimates. Aspen requires $n = 3f + 2p + 1$ replicas to cope with up to $f$ Byzantine nodes. The $2p$ extra nodes allow Aspen's fast path to proceed even if up to $p$ replicas diverge due to unpredictable network delays. When its optimistic conditions do not hold, Aspen falls back to PBFT-style protocol, guaranteeing safety and liveness under partial synchrony. In experiments with wide-area distributed replicas, Aspen commits requests in less than 75 ms, a 1.2 to 3.3$\\times$ improvement compared to previous protocols, while supporting 19,000 requests per second."}
{"id": "2601.03862", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.03862", "abs": "https://arxiv.org/abs/2601.03862", "authors": ["Francesco D'Amato", "Roberto Saltini", "Thanh-Hai Tran", "Yann Vonlanthen", "Luca Zanolini"], "title": "Majorum: Ebb-and-Flow Consensus with Dynamic Quorums", "comment": null, "summary": "Dynamic availability is the ability of a consensus protocol to remain live despite honest participants going offline and later rejoining. A well-known limitation is that dynamically available protocols, on their own, cannot provide strong safety guarantees during network partitions or extended asynchrony. Ebb-and-flow protocols [SP21] address this by combining a dynamically available protocol with a partially synchronous finality protocol that irrevocably finalizes a prefix.\n  We present Majorum, an ebb-and-flow construction whose dynamically available component builds on a quorum-based protocol (TOB-SVD). Under optimistic conditions, Majorum finalizes blocks in as few as three slots while requiring only a single voting phase per slot. In particular, when conditions remain favourable, each slot finalizes the next block extending the previously finalized one."}
{"id": "2601.03992", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03992", "abs": "https://arxiv.org/abs/2601.03992", "authors": ["Qi Wu", "Chao Fang", "Jiayuan Chen", "Ye Lin", "Yueqi Zhang", "Yichuan Bai", "Yuan Du", "Li Du"], "title": "A Scheduling Framework for Efficient MoE Inference on Edge GPU-NDP Systems", "comment": "To appear in 2026 Design, Automation and Test in Europe Conference (DATE 2026)", "summary": "Mixture-of-Experts (MoE) models facilitate edge deployment by decoupling model capacity from active computation, yet their large memory footprint drives the need for GPU systems with near-data processing (NDP) capabilities that offload experts to dedicated processing units. However, deploying MoE models on such edge-based GPU-NDP systems faces three critical challenges: 1) severe load imbalance across NDP units due to non-uniform expert selection and expert parallelism, 2) insufficient GPU utilization during expert computation within NDP units, and 3) extensive data pre-profiling necessitated by unpredictable expert activation patterns for pre-fetching. To address these challenges, this paper proposes an efficient inference framework featuring three key optimizations. First, the underexplored tensor parallelism in MoE inference is exploited to partition and compute large expert parameters across multiple NDP units simultaneously towards edge low-batch scenarios. Second, a load-balancing-aware scheduling algorithm distributes expert computations across NDP units and GPU to maximize resource utilization. Third, a dataset-free pre-fetching strategy proactively loads frequently accessed experts to minimize activation delays. Experimental results show that our framework enables GPU-NDP systems to achieve 2.41x on average and up to 2.56x speedup in end-to-end latency compared to state-of-the-art approaches, significantly enhancing MoE inference efficiency in resource-constrained environments."}
{"id": "2601.03271", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.03271", "abs": "https://arxiv.org/abs/2601.03271", "authors": ["Omar Garraoui"], "title": "Optimizing Exact String Matching via Statistical Anchoring", "comment": null, "summary": "In this work, we propose an enhancement to the Boyer-Moore-Horspool algorithm tailored for natural language text. The approach involves preprocessing the search pattern to identify its statistically least frequent character, referred to as the \"anchor.\" During the search, verification is first performed at this high-entropy position, allowing the algorithm to quickly discard non-matching windows. This fail-fast strategy reduces unnecessary comparisons, improving overall efficiency. Our implementation shows that incorporating basic linguistic statistics into classical pattern-matching techniques can boost performance without increasing complexity to the shift heuristics."}
{"id": "2601.04123", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.04123", "abs": "https://arxiv.org/abs/2601.04123", "authors": ["Francisco Ponce", "Simone Gazza", "Andrea D'Iapico", "Roberto Amadini", "Antonio Brogi", "Stefano Forti", "Saverio Giallorenzo", "Pierluigi Plebani", "Davide Usai", "Monica Vitali", "Gianluigi Zavattaro", "Jacopo Soldani"], "title": "Failure-Resilient and Carbon-Efficient Deployment of Microservices over the Cloud-Edge Continuum", "comment": "Submitted to Cluster Computing", "summary": "Deploying microservice-based applications (MSAs) on heterogeneous and dynamic Cloud-Edge infrastructures requires balancing conflicting objectives, such as failure resilience, performance, and environmental sustainability. In this article, we introduce the FREEDA toolchain, designed to automate the failure-resilient and carbon-efficient deployment of MSAs over the Cloud-Edge Continuum.\n  The FREEDA toolchain continuously adapts deployment configurations to changing operational conditions, resource availability, and sustainability constraints, aiming to maintain the MSA quality and service continuity while reducing carbon emissions. We also introduce an experimental suite using diverse simulated and emulated scenarios to validate the effectiveness of the toolchain against real-world challenges, including resource exhaustion, node failures, and carbon intensity fluctuations. The results demonstrate FREEDA's capability to autonomously reconfigure deployments by migrating services, adjusting flavour selections, or rebalancing workloads, successfully achieving an optimal balance among resilience, efficiency, and environmental impact."}
{"id": "2601.03768", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.03768", "abs": "https://arxiv.org/abs/2601.03768", "authors": ["Yichen Xu", "Martin Odersky"], "title": "Agentic Proof Automation: A Case Study", "comment": null, "summary": "Proof engineering is notoriously labor-intensive: proofs that are straightforward on paper often require lengthy scripts in theorem provers. Recent advances in large language models (LLMs) create new opportunities for proof automation: modern LLMs not only generate proof scripts, but also support agentic behavior, exploring codebases and iteratively refining their outputs against prover feedback. These advances enable an emerging scheme where LLM-based agents undertake most proof engineering under human guidance. Humans provide mathematical insight (definitions, theorems, proof strategies); agents handle the mechanical work of proof development. We call this scheme agentic proof automation. We present this scheme through a case study: mechanizing the semantic type soundness of a sophisticated formal system, System Capless, in Lean 4, comprising over 14,000 lines of code. Using off-the-shelf LLM agents with a single lightweight proof-checking tool, the agents completed 189 proof engineering tasks with an 87% success rate, only 16% requiring human intervention. The case study demonstrates that agents are capable proof engineers that substantially boost productivity, though they fall short in creative reasoning and still require human guidance in certain cases. We release an interactive explorer where readers can examine all agent interactions; the mechanization is open-sourced for experiments and extensions."}
{"id": "2601.03573", "categories": ["cs.DS", "cs.DB", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.03573", "abs": "https://arxiv.org/abs/2601.03573", "authors": ["Daniel Paul-Pena", "Vaishali Surianarayanan", "Deeparnab Chakrabarty", "C. Seshadhri"], "title": "Counting hypertriangles through hypergraph orientations", "comment": null, "summary": "Counting the number of small patterns is a central task in network analysis. While this problem is well studied for graphs, many real-world datasets are naturally modeled as hypergraphs, motivating the need for efficient hypergraph motif counting algorithms. In particular, we study the problem of counting hypertriangles - collections of three pairwise-intersecting hyperedges. These hypergraph patterns have a rich structure with multiple distinct intersection patterns unlike graph triangles.\n  Inspired by classical graph algorithms based on orientations and degeneracy, we develop a theoretical framework that generalizes these concepts to hypergraphs and yields provable algorithms for hypertriangle counting. We implement these ideas in DITCH (Degeneracy Inspired Triangle Counter for Hypergraphs) and show experimentally that it is 10-100x faster and more memory efficient than existing state-of-the-art methods."}
{"id": "2601.03836", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.03836", "abs": "https://arxiv.org/abs/2601.03836", "authors": ["Ivan Perez", "Angel Herranz"], "title": "Logic Programming with Extensible Types", "comment": "In Proceedings ICLP 2025, arXiv:2601.00047", "summary": "Logic programming languages present clear advantages in terms of declarativeness and conciseness. However, the ideas of logic programming have been met with resistance in other programming communities, and have not generally been adopted by other paradigms and languages. This paper proposes a novel way to incorporate logic programming in an existing codebase in a typed functional programming language. Our approach integrates with the host language without sacrificing static typing, and leverages strengths of typed functional programming such as polymorphism and higher-order. We do so by combining three ideas. First, we use the extensible types technique to allow values of the host language to contain logic variables. Second, we implement a unification algorithm that works for any data structure that supports certain operations.Third, we introduce a domain-specific language to define and query predicates. We demonstrate our proposal via a series of examples, and provide aids to make the notation convenient for users, showing that the proposed approach is not just technically possible but also practical. Our ideas have been implemented in the language Haskell with very good results."}
{"id": "2601.03643", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.03643", "abs": "https://arxiv.org/abs/2601.03643", "authors": ["Zeev Nutov"], "title": "On $k$-connectivity oracles in $k$-connected graphs", "comment": null, "summary": "A $k$-connectivity oracle for a graph $G=(V,E)$ is a data structure that given $s,t \\in V$ determines whether there are at least $k+1$ internally disjoint $st$-paths in $G$. For undirected graphs, Pettie, Saranurak & Yin [STOC 2022, pp. 151-161] proved that any $k$-connectivity oracle requires $Ω(kn)$ bits of space. They asked whether $Ω(kn)$ bits are still necessary if $G$ is $k$-connected. We will show by a very simple proof that this is so even if $G$ is $k$-connected, answering this open question."}
{"id": "2601.03854", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.03854", "abs": "https://arxiv.org/abs/2601.03854", "authors": ["Ziyi Yang", "George Pîrlea", "Ilya Sergey"], "title": "Inductive First-Order Formula Synthesis by ASP: A Case Study in Invariant Inference", "comment": "In Proceedings ICLP 2025, arXiv:2601.00047", "summary": "We present a framework for synthesising formulas in first-order logic (FOL) from examples, which unifies and advances state-of-the-art approaches for inference of transition system invariants. To do so, we study and categorise the existing methodologies, encoding techniques in their formula synthesis via answer set programming (ASP). Based on the derived categorisation, we propose orthogonal slices, a new technique for formula enumeration that partitions the search space into manageable chunks, enabling two approaches for incremental candidate pruning. Using a combination of existing techniques for first-order (FO) invariant synthesis and the orthogonal slices implemented in our framework FORCE, we significantly accelerate a state-of-the-art algorithm for distributed system invariant inference. We also show that our approach facilitates composition of different invariant inference frameworks, allowing for novel optimisations."}
{"id": "2601.03934", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.03934", "abs": "https://arxiv.org/abs/2601.03934", "authors": ["Matthias Bentert", "Esra Ceylan-Kettler", "Valentin Hübner", "Stefan Schmid", "Jiří Srba"], "title": "Complexity of Perfect and Ideal Resilience Verification in Fast Re-Route Networks", "comment": null, "summary": "To achieve fast recovery from link failures, most modern communication networks feature fully decentralized fast re-routing mechanisms. These re-routing mechanisms rely on pre-installed static re-routing rules at the nodes (the routers), which depend only on local failure information, namely on the failed links incident to the node. Ideally, a network is perfectly resilient: the re-routing rules ensure that packets are always successfully routed to their destinations as long as the source and the destination are still physically connected in the underlying network after the failures. Unfortunately, there are examples where achieving perfect resilience is not possible. Surprisingly, only very little is known about the algorithmic aspect of when and how perfect resilience can be achieved.\n  We investigate the computational complexity of analyzing such local fast re-routing mechanisms. Our main result is a negative one: we show that even checking whether a given set of static re-routing rules ensures perfect resilience is coNP-complete. We also show coNP-completeness of the so-called ideal resilience, a weaker notion of resilience often considered in the literature. Additionally, we investigate other fundamental variations of the problem. In particular, we show that our coNP-completeness proof also applies to scenarios where the re-routing rules have specific patterns (known as skipping in the literature).\n  On the positive side, for scenarios where nodes do not have information about the link from which a packet arrived (the so-called in-port), we present linear-time algorithms for both the verification and synthesis problem for perfect resilience."}
{"id": "2601.03897", "categories": ["cs.PL", "cs.DS"], "pdf": "https://arxiv.org/pdf/2601.03897", "abs": "https://arxiv.org/abs/2601.03897", "authors": ["Ziad Ismaili Alaoui", "Detlef Plump"], "title": "Implementing Binary Search Trees in GP 2 (Extended Abstract)", "comment": "In Proceedings GCM 2025, arXiv:2601.03249", "summary": "We present an approach to implement binary search trees in the rule-based graph programming language GP 2. Our implementation uses GP 2's rooted graph transformation rules to be fast and supports insertion, deletion and query operations. We argue that the worst-case runtime for each of the operations is O(n) for a tree with n nodes. In addition, we expect that, on average, the operations run in time O(log(n)). Hence the implementation would match the time complexity of binary search trees implementations in imperative languages."}
{"id": "2601.04169", "categories": ["cs.DS", "cs.CC", "cs.DM"], "pdf": "https://arxiv.org/pdf/2601.04169", "abs": "https://arxiv.org/abs/2601.04169", "authors": ["Thekla Hamm", "Sukanya Pandey", "Krisztina Szilágyi"], "title": "A Polynomial Kernel for Face Cover on Non-Embedded Planar Graphs", "comment": "Accepted to STACS 2026", "summary": "Given a planar graph, a subset of its vertices called terminals, and $k \\in \\mathbb{N}$, the Face Cover Number problem asks whether the terminals lie on the boundaries of at most $k$ faces of some embedding of the input graph. When a plane graph is given in the input, the problem is known to have a polynomial kernel~\\cite{GarneroST17}. In this paper, we present the first polynomial kernel for Face Cover Number when the input is a planar graph (without a fixed embedding). Our approach overcomes the challenge of not having a predefined set of face boundaries by building a kernel bottom-up on an SPR-tree while preserving the essential properties of the face cover along the way."}
{"id": "2601.03897", "categories": ["cs.PL", "cs.DS"], "pdf": "https://arxiv.org/pdf/2601.03897", "abs": "https://arxiv.org/abs/2601.03897", "authors": ["Ziad Ismaili Alaoui", "Detlef Plump"], "title": "Implementing Binary Search Trees in GP 2 (Extended Abstract)", "comment": "In Proceedings GCM 2025, arXiv:2601.03249", "summary": "We present an approach to implement binary search trees in the rule-based graph programming language GP 2. Our implementation uses GP 2's rooted graph transformation rules to be fast and supports insertion, deletion and query operations. We argue that the worst-case runtime for each of the operations is O(n) for a tree with n nodes. In addition, we expect that, on average, the operations run in time O(log(n)). Hence the implementation would match the time complexity of binary search trees implementations in imperative languages."}
