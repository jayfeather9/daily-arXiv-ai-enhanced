{"id": "2602.13434", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.13434", "abs": "https://arxiv.org/abs/2602.13434", "authors": ["Maccoy Merrell", "Daniel Puckett", "Gino Chacon", "Jeffrey Stuecheli", "Stavros Kalafatis", "Paul V. Gratz"], "title": "ORAP: Optimized Row Access Prefetching for Rowhammer-mitigated Memory", "comment": "15 pages, 19 figures", "summary": "Rowhammer is a well-studied DRAM phenomenon wherein multiple activations to a given row can cause bit flips in adjacent rows. Many mitigation techniques have been introduced to address Rowhammer, with some support being incorporated into the JEDEC DDR5 standard for per-row-activation-counter (PRAC) and refresh-management (RFM) systems. Mitigation schemes built on these mechanisms claim to have various levels of area, power, and performance overheads. To date the evaluation of existing mitigation schemes typically neglects the impact of other memory system components such as hardware prefetchers. Nearly all modern systems incorporate hardware prefetching and these can significantly improve processor performance through speculative cache population. These prefetchers induce higher numbers of downstream memory requests and increase DRAM activation rates. The performance overhead of Rowhammer mitigations are tied directly to memory access patterns, exposing both hardware prefetchers and Rowhammer mitigations to cross-interaction. We find that the performance improvement provided by prior-work hardware prefetchers is often severely impacted by Rowhammer mitigations. In effect, much of the benefit of speculative memory references from prefetching lies in accelerating and reordering DRAM references in ways that trigger mitigations, significantly reducing the benefits of prefetching. This work proposes the Optimized Row Access Prefetcher (ORAP), leveraging last-level-cache (LLC) space to cache large portions of DRAM rowbuffer contents to reduce the need for future activations. Working with the state-of-the-art Berti prefetcher, ORAP reduces DRAM activation rates by 51.3% and achieves a 4.6% speedup over the prefetcher configuration of Berti and SPP-PPF when prefetching in an RFM-mitigated memory system. Under PRAC mitigations, ORAP reduces energy overheads by 11.8%."}
{"id": "2602.13825", "categories": ["cs.AR", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.13825", "abs": "https://arxiv.org/abs/2602.13825", "authors": ["Paras Tiwari", "Narendra Singh Dhakad", "Shalu Rani", "Sanjay Kumar", "Themis Prodromakis"], "title": "Implementation and Performance Evaluation of CMOS-integrated Memristor-driven Flip-flop Circuits", "comment": null, "summary": "In this work, we report implementation and performance evaluation of memristor-driven fundamental logic gates, including NOT, AND, NAND, OR, NOR, and XOR, and novel and optimized design of the sequential logic circuits, such as D flip-flop, T-flip-flop, JK-flip-flop, and SR-flip-flop. The design, implementation, and optimization of these logic circuits were performed in SPECTRE in Cadence Virtuoso and integrated with 90 nm CMOS technology node. Additionally, we discuss an optimized design of memristor-driven logic gates and sequential logic circuits, and draw a comparative analysis with the other reported state-of-the-art work on sequential circuits. Moreover, the utilized memristor framework was experimentally pre-validated with the experimental data of Y2O3-based memristive devices, which shows significantly low values of variability during switching in both device-to-device (D2D) and cycle-to-cycle (C2C) operation. The performance metrics were calculated in terms of area, power, and delay of these sequential circuits and were found to be reduced by more than ~24%, 60%, and 58%, respectively, as compared to the other state-of-the-art work on sequential circuits. Therefore, the implemented memristor-based design significantly improves the performance of various logic designs, which makes it more area and power-efficient and shows the potential of memristor in designing various low-power, low-cost, ultrafast, and compact circuits."}
{"id": "2602.14262", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.14262", "abs": "https://arxiv.org/abs/2602.14262", "authors": ["Siddhartha Raman Sundara Raman", "Jaydeep P. Kulkarni"], "title": "ABI: A tightly integrated, unified, sparsity-aware, reconfigurable, compute near-register file/cache GPU architecture with light-weight softmax for deep learning, linear algebra, and Ising compute", "comment": null, "summary": "We present a tightly integrated and unified near-memory GPU architecture that delivers 6 to 16 times speedup and 6 to 13 times energy savings across Convolutional Neural Networks, Graph Convolutional Networks, Linear Programming, Large Language Models, and Ising workloads compared to MIAOW GPU. The design includes a custom sparsity-aware near-memory circuit providing about 1.5 times energy savings, and a lightweight softmax circuit providing about 1.6 times energy savings. The architecture supports reconfigurable compute up to INT16 with dynamic resolution updates and scales efficiently across problem sizes. ABI-enabled MI300 and Blackwell systems achieve about 4.5 times speedup over baseline MI300 and Blackwell."}
{"id": "2602.14393", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.14393", "abs": "https://arxiv.org/abs/2602.14393", "authors": ["Zongle Huang", "Hongyang Jia", "Kaiwei Zou", "Yongpan Liu"], "title": "Scope: A Scalable Merged Pipeline Framework for Multi-Chip-Module NN Accelerators", "comment": "Accepted in ASP-DAC 2026", "summary": "Neural network (NN) accelerators with multi-chip-module (MCM) architectures enable integration of massive computation capability; however, they face challenges of computing resource underutilization and off-chip communication overheads. Traditional parallelization schemes for NN inference on MCM architectures, such as intra-layer parallelism and inter-layer pipelining, show incompetency in breaking through both challenges, limiting the scalability of MCM architectures.\n  We observed that existing works typically deploy layers separately rather than considering them jointly. This underexploited dimension leads to compromises between system computation and communication, thus hindering optimal utilization, especially as hardware/software scale. To address this limitation, we propose Scope, a merged pipeline framework incorporating this overlooked multi-layer dimension, thereby achieving improved throughput and scalability by relaxing tradeoffs between computation, communication and memory costs. This new dimension, however, adds to the complexity of design space exploration (DSE). To tackle this, we develop a series of search algorithms that achieves exponential-to-linear complexity reduction, while identifying solutions that rank in the top 0.05% of performance. Experiments show that Scope achieves up to 1.73x throughput improvement while maintaining similar energy consumption for ResNet-152 inference compared to state-of-the-art approaches."}
{"id": "2602.14573", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2602.14573", "abs": "https://arxiv.org/abs/2602.14573", "authors": ["Marcel Moosbrugger", "Julian Müllner", "Ezio Bartocci", "Laura Kovács"], "title": "Polar: An Algebraic Analyzer for (Probabilistic) Loops", "comment": "Published in \"Principles of Verification: Cycling the Probabilistic Landscape\"", "summary": "We present the Polar framework for fully automating the analysis of classical and probabilistic loops using algebraic reasoning. The central theme in Polar comes with handling algebraic recurrences that precisely capture the loop semantics. To this end, our work implements a variety of techniques to compute exact closed-forms of recurrences over higher-order moments of variables, infer invariants, and derive loop sensitivities with respect to unknown parameters. Polar can analyze probabilistic loops containing if-statements, polynomial arithmetic, and common probability distributions. By translating loop analysis into linear recurrence solving, Polar uses the derived closed-forms of recurrences to compute the strongest polynomial invariant or to infer parameter sensitivity. Polar is both sound and complete within well-defined programming model restrictions. Lifting any of these restrictions results in significant hardness limits of computation. To overcome computational burdens for the sake of efficiency, Polar also provides incomplete but sound techniques to compute moments of combinations of variables."}
{"id": "2602.13789", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.13789", "abs": "https://arxiv.org/abs/2602.13789", "authors": ["Zhengyan Chu"], "title": "TEG: Exascale Cluster Governance via Non-Equilibrium Thermodynamics and Langevin Dynamics", "comment": "7 pages", "summary": "As cloud computing scales toward the Exascale regime ($10^5+$ nodes), the prevailing \"Newtonian\" orchestration paradigm -- exemplified by Kubernetes -- approaches fundamental physical limits. The centralized, deterministic scheduling model suffers from $O(N)$ latency scaling, \"Head-of-Line\" blocking, and thermodynamic blindness, rendering it incapable of managing the stochastic chaos of next-generation AI workloads. This paper proposes a paradigm shift from orchestration to Thermodynamic Governance. We model the compute cluster not as a static state machine, but as a Dissipative Structure far from equilibrium. We introduce TEG (Thermo-Economic Governor), a decentralized architecture that establishes a rigorous topological isomorphism between cluster resource contention and many-body physics. TEG replaces the global scheduler with Langevin Agents that execute Brownian motion on a Holographic Potential Field, reducing decision complexity to $O(1)$. System stability is maintained via a macro-scale Landau Phase Transition mechanism, which modulates global damping (taxation) to physically dissolve deadlocks. Crucially, we enforce Token Evaporation to mirror entropy dissipation, preventing economic inflation and ensuring an open thermodynamic system. We provide formal theoretical analysis proving that: (1) The system converges asymptotically to a Nash Equilibrium via Dual-Number Damping; (2) OOM catastrophic failures are converted into manageable Glassy States via an OS-level Airlock Mutex; and (3) Safety is mathematically guaranteed under high inertia using High-Order Control Barrier Functions (HOCBF). TEG demonstrates that emergent order, rather than deterministic control, is the necessary condition for Exascale scalability."}
{"id": "2602.13460", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.13460", "abs": "https://arxiv.org/abs/2602.13460", "authors": ["Michael Xie", "Jiayi Wu", "Dung Nguyen", "Aravind Srinivasan"], "title": "Differentially private graph coloring", "comment": null, "summary": "Differential Privacy is the gold standard in privacy-preserving data analysis. This paper addresses the challenge of producing a differentially edge-private vertex coloring. In this paper, we present two novel algorithms to approach this problem. Both algorithms initially randomly colors each vertex from a fixed size palette, then applies the exponential mechanism to locally resample colors for either all or a chosen subset of the vertices.\n  Any non-trivial differentially edge private coloring of graph needs to be defective. A coloring of a graph is k defective if all vertices of the graph share it's assigned color with at most k of its neighbors. This is the metric by which we will measure the utility of our algorithms. Our first algorithm applies to d-inductive graphs. Assume we have a d-inductive graph with n vertices and max degree $Δ$. We show that our algorithm provides a \\(3ε\\)-differentially private coloring with \\(O(\\frac{\\log n}ε+d)\\) max defectiveness, given a palette of size $Θ(\\fracΔ{\\log n}+\\frac{1}ε)$ Furthermore, we show that this algorithm can generalize to $O(\\fracΔ{cε}+d)$ defectiveness, where c is the size of the palette and $c=O(\\fracΔ{\\log n})$. Our second algorithm utilizes noisy thresholding to guarantee \\(O(\\frac{\\log n}ε)\\) max defectiveness, given a palette of size $Θ(\\fracΔ{\\log n}+\\frac{1}ε)$, generalizing to all graphs rather than just d-inductive ones."}
{"id": "2602.14717", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2602.14717", "abs": "https://arxiv.org/abs/2602.14717", "authors": ["Stephen Mell", "Steve Zdancewic", "Osbert Bastani"], "title": "Optimal Program Synthesis via Abstract Interpretation", "comment": null, "summary": "We consider the problem of synthesizing programs with numerical constants that optimize a quantitative objective, such as accuracy, over a set of input-output examples. We propose a general framework for optimal synthesis of such programs in a given domain specific language (DSL), with provable optimality guarantees. Our framework enumerates programs in a general search graph, where nodes represent subsets of concrete programs. To improve scalability, it uses A* search in conjunction with a search heuristic based on abstract interpretation; intuitively, this heuristic establishes upper bounds on the value of subtrees in the search graph, enabling the synthesizer to identify and prune subtrees that are provably suboptimal. In addition, we propose a natural strategy for constructing abstract transformers for monotonic semantics, which is a common property for components in DSLs for data classification. Finally, we implement our approach in the context of two such existing DSLs, demonstrating that our algorithm is more scalable than existing optimal synthesizers."}
{"id": "2602.14107", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.14107", "abs": "https://arxiv.org/abs/2602.14107", "authors": ["Yuze Liu", "Shibo Chu", "Tiehua Zhang", "Hao Zhou", "Zhishu Shen", "Jinze Wang", "Jianzhong Qi", "Feng Xia"], "title": "ML-ECS: A Collaborative Multimodal Learning Framework for Edge-Cloud Synergies", "comment": null, "summary": "Edge-cloud synergies provide a promising paradigm for privacy-preserving deployment of foundation models, where lightweight on-device models adapt to domain-specific data and cloud-hosted models coordinate knowledge sharing. However, in real-world edge environments, collaborative multimodal learning is challenged by modality heterogeneity (different modality combinations across domains) and model-structure heterogeneity (different modality-specific encoders/fusion modules. To address these issues, we propose ML-ECS, a collaborative multimodal learning framework that enables joint training between a server-based model and heterogeneous edge models. This framework consists of four components: (1) cross-modal contrastive learning (CCL) to align modality representations in a shared latent space, (2) adaptive multimodal tuning (AMT) to preserve domain-specific knowledge from local datasets, (3) modality-aware model aggregation (MMA) to robustly aggregate while mitigating noise caused by missing modalities, and (4) SLM-enhanced CCL (SE-CCL) to facilitate bidirectional knowledge transfer between cloud and edge. Experimental results on various multimodal tasks show that \\pname consistently outperform state-of-the-art baselines under varying modality availability, achieving improvements of 5.44% to 12.08% in Rouge-LSum and improving both client- and server-side performance. In addition, by communicating only low-rank LoRA parameters and fused representations, ML-ECS achieves high communication efficiency, requiring only 0.65% of the total parameter volume."}
{"id": "2602.13461", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.13461", "abs": "https://arxiv.org/abs/2602.13461", "authors": ["Paola Bonizzoni", "Davide Cozzi", "Younan Gao"], "title": "Optimal-Time Mapping in Run-Length Compressed PBWT", "comment": "To appear in CPM2026 (r1)", "summary": "The Positional Burrows--Wheeler Transform (PBWT) is a data structure designed for efficiently representing and querying large collections of sequences, such as haplotype panels in genomics. Forward and backward stepping operations -- analogues to LF- and FL-mapping in the traditional BWT -- are fundamental to the PBWT, underpinning many algorithms based on the PBWT for haplotype matching and related analyses. Although the run-length encoded variant of the PBWT (also known as the $μ$-PBWT) achieves $O(\\newR)$-word space usage, where $\\newR$ is the total number of runs, no data structure supporting both forward and backward stepping in constant time within this space bound was previously known. In this paper, we consider the multi-allelic PBWT that is extended from its original binary form to a general ordered alphabet $\\{0, \\dots, σ-1\\}$. We first establish bounds on the size $\\newR$ and then introduce a new $O(\\newR)$-word data structure built over a list of haplotypes $\\{S_1, \\dots, S_\\height\\}$, each of length $\\width$, that supports constant-time forward and backward stepping.\n  We further revisit two key applications -- haplotype retrieval and prefix search -- leveraging our efficient forward stepping technique. Specifically, we design an $O(\\newR)$-word space data structure that supports haplotype retrieval in $O(\\log \\log_{\\word} h + \\width)$ time. For prefix search, we present an $O(\\height + \\newR)$-word data structure that answers queries in $O(m' \\log\\log_{\\word} σ+ \\occ)$ time, where $m'$ denotes the length of the longest common prefix returned and $\\occ$ denotes the number of haplotypes prefixed the longest prefix."}
{"id": "2602.14302", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14302", "abs": "https://arxiv.org/abs/2602.14302", "authors": ["Chunlin Tian", "Kahou Tam", "Yebo Wu", "Shuaihang Zhong", "Li Li", "Nicholas D. Lane", "Chengzhong Xu"], "title": "Floe: Federated Specialization for Real-Time LLM-SLM Inference", "comment": "Accepted by IEEE Transactions on Parallel and Distributed Systems", "summary": "Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments. Floe combines a cloud-based black-box LLM with lightweight small language models (SLMs) on edge devices to enable low-latency, privacy-preserving inference. Personal data and fine-tuning remain on-device, while the cloud LLM contributes general knowledge without exposing proprietary weights. A heterogeneity-aware LoRA adaptation strategy enables efficient edge deployment across diverse hardware, and a logit-level fusion mechanism enables real-time coordination between edge and cloud models. Extensive experiments demonstrate that Floe enhances user privacy and personalization. Moreover, it significantly improves model performance and reduces inference latency on edge devices under real-time constraints compared with baseline approaches."}
{"id": "2602.13484", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.13484", "abs": "https://arxiv.org/abs/2602.13484", "authors": ["Diandre Miguel Sabale", "Wolfgang Gatterbauer", "Prashant Pandey"], "title": "How to Train Your Filter: Should You Learn, Stack or Adapt?", "comment": null, "summary": "Filters are ubiquitous in computer science, enabling space-efficient approximate membership testing. Since Bloom filters were introduced in 1970, decades of work improved their space efficiency and performance. Recently, three new paradigms have emerged offering orders-of-magnitude improvements in false positive rates (FPRs) by using information beyond the input set: (1) learned filters train a model to distinguish (non)members, (2) stacked filters use negative workload samples to build cascading layers, and (3) adaptive filters update internal representation in response to false positive feedback. Yet each paradigm targets specific use cases, introduces complex configuration tuning, and has been evaluated in isolation. This results in unclear trade-offs and a gap in understanding how these approaches compare and when each is most appropriate. This paper presents the first comprehensive evaluation of learned, stacked, and adaptive filters across real-world datasets and query workloads. Our results reveal critical trade-offs: (1) Learned filters achieve up to 10^2 times lower FPRs but exhibit high variance and lack robustness under skewed or dynamic workloads. Critically, model inference overhead leads to up to 10^4 times slower query latencies than stacked or adaptive filters. (2) Stacked filters reliably achieve up to 10^3 times lower FPRs on skewed workloads but require workload knowledge. (3) Adaptive filters are robust across settings, achieving up to 10^3 times lower FPRs under adversarial queries without workload assumptions. Based on our analysis, learned filters suit stable workloads where input features enable effective model training and space constraints are paramount, stacked filters excel when reliable query distributions are known, and adaptive filters are most generalizable, providing robust theoretically bound guarantees even in dynamic or adversarial environments."}
{"id": "2602.14516", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.14516", "abs": "https://arxiv.org/abs/2602.14516", "authors": ["Wenhao He", "Youhe Jiang", "Penghao Zhao", "Quanqing Xu", "Eiko Yoneki", "Bin Cui", "Fangcheng Fu"], "title": "Efficient Multi-round LLM Inference over Disaggregated Serving", "comment": null, "summary": "With the rapid evolution of Large Language Models (LLMs), multi-round workflows, such as autonomous agents and iterative retrieval, have become increasingly prevalent. However, this raises hurdles for serving LLMs under prefill-decode (PD) disaggregation, a widely adopted paradigm that separates the compute-bound prefill phase and memory-bound decode phase onto individual resources. Specifically, existing systems overlook the interleaved prefill-decode workload pattern in multi-round inference, leading to sub-optimal handling of the incremental prefill workloads and model deployment for the two phases.\n  In this work, we present AMPD, a brand new disaggregated serving framework for multi-round LLM inference. The core of AMPD is to coordinate the prefill workloads based on real-time workloads by adaptively determining where to carry out these workloads and how they are scheduled, in order to maximize service level objective (SLO) attainment. In addition, we tailor a planning algorithm for our scenario, facilitating the deduction of optimal resource allocation and parallel strategies for the two phases. Empirical results demonstrate that AMPD substantially improves SLO attainment compared to state-of-the-art baselines."}
{"id": "2602.13610", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.13610", "abs": "https://arxiv.org/abs/2602.13610", "authors": ["Tianshuo Zhou", "David H. Mathews", "Liang Huang"], "title": "Probabilistic RNA Designability via Interpretable Ensemble Approximation and Dynamic Decomposition", "comment": null, "summary": "Motivation: RNA design aims to find RNA sequences that fold into a given target secondary structure, a problem also known as RNA inverse folding. However, not all target structures are designable. Recent advances in RNA designability have focused primarily on minimum free energy (MFE)-based criteria, while ensemble-based notions of designability remain largely underexplored. To address this gap, we introduce a theory of ensemble approximation and a probability decomposition framework for bounding the folding probabilities of RNA structures in an explainable way. We further develop a linear-time dynamic programming algorithm that efficiently searches over exponentially many decompositions and identifies the optimal one that yields the tightest probabilistic bound for a given structure. Results: Applying our methods to both native and artificial RNA structures in the ArchiveII and Eterna100 benchmarks, we obtained probability bounds that are much tighter than prior approaches. In addition, our methods further provide anatomical tools for analyzing RNA structures and understanding the sources of design difficulty at the motif level. Availability: Source code and data are available at https://github.com/shanry/RNA-Undesign. Supplementary information: Supplementary text and data are available in a separate PDF."}
{"id": "2602.14704", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.14704", "abs": "https://arxiv.org/abs/2602.14704", "authors": ["Zong Yu Lee", "Xueyan Tang"], "title": "Evaluation of Dynamic Vector Bin Packing for Virtual Machine Placement", "comment": "Extended version of a paper that will appear in IEEE IPDPS 2026 conference", "summary": "Virtual machine placement is a crucial challenge in cloud computing for efficiently utilizing physical machine resources in data centers. Virtual machine placement can be formulated as a MinUsageTime Dynamic Vector Bin Packing (DVBP) problem, aiming to minimize the total usage time of the physical machines. This paper evaluates state-of-the-art MinUsageTime DVBP algorithms in non-clairvoyant, clairvoyant and learning-augmented online settings, where item durations (virtual machine lifetimes) are unknown, known and predicted, respectively. Besides the algorithms taken from the literature, we also develop several new algorithms or enhancements. Empirical experimentation is carried out with real-world datasets of Microsoft Azure. The insights from the experimental results are discussed to explore the structures of algorithms and promising design elements that work well in practice."}
{"id": "2602.13735", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.13735", "abs": "https://arxiv.org/abs/2602.13735", "authors": ["Dmitry Kosolobov"], "title": "Compressed Index with Construction in Compressed Space", "comment": "30 pages, 5 figures", "summary": "Suppose that we are given a string $s$ of length $n$ over an alphabet $\\{0,1,\\ldots,n^{O(1)}\\}$ and $δ$ is a compression measure for $s$ called string complexity. We describe an index on $s$ with $O(δ\\log\\frac{n}δ)$ space, measured in $O(\\log n)$-bit machine words, that can search in $s$ any string of length $m$ in $O(m + (\\mathrm{occ} + 1)\\log^εn)$ time, where $\\mathrm{occ}$ is the number of found occurrences and $ε> 0$ is any fixed constant (the big-O in the space bound hides factor $\\frac{1}ε$). Crucially, the index can be built within this space in $O(n\\log n)$ expected time by one left-to-right pass on the string $s$ in a streaming fashion. The index does not use the Karp--Rabin fingerprints, and the randomization in the construction time can be eliminated by using deterministic dictionaries instead of hash tables (with a slowdown). The search time matches currently best results and the space is almost optimal (the known optimum is $O(δ\\log\\frac{n}{δα})$, where $α= \\log_σn$ and $σ$ is the alphabet size, and it coincides with $O(δ\\log\\frac{n}δ)$ when $δ= O(n / α^2)$). This is the first index that can be constructed within such space and with such time guarantees. To avoid uninteresting marginal cases, all above bounds are stated for $δ\\ge Ω(\\log\\log n)$."}
{"id": "2602.13756", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.13756", "abs": "https://arxiv.org/abs/2602.13756", "authors": ["Yota Otachi"], "title": "Spanning tree congestion of proper interval graphs", "comment": "10 pages, 2 figures", "summary": "We show that the spanning tree congestion problem is NP-complete even for proper interval graphs of linear clique-width at most 4."}
{"id": "2602.13861", "categories": ["cs.DS", "math.CO", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.13861", "abs": "https://arxiv.org/abs/2602.13861", "authors": ["Hans Raj Tiwary", "Petr Kolman"], "title": "Min-Max Connected Multiway Cut", "comment": null, "summary": "We introduce a variant of the multiway cut that we call the min-max connected multiway cut. Given a graph $G=(V,E)$ and a set $Γ\\subseteq V$ of $t$ terminals, partition $V$ into $t$ parts such that each part is connected and contains exactly one terminal; the objective is to minimize the maximum weight of the edges leaving any part of the partition. This problem is a natural modification of the standard multiway cut problem and it differs from it in two ways: first, the cost of a partition is defined to be the maximum size of the boundary of any part, as opposed to the sum of all boundaries, and second, the subgraph induced by each part is required to be connected. Although the modified objective function has been considered before in the literature under the name min-max multiway cut, the requirement on each component to be connected has not been studied as far as we know.\n  We show various hardness results for this problem, including a proof of weak NP-hardness of the weighted version of the problem on graphs with tree-width two, and provide a pseudopolynomial time algorithm as well as an FPTAS for the weighted problem on trees. As a consequence of our investigation we also show that the (unconstrained) min-max multiway cut problem is NP-hard even for three terminals, strengthening the known results."}
{"id": "2602.13981", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.13981", "abs": "https://arxiv.org/abs/2602.13981", "authors": ["Huairui Chu", "Yuxi Liu", "Daniel Lokshtanov", "Junqiang Peng", "Kangyi Tian", "Mingyu Xiao"], "title": "Faster Parameterized Vertex Multicut", "comment": null, "summary": "In the {\\sc Vertex Multicut} problem the input consists of a graph $G$, integer $k$, and a set $\\mathbf{T} = \\{(s_1, t_1), \\ldots, (s_p, t_p)\\}$ of pairs of vertices of $G$. The task is to find a set $X$ of at most $k$ vertices such that, for every $(s_i, t_i) \\in \\mathbf{T}$, there is no path from $s_i$ to $t_i$ in $G - X$. Marx and Razgon [STOC 2011 and SICOMP 2014] and Bousquet, Daligault, and Thomassé [STOC 2011 and SICOMP 2018] independently and simultaneously gave the first algorithms for {\\sc Vertex Multicut} with running time $f(k)n^{O(1)}$. The running time of their algorithms is $2^{O(k^3)}n^{O(1)}$ and $2^{O(k^{O(1)})}n^{O(1)}$, respectively. As part of their result, Marx and Razgon introduce the {\\em shadow removal} technique, which was subsequently applied in algorithms for several parameterized cut and separation problems. The shadow removal step is the only step of the algorithm of Marx and Razgon which requires $2^{O(k^3)}n^{O(1)}$ time. Chitnis et al. [TALG 2015] gave an improved version of the shadow removal step, which, among other results, led to a $k^{O(k^2)}n^{O(1)}$ time algorithm for {\\sc Vertex Multicut}.\n  We give a faster algorithm for the {\\sc Vertex Multicut} problem with running time $k^{O(k)}n^{O(1)}$. Our main technical contribution is a refined shadow removal step for vertex separation problems that only introduces an overhead of $k^{O(k)}\\log n$ time. The new shadow removal step implies a $k^{O(k^2)}n^{O(1)}$ time algorithm for {\\sc Directed Subset Feedback Vertex Set} and a $k^{O(k)}n^{O(1)}$ time algorithm for {\\sc Directed Multiway Cut}, improving over the previously best known algorithms of Chitnis et al. [TALG 2015]."}
{"id": "2602.14084", "categories": ["cs.DS", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.14084", "abs": "https://arxiv.org/abs/2602.14084", "authors": ["Alexander Zhou", "Haoyang Li", "Anxin Tian", "Zhiyuan Li", "Yue Wang"], "title": "Counting Balanced Triangles on Social Networks With Uncertain Edge Signs", "comment": null, "summary": "On signed social networks, balanced and unbalanced triangles are a critical motif due to their role as the foundations of Structural Balance Theory. The uses for these motifs have been extensively explored in networks with known edge signs, however in the real-world graphs with ground-truth signs are near non-existent, particularly on a large-scale. In reality, edge signs are inferred via various techniques with differing levels of confidence, meaning the edge signs on these graphs should be modelled with a probability value. In this work, we adapt balanced and unbalanced triangles to a setting with uncertain edge signs and explore the problems of triangle counting and enumeration. We provide a baseline and improved method (leveraging the inherent information provided by the edge probabilities in order to reduce the search space) for fast exact counting and enumeration. We also explore approximate solutions for counting via different sampling approaches, including leveraging insights from our improved exact solution to significantly reduce the runtime of each sample resulting in upwards of two magnitudes more queries executed per second. We evaluate the efficiency of all our solutions as well as examine the effectiveness of our sampling approaches on real-world topological networks with a variety of probability distributions."}
{"id": "2602.14320", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.14320", "abs": "https://arxiv.org/abs/2602.14320", "authors": ["Alexandra Henzinger", "Edward Pyne", "Seyoon Ragavan"], "title": "Catalytic Tree Evaluation From Matching Vectors", "comment": null, "summary": "We give new algorithms for tree evaluation (S. Cook et al. TOCT 2012) in the catalytic-computing model (Buhrman et al. STOC 2014). Two existing approaches aim to solve tree evaluation (TreeEval) in low space: on the one hand, J. Cook and Mertz (STOC 2024) give an algorithm for TreeEval running in super-logarithmic space $O(\\log n\\log\\log n)$ and super-polynomial time $n^{O(\\log\\log n)}$. On the other hand, a simple reduction from TreeEval to circuit evaluation, combined with the result of Buhrman et al. (STOC 2014), gives a catalytic algorithm for TreeEval running in logarithmic $O(\\log n)$ free space and polynomial time, but with polynomial catalytic space.\n  We show that the latter result can be improved. We give a catalytic algorithm for TreeEval with logarithmic $O(\\log n)$ free space, polynomial runtime, and subpolynomial $2^{\\log^εn}$ catalytic space (for any $ε> 0$). Our result gives the first natural problem known to be solvable with logarithmic free space and even $n^{1-ε}$ catalytic space, that is not known to be in standard logspace even under assumptions. Our result immediately implies an improved simulation of time by catalytic space, by the reduction of Williams (STOC 2025).\n  Our catalytic TreeEval algorithm is inspired by a connection to matching vector families and private information retrieval, and improved constructions of (uniform) matching vector families would imply improvements to our algorithm."}
{"id": "2602.14326", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.14326", "abs": "https://arxiv.org/abs/2602.14326", "authors": ["Vihan Shah"], "title": "Sublinear-Time Lower Bounds for Approximating Matching Size using Non-Adaptive Queries", "comment": "51 pages, 6 figures. Published in SODA 2026", "summary": "We study the problem of estimating the size of the maximum matching in the sublinear-time setting. This problem has been extensively studied, with several known upper and lower bounds. A notable result by Behnezhad (FOCS 2021) established a 2-approximation in ~O(n) time.\n  However, all known upper and lower bounds are in the adaptive query model, where each query can depend on previous answers. In contrast, non-adaptive query models-where the distribution over all queries must be fixed in advance-are widely studied in property testing, often revealing fundamental gaps between adaptive and non-adaptive complexities. This raises the natural question: is adaptivity also necessary for approximating the maximum matching size in sublinear time? This motivates the goal of achieving a constant or even a polylogarithmic approximation using ~O(n) non-adaptive adjacency list queries, similar to what was done by Behnezhad using adaptive queries.\n  We show that this is not possible by proving that any randomized non-adaptive algorithm achieving an n^{1/3 - gamma}-approximation, for any constant gamma > 0, with probability at least 2/3, must make Omega(n^{1 + eps}) adjacency list queries, for some constant eps > 0 depending on gamma. This result highlights the necessity of adaptivity in achieving strong approximations. However, non-trivial upper bounds are still achievable: we present a simple randomized algorithm that achieves an n^{1/2}-approximation in O(n log^2 n) queries.\n  Moreover, our lower bound also extends to the newly defined variant of the non-adaptive model, where queries are issued according to a fixed query tree, introduced by Azarmehr, Behnezhad, Ghafari, and Sudan (FOCS 2025) in the context of Local Computation Algorithms."}
{"id": "2602.14385", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.14385", "abs": "https://arxiv.org/abs/2602.14385", "authors": ["Hideo Bannai", "Yuto Fujie", "Peaker Guo", "Shunsuke Inenaga", "Yuto Nakashima", "Simon J. Puglisi", "Cristian Urbina"], "title": "Sensitivity of Repetitiveness Measures to String Reversal", "comment": null, "summary": "We study the impact that string reversal can have on several repetitiveness measures. First, we exhibit an infinite family of strings where the number, $r$, of runs in the run-length encoding of the Burrows--Wheeler transform (BWT) can increase additively by $Θ(n)$ when reversing the string. This substantially improves the known $Ω(\\log n)$ lower-bound for the additive sensitivity of $r$ and it is asymptotically tight. We generalize our result to other variants of the BWT, including the variant with an appended end-of-string symbol and the bijective BWT. We show that an analogous result holds for the size $z$ of the Lempel--Ziv 77 (LZ) parsing of the text, and also for some of its variants, including the non-overlapping LZ parsing, and the LZ-end parsing. Moreover, we describe a family of strings for which the ratio $z(w^R)/z(w)$ approaches $3$ from below as $|w|\\rightarrow \\infty$. We also show an asymptotically tight lower-bound of $Θ(n)$ for the additive sensitivity of the size $v$ of the smallest lexicographic parsing to string reversal. Finally, we show that the multiplicative sensitivity of $v$ to reversing the string is $Θ(\\log n)$, and this lower-bound is also tight. Overall, our results expose the limitations of repetitiveness measures that are widely used in practice, against string reversal -- a simple and natural data transformation."}
{"id": "2602.14550", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.14550", "abs": "https://arxiv.org/abs/2602.14550", "authors": ["Yotam Kenneth-Mordoch"], "title": "Faster Pseudo-Deterministic Minimum Cut", "comment": null, "summary": "Pseudo-deterministic algorithms are randomized algorithms that, with high constant probability, output a fixed canonical solution. The study of pseudo-deterministic algorithms for the global minimum cut problem was recently initiated by Agarwala and Varma [ITCS'26], who gave a black-box reduction incurring an $O(\\log n \\log \\log n)$ overhead. We introduce a natural graph-theoretic tie-breaking mechanism that uniquely selects a canonical minimum cut. Using this mechanism, we obtain: (i) A pseudo-deterministic minimum cut algorithm for weighted graphs running in $O(m\\log^2 n)$ time, eliminating the $O(\\log n \\log \\log n)$ overhead of prior work and matching existing randomized algorithms. (ii) The first pseudo-deterministic algorithm for maintaining a canonical minimum cut in a fully-dynamic unweighted graph, with $\\mathrm{polylog}(n)$ update time and $\\tilde{O}(n)$ query time. (iii) Improved pseudo-deterministic algorithms for unweighted graphs in the dynamic streaming and cut-query models of computation, matching the best randomized algorithms."}
{"id": "2602.14625", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.14625", "abs": "https://arxiv.org/abs/2602.14625", "authors": ["Jan Dreier", "Clemens Kuske"], "title": "Near-Linear Time Computation of Welzl Orders on Graphs with Linear Neighborhood Complexity", "comment": null, "summary": "Orders with low crossing number, introduced by Welzl, are a fundamental tool in range searching and computational geometry. Recently, they have found important applications in structural graph theory: set systems with linear shatter functions correspond to graph classes with linear neighborhood complexity. For such systems, Welzl's theorem guarantees the existence of orders with only $\\mathcal{O}(\\log^2 n)$ crossings. A series of works has progressively improved the runtime for computing such orders, from Chazelle and Welzl's original $\\mathcal{O}(|U|^3 |\\mathcal{F}|)$ bound, through Har-Peled's $\\mathcal{O}(|U|^2|\\mathcal{F}|)$, to the recent sampling-based methods of Csikós and Mustafa.\n  We present a randomized algorithm that computes Welzl orders for set systems with linear primal and dual shatter functions in time $\\mathcal{O}(\\|S\\| \\log \\|S\\|)$, where $\\|S\\| = |U| + \\sum_{X \\in \\mathcal{F}} |X|$ is the size of the canonical input representation. As an application, we compute compact neighborhood covers in graph classes with (near-)linear neighborhood complexity in time \\(\\mathcal{O}(n \\log n)\\) and improve the runtime of first-order model checking on monadically stable graph classes from $\\mathcal{O}(n^{5+\\varepsilon})$ to $\\mathcal{O}(n^{3+\\varepsilon})$."}
{"id": "2602.14768", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2602.14768", "abs": "https://arxiv.org/abs/2602.14768", "authors": ["Susobhan Bandopadhyay", "Aritra Banik", "Diptapriyo Majumdar", "Abhishek Sahu"], "title": "On the Parameterized Tractability of Packing Vertex-Disjoint A-Paths with Length Constraints", "comment": null, "summary": "Given an undirected graph G and a set A \\subseteq V(G), an A-path is a path in G that starts and ends at two distinct vertices of A with intermediate vertices in V(G) \\setminus A. An A-path is called an (A,\\ell)-path if the length of the path is exactly \\ell. In the {\\sc (A, \\ell)-Path Packing} problem (ALPP), we seek to determine whether there exist k vertex-disjoint (A, \\ell)-paths in G or not. We pursue this problem with respect to structural parameters. We prove that ALPP is W[1]-hard when it is parameterized by the combined parameter distance to path (dtp) and |A|. In addition, we consider the combined parameters distance to cluster (cvd) + |A| and distance to cluster (cvd) + \\ell. For both these combined parameters, we provide FPT algorithms. Finally, we consider the vertex cover number (vc) as the parameter and provide a kernel with O(vc^2) vertices."}
{"id": "2602.15015", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.15015", "abs": "https://arxiv.org/abs/2602.15015", "authors": ["Nikhil Bansal", "Arun Jambulapati", "Thatchaphol Saranurak"], "title": "Expander Decomposition with Almost Optimal Overhead", "comment": null, "summary": "We present the first polynomial-time algorithm for computing a near-optimal \\emph{flow}-expander decomposition. Given a graph $G$ and a parameter $φ$, our algorithm removes at most a $φ\\log^{1+o(1)}n$ fraction of edges so that every remaining connected component is a $φ$-\\emph{flow}-expander (a stronger guarantee than being a $φ$-\\emph{cut}-expander). This achieves overhead $\\log^{1+o(1)}n$, nearly matching the $Ω(\\log n)$ graph-theoretic lower bound that already holds for cut-expander decompositions, up to a $\\log^{o(1)}n$ factor. Prior polynomial-time algorithms required removing $O(φ\\log^{1.5}n)$ and $O(φ\\log^{2}n)$ fractions of edges to guarantee $φ$-cut-expander and $φ$-flow-expander components, respectively."}
{"id": "2602.14704", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.14704", "abs": "https://arxiv.org/abs/2602.14704", "authors": ["Zong Yu Lee", "Xueyan Tang"], "title": "Evaluation of Dynamic Vector Bin Packing for Virtual Machine Placement", "comment": "Extended version of a paper that will appear in IEEE IPDPS 2026 conference", "summary": "Virtual machine placement is a crucial challenge in cloud computing for efficiently utilizing physical machine resources in data centers. Virtual machine placement can be formulated as a MinUsageTime Dynamic Vector Bin Packing (DVBP) problem, aiming to minimize the total usage time of the physical machines. This paper evaluates state-of-the-art MinUsageTime DVBP algorithms in non-clairvoyant, clairvoyant and learning-augmented online settings, where item durations (virtual machine lifetimes) are unknown, known and predicted, respectively. Besides the algorithms taken from the literature, we also develop several new algorithms or enhancements. Empirical experimentation is carried out with real-world datasets of Microsoft Azure. The insights from the experimental results are discussed to explore the structures of algorithms and promising design elements that work well in practice."}
