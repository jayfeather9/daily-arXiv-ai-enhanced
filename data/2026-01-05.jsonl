{"id": "2601.00094", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.00094", "abs": "https://arxiv.org/abs/2601.00094", "authors": ["Ali Dasdan"], "title": "Bounds on Longest Simple Cycles in Weighted Directed Graphs via Optimum Cycle Means", "comment": "17 pages, 2 figures", "summary": "The problem of finding the longest simple cycle in a directed graph is NP-hard, with critical applications in computational biology, scheduling, and network analysis. While polynomial-time approximation algorithms exist for restricted graph classes, general bounds remain loose or computationally expensive. In this paper, we exploit optimum cycle means (minimum and maximum cycle means), which are computable in strongly polynomial time, to derive both strict algebraic bounds and heuristic approximations for the weight and length of the longest simple cycle. We rigorously analyze the algebraic relationships between these mean statistics and the properties of longest cycles, and present dual results for shortest cycles. While the strict bounds provide polynomial-time computable constraints suitable for pruning search spaces in branch-and-bound algorithms, our proposed heuristic approximations offer precise estimates for the objective value. Experimental evaluation on ISCAS benchmark circuits demonstrates this trade-off: while the strict algebraic lower bounds are often loose (median 85--93% below true values), the heuristic approximations achieve median errors of only 6--14%. We also observe that maximum weight and maximum length cycles frequently coincide, suggesting that long cycles tend to accumulate large weights."}
{"id": "2601.00272", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.00272", "abs": "https://arxiv.org/abs/2601.00272", "authors": ["Alexandr Andoni", "Themistoklis Haris", "Esty Kelman", "Krzysztof Onak"], "title": "Efficient Algorithms for Adversarially Robust Approximate Nearest Neighbor Search", "comment": "36 pages, 3 figures", "summary": "We study the Approximate Nearest Neighbor (ANN) problem under a powerful adaptive adversary that controls both the dataset and a sequence of $Q$ queries.\n  Primarily, for the high-dimensional regime of $d = ω(\\sqrt{Q})$, we introduce a sequence of algorithms with progressively stronger guarantees. We first establish a novel connection between adaptive security and \\textit{fairness}, leveraging fair ANN search to hide internal randomness from the adversary with information-theoretic guarantees. To achieve data-independent performance, we then reduce the search problem to a robust decision primitive, solved using a differentially private mechanism on a Locality-Sensitive Hashing (LSH) data structure. This approach, however, faces an inherent $\\sqrt{n}$ query time barrier. To break the barrier, we propose a novel concentric-annuli LSH construction that synthesizes these fairness and differential privacy techniques. The analysis introduces a new method for robustly releasing timing information from the underlying algorithm instances and, as a corollary, also improves existing results for fair ANN.\n  In addition, for the low-dimensional regime $d = O(\\sqrt{Q})$, we propose specialized algorithms that provide a strong ``for-all'' guarantee: correctness on \\textit{every} possible query with high probability. We introduce novel metric covering constructions that simplify and improve prior approaches for ANN in Hamming and $\\ell_p$ spaces."}
{"id": "2601.00361", "categories": ["cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00361", "abs": "https://arxiv.org/abs/2601.00361", "authors": ["Rachit Chhaya", "Anirban Dasgupta", "Dan Feldman", "Supratim Shit"], "title": "Deterministic Coreset for Lp Subspace", "comment": null, "summary": "We introduce the first iterative algorithm for constructing a $\\varepsilon$-coreset that guarantees deterministic $\\ell_p$ subspace embedding for any $p \\in [1,\\infty)$ and any $\\varepsilon > 0$. For a given full rank matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ where $n \\gg d$, $\\mathbf{X}' \\in \\mathbb{R}^{m \\times d}$ is an $(\\varepsilon,\\ell_p)$-subspace embedding of $\\mathbf{X}$, if for every $\\mathbf{q} \\in \\mathbb{R}^d$, $(1-\\varepsilon)\\|\\mathbf{Xq}\\|_{p}^{p} \\leq \\|\\mathbf{X'q}\\|_{p}^{p} \\leq (1+\\varepsilon)\\|\\mathbf{Xq}\\|_{p}^{p}$. Specifically, in this paper, $\\mathbf{X}'$ is a weighted subset of rows of $\\mathbf{X}$ which is commonly known in the literature as a coreset. In every iteration, the algorithm ensures that the loss on the maintained set is upper and lower bounded by the loss on the original dataset with appropriate scalings. So, unlike typical coreset guarantees, due to bounded loss, our coreset gives a deterministic guarantee for the $\\ell_p$ subspace embedding. For an error parameter $\\varepsilon$, our algorithm takes $O(\\mathrm{poly}(n,d,\\varepsilon^{-1}))$ time and returns a deterministic $\\varepsilon$-coreset, for $\\ell_p$ subspace embedding whose size is $O\\left(\\frac{d^{\\max\\{1,p/2\\}}}{\\varepsilon^{2}}\\right)$. Here, we remove the $\\log$ factors in the coreset size, which had been a long-standing open problem. Our coresets are optimal as they are tight with the lower bound. As an application, our coreset can also be used for approximately solving the $\\ell_p$ regression problem in a deterministic manner."}
{"id": "2601.00768", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.00768", "abs": "https://arxiv.org/abs/2601.00768", "authors": ["Mihail Stoian"], "title": "Mind the Gap. Doubling Constant Parametrization of Weighted Problems: TSP, Max-Cut, and More", "comment": "To appear at STACS 2026", "summary": "Despite much research, hard weighted problems still resist super-polynomial improvements over their textbook solution. On the other hand, the unweighted versions of these problems have recently witnessed the sought-after speedups. Currently, the only way to repurpose the algorithm of the unweighted version for the weighted version is to employ a polynomial embedding of the input weights. This, however, introduces a pseudo-polynomial factor into the running time, which becomes impractical for arbitrarily weighted instances.\n  In this paper, we introduce a new way to repurpose the algorithm of the unweighted problem. Specifically, we show that the time complexity of several well-known NP-hard problems operating over the $(\\min, +)$ and $(\\max, +)$ semirings, such as TSP, Weighted Max-Cut, and Edge-Weighted $k$-Clique, is proportional to that of their unweighted versions when the set of input weights has small doubling. We achieve this by a meta-algorithm that converts the input weights into polynomially bounded integers using the recent constructive Freiman's theorem by Randolph and Węgrzycki [ESA 2024] before applying the polynomial embedding."}
{"id": "2601.00380", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00380", "abs": "https://arxiv.org/abs/2601.00380", "authors": ["Hanzhe Li", "Bingchen Lin", "Mengyuan Xu"], "title": "Word Frequency Counting Based on Serverless MapReduce", "comment": "6 pages, 4 figures, International Conference on Engineering Management, Information Technology and Intelligence (EMITI 2024)", "summary": "With the increasing demand for high-performance and high-efficiency computing, cloud computing, especially serverless computing, has gradually become a research hotspot in recent years, attracting numerous research attention. Meanwhile, MapReduce, which is a popular big data processing model in the industry, has been widely applied in various fields. Inspired by the serverless framework of Function as a Service and the high concurrency and robustness of MapReduce programming model, this paper focus on combining them to reduce the time span and increase the efficiency when executing the word frequency counting task. In this case, the paper use a MapReduce programming model based on a serverless computing platform to figure out the most optimized number of Map functions and Reduce functions for a particular task. For the same amount of workload, extensive experiments show both execution time reduces and the overall efficiency of the program improves at different rates as the number of map functions and reduce functions increases. This paper suppose the discovery of the most optimized number of map and reduce functions can help cooperations and programmers figure out the most optimized solutions."}
{"id": "2601.00450", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.00450", "abs": "https://arxiv.org/abs/2601.00450", "authors": ["Elham Cheshmikhani", "Hamed Farbeh", "Hossein Asadi"], "title": "Enhancing Reliability of STT-MRAM Caches by Eliminating Read Disturbance Accumulation", "comment": null, "summary": "Spin-Transfer Torque Magnetic RAM (STT-MRAM) as one of the most promising replacements for SRAMs in on-chip cache memories benefits from higher density and scalability, near-zero leakage power, and non-volatility, but its reliability is threatened by high read disturbance error rate. Error-Correcting Codes (ECCs) are conventionally suggested to overcome the read disturbance errors in STT-MRAM caches. By employing aggressive ECCs and checking out a cache block on every read access, a high level of cache reliability is achieved. However, to minimize the cache access time in modern processors, all blocks in the target cache set are simultaneously read in parallel for tags comparison operation and only the requested block is sent out, if any, after checking its ECC. These extra cache block reads without checking their ECCs until requesting the blocks by the processor cause the accumulation of read disturbance error, which significantly degrade the cache reliability. In this paper, we first introduce and formulate the read disturbance accumulation phenomenon and reveal that this accumulation due to conventional parallel accesses of cache blocks significantly increases the cache error rate. Then, we propose a simple yet effective scheme, so-called Read Error Accumulation Preventer cache (REAP-cache), to completely eliminate the accumulation of read disturbances without compromising the cache performance. Our evaluations show that the proposed REAP-cache extends the cache Mean Time To Failure (MTTF) by 171x, while increases the cache area by less than 1% and energy consumption by only 2.7%."}
{"id": "2601.00397", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00397", "abs": "https://arxiv.org/abs/2601.00397", "authors": ["Amey Agrawal", "Mayank Yadav", "Sukrit Kumar", "Anirudha Agrawal", "Garv Ghai", "Souradeep Bera", "Elton Pinto", "Sirish Gambhira", "Mohammad Adain", "Kasra Sohrab", "Chus Antonanzas", "Alexey Tumanov"], "title": "Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving", "comment": null, "summary": "Deploying LLMs efficiently requires testing hundreds of serving configurations, but evaluating each one on a GPU cluster takes hours and costs thousands of dollars. Discrete-event simulators are faster and cheaper, but they require re-implementing the serving system's control logic -- a burden that compounds as frameworks evolve.\n  We present Revati, a time-warp emulator that enables performance modeling by directly executing real serving system code at simulation-like speed. The system intercepts CUDA API calls to virtualize device management, allowing serving frameworks to run without physical GPUs. Instead of executing GPU kernels, it performs time jumps -- fast-forwarding virtual time by predicted kernel durations. We propose a coordination protocol that synchronizes these jumps across distributed processes while preserving causality. On vLLM and SGLang, Revati achieves less than 5% prediction error across multiple models and parallelism configurations, while running 5-17x faster than real GPU execution."}
{"id": "2601.00456", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.00456", "abs": "https://arxiv.org/abs/2601.00456", "authors": ["Elham Cheshmikhani", "Hamed Farbeh", "Hossein Asadi"], "title": "ROBIN: Incremental Oblique Interleaved ECC for Reliability Improvement in STT-MRAM Caches", "comment": null, "summary": "Spin-Transfer Torque Magnetic RAM} (STT-MRAM) is a promising alternative for SRAMs in on-chip cache memories. Besides all its advantages, high error rate in STT-MRAM is a major limiting factor for on-chip cache memories. In this paper, we first present a comprehensive analysis that reveals that the conventional Error-Correcting Codes (ECCs) lose their efficiency due to data-dependent error patterns, and then propose an efficient ECC configuration, so-called ROBIN, to improve the correction capability. The evaluations show that the inefficiency of conventional ECC increases the cache error rate by an average of 151.7% while ROBIN reduces this value by more than 28.6x."}
{"id": "2601.00530", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.00530", "abs": "https://arxiv.org/abs/2601.00530", "authors": ["Ravi Teja Pagidoju"], "title": "Cost-Performance Analysis of Cloud-Based Retail Point-of-Sale Systems: A Comparative Study of Google Cloud Platform and Microsoft Azure", "comment": "Accepted at 38th International Conference on Computer Applications in Industry and Engineering (CAINE 2025)", "summary": "Althoughthereislittleempiricalresearchonplatform-specific performance for retail workloads, the digital transformation of the retail industry has accelerated the adoption of cloud-based Point-of-Sale (POS) systems. This paper presents a systematic, repeatable comparison of POS workload deployments on Google Cloud Platform (GCP) and Microsoft Azure using real-time API endpoints and open-source benchmarking code. Using free-tier cloud resources, we offer a transparent methodology for POS workload evaluation that small retailers and researchers can use. Our approach measures important performance metrics like response latency, throughput, and scalability while estimating operational costs based on actual resource usage and current public cloud pricing because there is no direct billing under free-tier usage. All the tables and figures in this study are generated directly from code outputs, ensuring that the experimental data and the reported results are consistent. Our analysis shows that GCP achieves 23.0% faster response times at baseline load, while Azure shows 71.9% higher cost efficiency for steady-state operations. We look at the architectural components that lead to these differences and provide a helpful framework for merchants considering cloud point-of-sale implementation. This study establishes a strong, open benchmarking methodology for retail cloud applications and offers the first comprehensive, code-driven comparison of workloads unique to point-of-sale systems across leading cloud platforms."}
{"id": "2601.00644", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.00644", "abs": "https://arxiv.org/abs/2601.00644", "authors": ["Yuchen Li", "Rui Kong", "Zhonghao Lyu", "Qiyang Li", "Xinran Chen", "Hengyi Cai", "Lingyong Yan", "Shuaiqiang Wang", "Jiashu Zhao", "Guangxu Zhu", "Linghe Kong", "Guihai Chen", "Haoyi Xiong", "Dawei Yin"], "title": "FlexSpec: Frozen Drafts Meet Evolving Targets in Edge-Cloud Collaborative LLM Speculative Decoding", "comment": null, "summary": "Deploying large language models (LLMs) in mobile and edge computing environments is constrained by limited on-device resources, scarce wireless bandwidth, and frequent model evolution. Although edge-cloud collaborative inference with speculative decoding (SD) can reduce end-to-end latency by executing a lightweight draft model at the edge and verifying it with a cloud-side target model, existing frameworks fundamentally rely on tight coupling between the two models. Consequently, repeated model synchronization introduces excessive communication overhead, increasing end-to-end latency, and ultimately limiting the scalability of SD in edge environments. To address these limitations, we propose FlexSpec, a communication-efficient collaborative inference framework tailored for evolving edge-cloud systems. The core design of FlexSpec is a shared-backbone architecture that allows a single and static edge-side draft model to remain compatible with a large family of evolving cloud-side target models. By decoupling edge deployment from cloud-side model updates, FlexSpec eliminates the need for edge-side retraining or repeated model downloads, substantially reducing communication and maintenance costs. Furthermore, to accommodate time-varying wireless conditions and heterogeneous device constraints, we develop a channel-aware adaptive speculation mechanism that dynamically adjusts the speculative draft length based on real-time channel state information and device energy budgets. Extensive experiments demonstrate that FlexSpec achieves superior performance compared to conventional SD approaches in terms of inference efficiency."}
