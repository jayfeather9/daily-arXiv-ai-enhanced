<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.DS](#cs.DS) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs](https://arxiv.org/abs/2512.15306)
*Erik Schultheis,Dan Alistarh*

Main category: cs.DC

TL;DR: 涉及MLIR、Compiler、HLS、DSL、Graph Processing部分：无。涉及Compiler部分：无。涉及HLS部分：无。涉及DSL部分：无。涉及Graph Processing部分：无。涉及MLIR部分：无。

LLMQ是一个端到端的CUDA/C++实现，专门用于在资源受限的商用（如游戏）GPU上进行中等规模语言模型（3B到32B）的训练。该框架通过整合激活检查点、内存卸载和基于复制引擎的通信优化等技术，有效解决了这些设备低内存和慢通信的瓶颈，使得用户可以在单张16GB中端卡上训练7B模型，或在四张RTX 4090上训练32B模型，且保持约50%的FLOP利用率和生产级系统的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的中等规模语言模型（3B到32B参数）的训练通常需要昂贵、内存大、通信快速的数据中心级GPU。LLMQ的开发动机是使这些模型的训练能够在资源受限、内存小、通信慢的、价格可承受的商用/消费级GPU上进行。

Method: LLMQ通过一系列优化措施来解决商用GPU上的内存和通信瓶颈，包括激活检查点（activation checkpointing）、内存卸载（offloading）以及基于复制引擎（copy-engine）的集合通信（collectives）。

Result: LLMQ成功实现了目标，它可以在单个16GB中端游戏卡上训练或微调7B模型，或在配备4块RTX 4090的工作站上训练32B模型。整个过程采用标准的8位训练流程，没有额外的算法近似，同时保持了约50%的FLOP利用率，其效率与昂贵云级GPU上的生产规模系统相当。

Conclusion: LLMQ是一个用于在资源受限的商用GPU上进行中等规模语言模型训练的高效、通用框架。通过针对性优化，它在保持标准训练流程和性能的同时，显著降低了硬件门槛，使更多用户能够负担得起模型训练，其效率可与昂贵的云级系统相媲美。

Abstract: We present LLMQ, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow communication compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. LLMQ is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of LLMQ rivals that of production-scale systems on much more expensive cloud-grade GPUs.

</details>


### [2] [Optimizing Bloom Filters for Modern GPU Architectures](https://arxiv.org/abs/2512.15595)
*Daniel Jünger,Kevin Kristensen,Yunsong Wang,Xiangyao Yu,Bertil Schmidt*

Main category: cs.DC

TL;DR: 该论文与编译器相关部分是其软件实现，但核心是通用高性能计算和异构计算；具体来说，本篇论文的核心领域是**高性能计算（HPC）/异构计算**和**数据结构/算法加速**。所提出的方法利用 GPU 的大规模并行性，通过优化向量化、线程协作和计算延迟来加速 Bloom Filter 及其变体的操作，显著提高了吞吐量并打破了速度与精度之间的传统权衡，在批量查找和构造上分别比现有技术提高了 11.35 倍和 15.4 倍。


<details>
  <summary>Details</summary>
Motivation: Bloom Filter 及其变体是近似成员查询的基本数据结构，广泛应用于数据分析、数据库和基因组学等领域。尽管 CPU 上的优化实现已经得到充分研究，但 GPU 上的设计仍未得到充分探索，尤其是在利用 GPU 的大规模线程级并行性和高带宽内存方面。因此，作者的动机在于探索 GPU 上的设计空间，以加速 Bloom Filter 的操作，目标是达到每秒数十亿次操作，从而弥补 GPU 设计方面的空白。主要目的是打破速度和精度之间的传统权衡，并大幅提升性能。

Method: 作者通过在 GPU 上探索三个优化维度（向量化、线程协作和计算延迟）来优化 Bloom Filter 及其变体。方法论涉及设计实验来评估这些优化点组合对吞吐量的影响，并研究硬件如何响应不同的参数配置。关键在于识别出当过滤器适应 GPU 缓存域时能获得最大增益。最终目标是提出一种打破速度和精度之间传统平衡的优化设计。作者使用模块化的 CUDA/C++ 实现来验证其方法。

Result: 优化设计在 Bloom Filter 操作中获得了显著的性能提升。结果显示，优化的关键在于当过滤器适应 GPU 的缓存域时，能实现最大的吞吐量增益。在保持高精度配置的同时，所提出的方法实现了高吞吐量，克服了速度与精度之间的传统权衡。在相同的错误率下，该方法在批量过滤器查找上比现有技术提高了 11.35 倍，在构造上提高了 15.4 倍。在 B200 GPU 上，该方法在各种配置下达到了实际速度（practical speed-of-light）的 92% 以上。作者提出了一种模块化的 CUDA/C++ 实现。

Conclusion: 本文通过探索 GPU 上的三个优化维度（向量化、线程协作和计算延迟），打破了 Bloom Filter 及其变体中速度与精度之间的传统权衡。新的优化设计能够在保持高精度配置的卓越准确性的同时，实现通常只有高错误率变体才能达到的高吞吐量。在相同的错误率下，所提出的方法在批量查找（构造）方面分别比现有技术提高了 11.35 倍（15.4 倍），并在 B200 GPU 上广泛的配置范围内达到了实际速度（practical speed-of-light）的 92% 以上。

Abstract: Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.
  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\times$ ($15.4\times$) for bulk filter lookup (construction), respectively, achieving above $92\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.

</details>


### [3] [Dynamic Rebatching for Efficient Early-Exit Inference with DREX](https://arxiv.org/abs/2512.15705)
*Xuting Liu,Daniel Alexander,Siva Kesava Reddy Kakarla,Behnaz Arzani,Vincent Liu*

Main category: cs.DC

TL;DR: 相关：本文与LLM的推理加速（属于编译优化和系统设计范畴）相关。该工作提出了DREX系统，用于解决Early-Exit（EE）LLM在传统批处理框架下的效率和质量问题。
总结：早退出（EE）LLM通过允许更容易的token在模型浅层提前退出以加速推理。传统的批处理框架不适用EE LLM，因为请求退出时间不一致。本文提出动态重批处理（Dynamic Rebatching）机制，并在DREX系统中实现，结合零拷贝重批处理缓冲区和EE及SLA感知的调度器来优化EE LLM推理。DREX在保持输出质量的同时，将吞吐量提高了2%到12%，并完全消除了非自愿退出，保证了模型质量。


<details>
  <summary>Details</summary>
Motivation: 早退出（EE）LLM架构通过允许更容易的token仅使用模型层的一个子集来生成，从而加速推理。然而，传统的批处理框架不适合EE LLM，因为批次中并非所有请求都能同时退出。现有解决方案要么强制批次做出统一决策，从而忽略了EE机会，要么通过强制提前退出而降低了输出质量。旨在解决现有批处理框架与EE LLM不兼容的问题，特别是如何提高吞吐量并保持输出质量。

Method: 提出动态重批处理（Dynamic Rebatching）机制，在每个早退出点动态地重组批次。满足退出标准的请求立即处理，未满足的请求被放入缓冲区，重新分组为新批次，并送入更深的层。实现了一个名为DREX的早退出推理系统，包含两个关键优化：1）一个零拷贝重批处理缓冲区，避免物理数据移动；2）一个EE和SLA感知的调度器，用于分析性地预测重批处理操作是否有利。DREX还通过节省内存的状态复制有效地处理了被跳过的层的缺失的KV缓存。

Result: DREX系统在保持输出质量的同时，将吞吐量比基线方法提高了2%到12%。更重要的是，DREX完全消除了非自愿退出（involuntary exits），为保证EE模型预期的输出质量提供了关键保证。

Conclusion: DREX系统通过动态重批处理、零拷贝重批处理缓冲区和EE及SLA感知的调度器，成功地在保持输出质量的同时提升了EE LLM的推理吞吐量。它消除了非自愿退出，确保了EE模型预期的输出质量得到了维护。

Abstract: Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [4] [Sharing State Between Prompts and Programs](https://arxiv.org/abs/2512.14805)
*Ellie Y. Cheng,Logan Weber,Tian Jin,Michael Carbin*

Main category: cs.PL

TL;DR: 该论文与**编译器**相关，因为它研究并实现了一种新的编程抽象和编程系统（Nightjar），旨在将自然语言代码（由LLMs驱动）与形式语言（如Python）进行深度整合，涉及编程语言设计和执行环境的互操作性。与**DSL**相关，因为自然语言编程本身可以视为一种非形式化的DSL，而文中提出的“自然函数接口”是为这种“语言”提供结构化规范。

**TLDR:** 随着大型语言模型普及，自然语言编程（提示词）开始流行。本文提出了一个新颖的编程抽象——“共享程序状态”，它允许自然语言代码直接操作Python程序的变量、对象和控制流，从而消除了自然语言代码与形式语言互操作所需的手动工作。通过在Nightjar系统中实现和评估，该方法在保持或提高任务准确性（+4%-19%）的同时，显著减少了代码行数（平均减少39.6%），但可能会引入运行时开销（0.4-4.3x）。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的兴起，出现了一种新的编程范式——自然语言编程。用户通过编写提示词（prompts）来指导LLMs执行自然语言处理、代码生成和推理等任务，实际上就是在用自然语言编写代码（natural language code）。现有的研究领域开始关注如何使这种自然语言代码与像Python这样的正式语言实现互操作性。然而，实现这种互操作性需要大量的手动工作。本文的动机是解决这一问题，即提供一种机制来移除这种手动工作，使自然语言代码可以直接、无缝地访问和操作程序状态。

Method: 本文提出了一种名为“共享程序状态”（shared program state）的新颖编程抽象，用于实现自然语言代码与正式编程语言（如Python）之间的互操作性。它允许自然语言代码直接读写程序变量、使用程序对象进行计算并实现程序控制流。为了支持这一抽象，作者提出了一种用于指定自然函数接口（natural function interfaces）的模式，并将共享程序状态指定为一种自然函数接口来实现。该方法在Nightjar编程系统中进行了实现，Nightjar允许Python程序包含与Python程序状态共享的自然语言代码。

Result: 本文提出的Nightjar系统实现了共享程序状态抽象，并进行了评估。结果显示：
1. **任务准确性：** Nightjar程序实现了与手动编写实现相当或更高的任务准确性（提高了4%到19%）。
2. **代码量减少：** Nightjar程序平均减少了39.6%的代码行数。
3. **运行时开销：** 引入共享程序状态的代价是可能产生一定的运行时开销（是手动实现的0.4到4.3倍）。这表明该方法在提高生产力和准确性的同时，牺牲了部分运行时性能。

Conclusion: 本文提出了一种名为“共享程序状态”（shared program state）的新颖编程抽象，它能够消除在自然语言编程（如LLMs的提示）与正式编程语言（如Python）之间实现互操作性所需的手动工作。通过Nightjar编程系统的实现和评估，证明了共享程序状态不仅提高了任务准确性（+4%-19%），大幅减少了代码行数（平均减少39.6%），同时允许程序员在自然语言中直接操作程序变量、对象和实现控制流。虽然会带来一定的运行时开销（0.4-4.3倍），但它为大模型背景下的混合式自然语言编程提供了一个高效且强大的解决方案。

Abstract: The rise of large language models (LLMs) has introduced a new type of programming: natural language programming. By writing prompts that direct LLMs to perform natural language processing, code generation, reasoning, etc., users are writing code in natural language -- natural language code -- for the LLM to execute.
  An emerging area of research enables interoperability between natural language code and formal languages such as Python. We present a novel programming abstraction, shared program state, that removes the manual work required to enable interoperability between natural language code and program state. With shared program state, programmers can write natural code that directly writes program variables, computes with program objects, and implements control flow in the program. We present a schema for specifying natural function interfaces that extend programming systems to support natural code and leverage this schema to specify shared program state as a natural function interface.
  We implement shared program state in the Nightjar programming system. Nightjar enables programmers to write Python programs that contain natural code that shares the Python program state. We show that Nightjar programs achieve comparable or higher task accuracy than manually written implementations (+4-19%), while decreasing the lines of code by 39.6% on average. The tradeoff to using Nightjar is that it may incur runtime overhead (0.4-4.3x runtime of manual implementations).

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [5] [FAME: FPGA Acceleration of Secure Matrix Multiplication with Homomorphic Encryption](https://arxiv.org/abs/2512.15515)
*Zhihan Xu,Rajgopal Kannan,Viktor K. Prasanna*

Main category: cs.AR

TL;DR: 该论文与编译器（更具体地说是硬件加速器设计和实现）以及 HLS 相关。
对于云计算中的隐私保护计算，同态加密（HE）由于其高昂的计算成本，特别是矩阵乘法（MM），难以实际应用。本文提出了 FAME，这是首个专用于 HE MM 的带宽高效 FPGA 加速器。通过成本模型分析和设计一种新颖的 HLT 数据路径以实现细粒度数据重用，FAME 显著减少了内存开销。实验结果表明，FAME 相比现有 CPU 实现，平均加速达 221 倍，证明了其在大规模 HE MM 工作负载中的实用性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 同态加密（HE）可以在加密数据上进行安全计算，解决了云计算中的隐私问题。然而，HE 操作，特别是矩阵乘法（MM），计算成本高昂，严重阻碍了其实际应用。因此，加速同态加密矩阵乘法（HE MM）对于隐私保护的机器学习等应用至关重要。本文的动机在于解决 HE MM 的高计算成本问题，并通过优化片上内存使用来提高 HE MM 的效率和可扩展性。

Method: 本文首先建立了一个成本模型来评估给定 HE 参数集和输入矩阵尺寸下的片上内存需求，强调了优化片上内存的关键性。随后，针对 HE MM 中的主要瓶颈——同态线性变换（HLT），本文设计了一种新颖的数据路径。该数据路径通过实现细粒度的数据重用，显著减少了片外内存流量和片上内存需求。最后，利用该数据路径，本文提出了 FAME 加速器，这是首个专用于 HE MM 的 FPGA 加速器，支持任意矩阵形状和广泛的 HE 参数集。

Result: 本文提出的 FAME 加速器在 Alveo U280 FPGA 上实现，并针对不同的矩阵尺寸和形状进行了性能评估。实验结果表明，FAME 相对于现有最先进的基于 CPU 的实现，实现了平均 221 倍的加速。这展示了 FAME 对于大规模连续 HE MM 和实际工作负载的优越的可扩展性和实用性。

Conclusion: 本文设计并实现了一个名为 FAME 的 FPGA 加速器，专门用于同态加密矩阵乘法（HE MM）。FAME 通过新颖的同态线性变换（HLT）数据路径，显著提高了带宽和片上内存利用率，从而实现了高效的 HE MM。实验结果表明，相对于现有的 CPU 实现，FAME 实现了平均 221 倍的加速，证明了其对于大规模连续 HE MM 和实际工作负载的实用性和可扩展性。

Abstract: Homomorphic Encryption (HE) enables secure computation on encrypted data, addressing privacy concerns in cloud computing. However, the high computational cost of HE operations, particularly matrix multiplication (MM), remains a major barrier to its practical deployment. Accelerating homomorphic encrypted MM (HE MM) is therefore crucial for applications such as privacy-preserving machine learning.
  In this paper, we present a bandwidth-efficient FPGA implementation of HE MM. We first develop a cost model to evaluate the on-chip memory requirements for a given set of HE parameters and input matrix sizes. Our analysis shows that optimizing on-chip memory usage is critical for scalable and efficient HE MM. To this end, we design a novel datapath for Homomorphic Linear Transformation (HLT), the primary bottleneck in HE MM. The proposed datapath significantly reduces off-chip memory traffic and on-chip memory demand by enabling fine-grained data reuse. Leveraging this datapath, we introduce FAME, the first FPGA-based accelerator specifically tailored for HE MM. FAME supports arbitrary matrix shapes and is configurable across a wide range of HE parameter sets. We implement FAME on an Alveo U280 FPGA and evaluate its performance across diverse matrix sizes and shapes. Experimental results show that FAME achieves an average speedup of 221x over state-of-the-art CPU-based implementations, demonstrating its scalability and practicality for large-scale consecutive HE MM and real-world workloads.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [6] [Label-consistent clustering for evolving data](https://arxiv.org/abs/2512.15210)
*Ameet Gadekar,Aristides Gionis,Thibault Marette*

Main category: cs.DS

TL;DR: 该论文与 DSL 或图处理或 MLIR 或编译器或 HLS 无关。

太长不读：本文研究了在迭代数据分析中如何实现聚类解的平稳演化，提出了标签一致性 $k$-中心聚类问题，目标是找到一个新的聚类方案，在最小化聚类成本的同时，与先前的聚类方案保持最小的变化。为此，论文提出了两种具有常数近似比的近似算法，并通过实验证明了其在实际数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 数据分析通常涉及迭代过程，需要根据新数据不断完善现有解决方案。在寻找高质量解决方案的同时，还要确保与先前解决方案保持最小的变化，以实现解决方案的平稳和渐进演化。具体到聚类问题，就是在给定先前的聚类解的情况下，计算一个新的聚类解，使其在最小化聚类成本的同时，与先前解之间的差异（变化）不超过一个预算 $b$。

Method: 提出了两种针对标签一致性 $k$-中心问题的常数因子近似算法。

Result: 提出了两种常数因子近似算法，并进行了实验验证，结果显示了所提方法在实际数据集上的有效性。

Conclusion: 本文研究了标签一致性 $k$-中心聚类问题，并提出了两种具有常数近似比的近似算法。实验评估证明了所提出方法在实际数据集上的有效性。

Abstract: Data analysis often involves an iterative process, where solutions must be continuously refined in response to new data. Typically, as new data becomes available, an existing solution must be updated to incorporate the latest information. In addition to seeking a high-quality solution for the task at hand, it is also crucial to ensure consistency by minimizing drastic changes from previous solutions. Applying this approach across many iterations, ensures that the solution evolves gradually and smoothly.
  In this paper, we study the above problem in the context of clustering, specifically focusing on the $k$-center problem. More precisely, we study the following problem: Given a set of points $X$, parameters $k$ and $b$, and a prior clustering solution $H$ for $X$, our goal is to compute a new solution $C$ for $X$, consisting of $k$ centers, which minimizes the clustering cost while introducing at most $b$ changes from $H$. We refer to this problem as label-consistent $k$-center, and we propose two constant-factor approximation algorithms for it. We complement our theoretical findings with an experimental evaluation demonstrating the effectiveness of our methods on real-world datasets.

</details>


### [7] [A Constant-Factor Approximation for Directed Latency](https://arxiv.org/abs/2512.15473)
*Jannis Blauth,Ramin Mousavi*

Main category: cs.DS

TL;DR: This content has not passed the compliance test and has been hidden.


<details>
  <summary>Details</summary>
Motivation: “有向延迟”问题，与对称版本（如“送货员问题”或“修理工问题”）相比，理解上存在显著差距。最佳近似比在十多年里一直维持在$O(\log n)$。虽然最近的研究取得了准多项式时间内的常数因子近似比，但计算时间过长。因此，研究的动机是找到第一个在多项式时间内解决“有向延迟”问题的常数因子近似算法，以弥补理论上的空白并提供更实用的解决方案。

Method: 文章提出了一种全新的分桶方法来强化标准的线性规划（LP）松弛。虽然由此得到的LP不再是“有向延迟”问题的松弛，但其允许一个良好的解。在此基础上，作者设计了一个取整算法，该算法主要利用了对LP可行域的限制方式，从而从LP的分数解中得到一个常数因子近似解。

Result: 本文首次提出了一个在多项式时间内为“有向延迟”问题找到常数因子近似解的算法，解决了该问题在近似比和计算时间上的长期挑战。这个突破性结果依赖于一种全新的、不依赖于几何递增距离的分桶方法，并结合一个经过修正和强化的线性规划松弛。

Conclusion: 本文介绍了第一个在多项式时间内为“有向延迟”问题（Directed Latency）提供常数因子近似比的算法。该算法通过引入一种全新的分桶方法，对标准线性规划（LP）松弛进行强化，并设计了相应的取整算法。尽管得到的LP不再是原始问题的松弛，但它仍然能获得一个好的解，突破了之前算法在近似比和时间复杂度上的限制。

Abstract: In the Directed Latency problem, we are given an asymmetric metric on a set of vertices (or clients), and a given depot $s$. We seek a path $P$ starting at $s$ and visiting all the clients so as to minimize the sum of client waiting times (also known as latency) before being visited on the path.
  In contrast to the symmetric version of this problem (also known as the Deliveryperson problem and the Repairperson problem in the literature), there are significant gaps in our understanding of Directed Latency. The best approximation factor has remained at $O(\log n)$, where $n$ is the number of clients, for more than a decade [Friggstad, Salavatipour, and Svitkina, '13]. Only recently, [Friggstad and Swamy, '22] presented a constant-factor approximation but in quasi-polynomial time. Both results follow similar ideas: they consider buckets with geometrically-increasing distances, build paths in each bucket, and then stitch together all these paths to get a feasible solution. [Friggstad and Swamy, '22] showed if we guess a vertex from each bucket and augment a standard LP relaxation with these guesses, then one can reduce the stitching cost. Unfortunately, there are logarithmically many buckets so the running time of their algorithm is quasi-polynomial.
  In this paper, we present the first constant-factor approximation for Directed Latency in polynomial time by introducing a completely new way of bucketing which helps us strengthen a standard LP relaxation with less aggressive guessing. Although the resulting LP is no longer a relaxation of Directed Latency, it still admits a good solution. We present a rounding algorithm for fractional solutions of our LP, crucially exploiting the way we restricted the feasibility region of the LP formulation.

</details>
