<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DS](#cs.DS) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Simple Modal Types for Functional Reactive Programming](https://arxiv.org/abs/2512.09412)
*Patrick Bahr*

Main category: cs.PL

TL;DR: 该论文与DSL（函数式反应式编程是一种声明式编程范式，可以视作一种特定领域的建模语言）相关。
函数式反应式编程（FRP）是一种用于实现高抽象级别反应式程序的声明性编程范式，但其要求程序必须保证因果性、生产性和无空间泄漏。现有的模态类型系统虽然能强制实现这些操作属性，但限制了程序表达能力。本文提出了一种新的FRP语言，该语言采用显著简化的模态类型系统，通过改变信号的语义，允许将信号建模为可变引用，并由'later'类型模态严格控制其可变性。这种方法在保证核心操作属性的同时，提高了语言的表达能力，并实现了更高效的信号就地更新，同时保留了函数式编程风格。


<details>
  <summary>Details</summary>
Motivation: 传统的函数式反应式编程（FRP）需要保证程序是因果的（causal）、生产性的（productive）和没有空间泄漏的（free from space leaks）。已有的模态类型系统虽然能强制实现这些操作属性，但往往限制了程序的表达能力。因此，本文的动机是希望设计一个具有显著简化模态类型系统的新FRP语言，在保证核心操作属性的同时，减少类型系统的限制，提高语言的表达能力和效率。

Method: 本文设计并提出了一种新的FRP语言，其具有显著简化的模态类型系统。关键方法是通过改变信号的语义，将信号建模为可变引用，并利用'later'类型模态来严格控制其可变性，从而在保证程序因果性、生产性和无空间泄漏的同时，允许更多的程序通过类型检查，并实现更高效的信号就地更新。

Result: 本文成功提出了一种新的FRP语言，其模态类型系统被显著简化，相比现有模态FRP语言施加的限制更少。通过改变信号的语义，新的系统能够安全地允许更多的程序通过类型检查，增强了语言的表达能力。此外，将信号建模为被‘later’类型模态严格控制的可变引用，实现了信号的更高效就地更新，同时保持了函数式编程风格。

Conclusion: 本文介绍了新的FRP语言及其简化的模态类型系统，强调其在保证核心操作属性的同时，提高了语言的表达能力和效率。通过对信号语义的修改，允许信号被建模为可变引用，并由'later'类型模态严格控制，这种受控的可变性在保持函数式编程风格的同时，实现了更高效的就地更新。

Abstract: Functional reactive programming (FRP) is a declarative programming paradigm for implementing reactive programs at a high level of abstraction. It applies functional programming principles to construct and manipulate time-varying values, also known as signals. However, for this programming paradigm to work in practice, an FRP language must ensure that programs are causal, productive, and free from space leaks. Over the past fifteen years, several modal type systems to enforce these operational properties have been developed.
  We present a new FRP language with a significantly simplified modal type system that imposes fewer restrictions than previous modal FRP languages while still guaranteeing the central operational properties of causality, productivity, and absence of space leaks. The key enabling idea is to alter the semantics of signals so that the type system can safely allow more programs to type-check, which also makes the language more expressive. With this new semantics, signals are modelled as mutable references whose mutability is tightly controlled by the 'later' type modality. This disciplined form of mutability also enables more efficient in-place updates of signals, all while preserving a functional programming style.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [RACAM: Enhancing DRAM with Reuse-Aware Computation and Automated Mapping for ML Inference](https://arxiv.org/abs/2512.09304)
*Siyuan Ma,Jiajun Hu,Jeeho Ryoo,Aman Arora,Lizy Kurian John*

Main category: cs.AR

TL;DR: 与此论文相关的领域包括：图处理（Graph processing） (没有直接提及), MLIR (没有直接提及), 编译器 (没有直接提及), HLS (没有直接提及)。主要相关领域是**DSL** (没有直接提及，但其加速目标大型语言模型推理可能涉及领域特定优化), **编译器** (涉及工作负载映射和优化).

**太长不看 (TLDR) 概要：**为了解决现有位串行DRAM-PIM架构在大型语言模型 (LLM) 推理中缺乏数据重用、冗余数据传输和工作负载映射不足的问题，本文提出了RACAM，这是首个采用专用局部性缓冲区、位串行PE、以及归约和广播单元的DRAM内位串行架构，以实现数据重用和减少传输冗余。 RACAM还提出了一种工作负载映射机制来充分利用DRAM并行性并优化映射方案。 在LLM推理评估中，RACAM相比GPU实现了9倍到102倍的加速，并且相对于最先进的DRAM内PIM系统Proteus，性能密度提高了233倍。


<details>
  <summary>Details</summary>
Motivation: 现有的内存处理（PIM）技术，尤其是位串行DRAM-PIM架构，虽然通过支持运行时可变数据精度在LLM推理等新兴工作负载中显示出潜力，但仍存在主要限制：缺乏数据重用、大量的冗余数据传输以及工作负载映射支持不足。这些限制阻碍了其效率的进一步提升。

Method: RACAM是首个DRAM内位串行架构，其核心机制包括：1. 专用的局部性缓冲区（dedicated locality buffers）以提高数据重用。2. 位串行处理单元（bit-serial PEs）支持运行时的可变数据精度。3. popcount归约单元（popcount reduction units）和广播单元（broadcast units）以减轻冗余数据传输。4. 提出了一种工作负载映射机制（workload mapping mechanism）以充分利用DRAM架构的大规模并行性，并找到给定工作负载的最佳映射方案。

Result: RACAM在端到端LLM推理方面的评估结果显示：相对于GPU，RACAM实现了9倍到102倍的加速。在GPT3的情况下，RACAM的每平方毫米性能（performance per mm2）比现有的最先进DRAM内PIM系统Proteus高出233倍。

Conclusion: RACAM是首个DRAM内位串行PIM架构，通过专用的局部性缓冲区、位串行PE、popcount归约单元和广播单元实现了数据重用并减少了冗余数据传输。同时，提出的工作负载映射机制充分利用了DRAM架构的并行性，并确定了给定工作负载的最佳映射方案。实验证明，RACAM在端到端大型语言模型推理方面，相对于GPU和现有最先进的DRAM内PIM系统Proteus，具有显著的性能提升和更高的每平方毫米性能。

Abstract: In-DRAM Processing-In-Memory (DRAM-PIM) has emerged as a promising approach to accelerate memory-intensive workloads by mitigating data transfer overhead between DRAM and the host processor. Bit-serial DRAM-PIM architectures, further enhance efficiency by supporting runtime variable data precision, which is critical for emerging workloads, such as large language model (LLM) inference. However, existing works still have major limitations: lack of data reuse, significant amounts of redundant data transfer, and insufficient support for workload mapping. To address these issues, we propose RACAM, the first in-DRAM bit-serial architecture which uses dedicated locality buffers, bit-serial PEs, popcount reduction units and broadcast units to enable data reuse and alleviate redundant data transfers. Furthermore, a workload mapping mechanism is proposed to fully explore the massive parallelism of DRAM architecture and identify the best mapping scheme of a given workload. We evaluate RACAM against GPUs and the state-of-the-art, in-DRAM PIM system, Proteus, across end-to-end LLM inferences. RACAM achieves 9x to 102x speedup over GPUs and 233x higher performance per mm2 compared to Proteus in case of GPT3.

</details>


### [3] [ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators](https://arxiv.org/abs/2512.09427)
*Guoqiang Zou,Wanyu Wang,Hao Zheng,Longxiang Yin,Yinhe Han*

Main category: cs.AR

TL;DR: 相关性：该论文与编译器（硬件感知分配）和图处理（Cambricon MLU370是针对AI计算的加速器，通常涉及图或张量处理，尽管抽象中未直接提及图处理，但其上下文强相关）相关，属于LLM推理的系统优化和存储管理范畴。此外，它提到了硬件（Cambricon MLU370）和低带宽内存（LPDDR5-based），与HLS（High-Level Synthesis）或特定硬件优化方法有潜在关联。

太长不看（TLDR）：当在受随机访问带宽限制的加速器（RACM，如使用LPDDR5的加速器）上部署大型语言模型（LLM）时，当前的内存管理方法效率低下（静态预分配浪费内存，细粒度分页因高随机访问成本不适合）。本文提出ODMA，一种用于RACM的按需内存分配框架，它通过结合轻量级长度预测器、动态桶分区和大型桶保护机制，处理请求分布漂移和重尾请求。实验证明，ODMA显著提高了预测准确率（如从82.68%到93.36%），并将Cambricon MLU370上LLM服务的内存利用率从55.05%提高到72.45%，RPS和TPS分别提高了29%和27%。这表明硬件感知的分配是解锁RACM平台上高效LLM服务的关键。


<details>
  <summary>Details</summary>
Motivation: 现有的内存管理器（如静态预分配和细粒度分页PagingAttention）在随机访问受限的内存（RACM）加速器（如基于LPDDR5的加速器）上服务大型语言模型（LLMs）时效率低下。静态预分配浪费内存，而细粒度分页由于高随机访问成本而不适用。现有的以HBM为中心的解决方案未针对RACM的特性进行优化。

Method: ODMA通过结合轻量级长度预测器、动态桶分区和大型桶保护机制，来处理请求长度分布漂移和重尾请求。同时，它周期性地根据实时追踪数据更新分界，以最大化内存利用率。

Result: 在Alpaca和Google-NQ数据集上，ODMA显著提高了预测准确率（例如，从82.68%提高到93.36%）。在Cambricon MLU370-X4上服务DeepSeek-R1-Distill-Qwen-7B时，ODMA将内存利用率从55.05%提高到72.45%，并相对于静态基线将RPS和TPS分别提高了29%和27%。

Conclusion: ODMA证明了硬件感知的内存分配可以有效提升LLM在受随机访存带宽限制的加速器（RACM）上的服务效率。

Abstract: Serving large language models (LLMs) on accelerators with poor random-access bandwidth (e.g., LPDDR5-based) is limited by current memory managers. Static pre-allocation wastes memory, while fine-grained paging (e.g., PagedAttention) is ill-suited due to high random-access costs. Existing HBM-centric solutions do not exploit the characteristics of random-access-constrained memory (RACM) accelerators like Cambricon MLU370. We present ODMA, an on-demand memory allocation framework for RACM. ODMA addresses distribution drift and heavy-tailed requests by coupling a lightweight length predictor with dynamic bucket partitioning and a large-bucket safeguard. Boundaries are periodically updated from live traces to maximize utilization. On Alpaca and Google-NQ, ODMA improves prediction accuracy of prior work significantly (e.g., from 82.68% to 93.36%). Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA raises memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines. This demonstrates that hardware-aware allocation unlocks efficient LLM serving on RACM platforms.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Efficient MoE Serving in the Memory-Bound Regime: Balance Activated Experts, Not Tokens](https://arxiv.org/abs/2512.09277)
*Yanpeng Yu,Haiyue Ma,Krish Agarwal,Nicolai Oswald,Qijing Huang,Hugo Linsenmaier,Chunhui Mei,Ritchie Zhao,Ritika Borkar,Bita Darvish Rouhani,David Nellans,Ronny Krashinsky,Anurag Khandelwal*

Main category: cs.DC

TL;DR: 本文与**图处理**、**MLIR**、**编译器**、**HLS**和**DSL**无关，与**编译器**中的**优化**相关。本文提出 METRO (Minimum Expert Token ROuting) 算法来解决 MoE 专家并行服务在内存受限的解码阶段中，现有旨在平衡令牌数量的负载均衡策略反而降低性能的问题。METRO 转而平衡每 GPU 激活专家数量，减少内存压力。实验表明，METRO 相比基线 EPLB 可显著降低解码延迟 (11-22%) 并提高吞吐量 (3-21%)，甚至在固定 SLO 下可将解码吞吐量提升高达 4.11 倍。


<details>
  <summary>Details</summary>
Motivation: 现有的专家并行（EP）MoE 模型负载均衡方法通常旨在平衡每个 GPU 处理的令牌数量，以解决负载不均衡问题。然而，在 MoE 服务中，特别是在处理内存受限（memory-bound）的解码阶段时，作者发现这种以平衡令牌数量为目标的做法反而会降低性能。究其原因，平衡令牌数量会增加激活的专家数量，从而加剧了内存受限状态下的内存压力。因此，作者的动机在于提出一种新的负载均衡策略，能够有效地在内存受限的 MoE 服务中提升性能。

Method: 作者提出了 METRO (Minimum Expert Token ROuting) 算法，这是一种新颖的令牌路由算法，用于在内存受限（memory-bound）状态下实现高性能的专家并行 MoE 服务。METRO 的核心思想是平衡每个 GPU 上的激活专家数量，而不是令牌数量。为了保证路由质量，METRO 采用了一种新的 allGather 方案来收集全局的 top-k 知识，该方案与传统的 allToAll 相比具有最小的开销。同时，METRO 旨在通过联合优化算法效率和利用 GPU 的并行处理能力，以最小的计算开销实现接近最优的路由质量。

Result: 作者在真实系统（vLLM over 8 A100 GPUs）和专有模拟器（8-16 B200 GPUs）上评估了 METRO 算法，并将其与 EPLB 进行了比较。实验结果显示：
1. 在 Qwen3 和 DeepSeek-V3 服务中，当预填充和解码阶段协同部署时，METRO 相比 EPLB 降低了解码延迟 11% 到 22%，并提升了总令牌吞吐量 3% 到 21%。
2. 通过牺牲延迟余量来换取吞吐量时，在固定的解码服务等级目标（SLO）下，METRO 相比 EPLB 可将解码吞吐量提高高达 4.11 倍。
这些结果表明 METRO 在内存受限场景下的 MoE 服务中具有显著的性能优势。

Conclusion: 本文发现了在专家并行（EP）MoE 服务中，特别是在内存受限的解码阶段，现有旨在平衡每 GPU 令牌数量的负载均衡策略实际上会降低性能。作者提出了 METRO 算法，转而平衡每 GPU 的激活专家数量，并结合高效的路由和新颖的 allGather 方案。实验证明，METRO 在实际系统和模拟器上相对于 EPLB 显著降低了解码延迟并提升了整体吞吐量或解码吞吐量，特别是在内存受限场景下，展现出其在高性能 MoE 服务中的优越性。

Abstract: Expert Parallelism (EP) permits Mixture of Experts (MoE) models to scale beyond a single GPU. To address load imbalance across GPUs in EP, existing approaches aim to balance the number of tokens each GPU processes. Surprisingly, we find that this objective degrades performance rather than improving it when processing is memory-bound - a common occurrence in MoE serving, especially in the decode phase. Our analysis reveals that balancing the number of tokens processed per GPU increases the number of activated experts, exacerbating memory pressure in the memory-bound regime.
  We propose Minimum Expert Token ROuting, a novel token-routing algorithm for high-performance expert-parallel MoE serving in the memory-bound regime that balances the number of activated experts per GPU rather than token counts. METRO achieves near-optimal routing quality with minimal computational overhead by jointly optimizing algorithmic efficiency and leveraging the GPU's parallel processing power. To guarantee routing quality, METRO also employs a novel allGather scheme to gather global top-k knowledge, which has minimal overhead compared to conventional allToAll. Our evaluation of METRO against EPLB on both real systems (vLLM over 8 A100 GPUs) and a proprietary simulator (8-16 B200 GPUs) shows that METRO reduces decode latency by 11 - 22%, and total token throughput by 3 - 21% for Qwen3 and DeepSeek-V3 serving, where prefill and decode phases are co-deployed. In addition, by trading latency headroom for throughput, METRO improves decode throughput by up to 4.11x over EPLB at a fixed decode SLO.

</details>


### [5] [A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge](https://arxiv.org/abs/2512.09309)
*Zihao Ding,Mufeng Zhu,Zhongze Tang,Sheng Wei,Yao Liu*

Main category: cs.DC

TL;DR: 该论文与编译器、HLS、DSL、MLIR、图处理等均不直接相关。该论文提出了一种面向 ViTs 的分布式、分层卸载框架，通过利用本地边缘设备作为编排器，将视觉数据分割并分配给多个云服务器，解决了传统云卸载方案中的隐私泄露问题，并在 Segment Anything Model (SAM) 上验证了其在保持性能的同时增强隐私保护的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉智能工具计算要求高，超出了资源受限的移动和可穿戴设备的能力。传统的将视觉数据卸载到云端的解决方案，在传输和服务器端计算过程中存在显著的隐私漏洞。为了解决这些隐私挑战，本文提出了一个新颖的框架。

Method: 本文提出了一种分布式、分层卸载框架，用于 ViTs（Vision Transformers）。该方法利用本地可信边缘设备（如手机或 Nvidia Jetson）作为边缘编排器，将用户视觉数据分成较小的片段，并分发到多个独立的云服务器。通过这种方式，确保没有单个外部服务器拥有完整的图像，防止了数据的全面重建。最终的数据合并和聚合计算仅在用户的可信边缘设备上进行。以 Segment Anything Model (SAM) 为案例研究，演示了该方法的有效性。

Result: 应用该框架到 Segment Anything Model (SAM) 的案例研究表明，该方法相比传统的基于云的方法，显著增强了内容隐私。评估结果显示，该框架在保持接近基线分割性能的同时，显著降低了内容重建和用户数据暴露的风险。

Conclusion: 本文提出了一种面向 ViTs 的分布式、分层卸载框架，可以在边缘-云连续体中为视觉任务提供可扩展的、保护隐私的解决方案。该框架通过将用户数据分割并分配给多个独立的云服务器，确保没有单个服务器拥有完整的图像，从而显著增强了内容隐私，同时保持了接近基线的性能。

Abstract: Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.

</details>


### [6] [Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN](https://arxiv.org/abs/2512.09331)
*Nam Anh Dang,Ben Landrum,Ken Birman*

Main category: cs.DC

TL;DR: 该论文与图处理相关，因为它研究了在分布式向量搜索中如何构建和操作单个全局图（HNSW 等图结构）。它还与编译器和 HLS 的关系不大。
向量搜索是现代信息检索系统的基础。当数据集扩展到数十亿向量时，单服务器已无法满足需求，即使是基于磁盘的向量搜索也面临挑战。BatANN 是一种分布式磁盘近似最近邻（ANN）系统，它维护一个单个全局图以保留对数搜索效率，并通过在跨机器访问邻域时发送完整的查询状态来提高局部性，从而实现了接近线性的吞吐量扩展。在 10 台服务器上，BatANN 在 1 亿和 10 亿点数据集上，相对于基线系统，分别实现了 6.21-6.49 倍和 2.5-5.10 倍的吞吐量提升，同时将平均延迟保持在 6 毫秒以下。它是首个基于单个全局图的开源分布式磁盘向量搜索系统。


<details>
  <summary>Details</summary>
Motivation: 现代信息检索系统（如 RAG 和搜索引擎）依赖向量搜索，但随着数据集规模扩大到数十亿，即使是基于磁盘的向量搜索也面临挑战，因为数据可能超出一个单服务器的容量。因此，需要一个能够处理超大数据集、具备良好扩展性的分布式磁盘近似最近邻（ANN）系统。

Method: BatANN 通过在机器之间移动查询的完整状态（全查询状态迁移）来处理跨机器的邻域访问，以提高查询的局部性。这种方法允许查询在存储所需数据的机器上继续执行，从而优化了分布式环境下的近似最近邻（ANN）搜索性能。

Result: 在具有 0.95 召回率的 1 亿和 10 亿点数据集上，BatANN 在使用 10 台服务器时，相对于分散-聚集（scatter-gather）基线系统的吞吐量分别提高了 6.21-6.49 倍和 2.5-5.10 倍，同时平均延迟保持在 6 毫秒以下。这些结果是在标准 TCP 上实现的。

Conclusion: BatANN 是第一个基于单个全局图的开源分布式磁盘向量搜索系统，它在保持对数搜索效率的同时，实现了近乎线性的吞吐量扩展，并在千万级和十亿级数据集上，相对于基线系统展现出显著的性能优势。

Abstract: Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.

</details>


### [7] [WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving](https://arxiv.org/abs/2512.09472)
*Chiheng Lou,Sheng Qi,Rui Kang,Yong Zhang,Chen Sun,Pengcheng Wang,Bingyang Liu,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: 关联：该论文与**图处理 (GPU)**、**编译器 (系统)**相关，因为它涉及在共享GPU集群上部署和优化大型语言模型（LLM）的服务系统。

TLDR: 现有的多模型LLM服务系统在提高GPU利用率时，会牺牲首个令牌生成时间（TTFT），原因在于它们没有感知未来工作负载。然而，LLM工作负载是可预测的。本文提出了WarmServe系统，通过利用工作负载的可预知性设计**通用GPU工作器**实现“一替多”预热，并结合**驱逐感知模型放置**、**主动预热**和**零开销内存切换**机制来优化GPU内存管理和预热干扰。实验表明，WarmServe相较于现有系统，TTFT最高提升50.8倍，请求服务能力提升2.5倍。


<details>
  <summary>Details</summary>
Motivation: 现有的多模型LLM服务系统在提高GPU资源利用率的同时，牺牲了推理性能，特别是首个令牌生成时间（TTFT）的性能。这种折衷的根本原因在于它们对未来工作负载特性缺乏感知。然而，最近的分析表明LLM服务工作负载具有高度的周期性和长期可预测性，这激发了可以利用这些特性来优化性能的系统设计。

Method:  WarmServe的核心方法是利用LLM服务负载的周期性和可预测性，通过“一替多”的GPU预热方法来提前加载模型，从而避免TTFT恶化。具体实施包括：1. **通用GPU工作器（Universal GPU Workers）**：用于实现预热，提前加载未来可能需要的模型。2. **驱逐感知模型放置策略（Evict-aware Model Placement Strategy）**：缓解集群范围内的预热干扰。3. **主动预热（Proactive Prewarming）**：提前准备通用GPU工作器。4. **零开销内存切换机制（Zero-overhead Memory Switching Mechanism）**：高效管理GPU内存。

Result: WarmServe在真实世界数据集上的评估结果显示：1. 相比于最先进的基于自动扩展的系统，WarmServe将TTFT提高了高达50.8倍。2. 相比于GPU共享系统，WarmServe能够服务多达2.5倍的请求。这表明WarmServe成功地在不牺牲性能的前提下，显著提高了资源效率和推理性能。

Conclusion: WarmServe通过利用LLM服务工作负载的长期可预测性，并设计了包括通用GPU工作器、驱逐感知模型放置策略、主动预热以及零开销内存切换机制在内的系统，成功地在多模型LLM服务系统中解决了现有方案中GPU利用率和TTFT之间的冲突。实验结果表明，WarmServe在TTFT和请求吞吐量方面均显著优于现有最先进的系统。

Abstract: Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.
  We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\times$ more requests compared to the GPU-sharing system.

</details>


### [8] [PHWSOA: A Pareto-based Hybrid Whale-Seagull Scheduling for Multi-Objective Tasks in Cloud Computing](https://arxiv.org/abs/2512.09568)
*Zhi Zhao,Hang Xiao,Wei Rang*

Main category: cs.DC

TL;DR: 该论文与编译器、DSL、MLIR、或HLS无关，但是与图处理中的优化算法有关（WOA/SOA/PHWSOA）。

本文提出了一种基于Pareto的混合鲸鱼-海鸥优化算法（PHWSOA），用于云计算任务调度中的多目标优化。PHWSOA结合了鲸鱼优化算法（WOA）和海鸥优化算法（SOA）的优势，并利用Pareto支配原理同时优化完工时间、VM负载均衡和经济成本。通过引入Halton序列初始化、Pareto指导的变异和动态VM负载再分配等增强功能，PHWSOA在真实工作负载轨迹上的实验表现出显著优于现有基线算法的性能，实现了高达72.1%的完工时间缩减、36.8%的VM负载均衡改善和23.5%的成本节省，展示了其在高效云资源管理中的强大潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的云计算任务调度解决方案大多只优化单一或有限指标，例如执行时间或资源利用率，而忽略了对完工时间、虚拟机负载均衡和经济成本等多个关键目标进行全面多目标优化的需求。本文旨在弥补这一差距。

Method: 本文提出了基于Pareto的混合鲸鱼-海鸥优化算法（PHWSOA）。该算法结合了鲸鱼优化算法（WOA）和海鸥优化算法（SOA）的优点，以弥补WOA在局部开发和SOA在全局探索中的不足。PHWSOA利用Pareto支配原理，同时优化完工时间（makespan）、虚拟机（VM）负载均衡和经济成本三个关键目标。核心增强包括：使用Halton序列初始化以提高种群多样性；引入Pareto指导的变异机制以避免过早收敛；采用并行处理以加速收敛。此外，还集成了动态VM负载再分配机制以改进任务执行期间的负载均衡。实验在CloudSim模拟器上，使用来自NASA-iPSC和HPC2N的真实工作负载轨迹进行。

Result: 在CloudSim模拟器上使用真实工作负载轨迹进行的广泛实验表明，PHWSOA带来了显著的性能提升。具体而言，它在完工时间方面实现了高达72.1%的减少，在VM负载均衡方面提升了36.8%，在经济成本方面节省了23.5%。这些结果显著优于包括WOA、GA、PEWOA和GCWOA在内的基线方法。

Conclusion: PHWSOA算法通过结合WOA和SOA的优势，并引入Pareto支配、Halton序列初始化、Pareto指导的变异机制、并行处理和动态VM负载再分配等增强功能，成功解决了云计算任务调度中的多目标优化问题。实验证明，PHWSOA在缩短完工时间、改善VM负载均衡和降低经济成本方面显著优于现有基线方法，展现了其在实际云环境中实现高效资源管理的巨大潜力。

Abstract: Task scheduling is a critical research challenge in cloud computing, a transformative technology widely adopted across industries. Although numerous scheduling solutions exist, they predominantly optimize singular or limited metrics such as execution time or resource utilization often neglecting the need for comprehensive multi-objective optimization. To bridge this gap, this paper proposes the Pareto-based Hybrid Whale-Seagull Optimization Algorithm (PHWSOA). This algorithm synergistically combines the strengths of the Whale Optimization Algorithm (WOA) and the Seagull Optimization Algorithm (SOA), specifically mitigating WOA's limitations in local exploitation and SOA's constraints in global exploration. Leveraging Pareto dominance principles, PHWSOA simultaneously optimizes three key objectives: makespan, virtual machine (VM) load balancing, and economic cost. Key enhancements include: Halton sequence initialization for superior population diversity, a Pareto-guided mutation mechanism to avert premature convergence, and parallel processing for accelerated convergence. Furthermore, a dynamic VM load redistribution mechanism is integrated to improve load balancing during task execution. Extensive experiments conducted on the CloudSim simulator, utilizing real-world workload traces from NASA-iPSC and HPC2N, demonstrate that PHWSOA delivers substantial performance gains. Specifically, it achieves up to a 72.1% reduction in makespan, a 36.8% improvement in VM load balancing, and 23.5% cost savings. These results substantially outperform baseline methods including WOA, GA, PEWOA, and GCWOA underscoring PHWSOA's strong potential for enabling efficient resource management in practical cloud environments.

</details>


### [9] [SynthPix: A lightspeed PIV images generator](https://arxiv.org/abs/2512.09664)
*Antonio Terpin,Alan Bonomi,Francesco Banelli,Raffaello D'Andrea*

Main category: cs.DC

TL;DR: Paper is related to **DSL** (JAX's functional programming/compiler optimization aspects, though not a general-purpose DSL paper) and **Compiler** (JAX relies on XLA compiler for performance). **Graph Processing** and **MLIR** and **HLS** are not directly related.
SynthPix是一个基于JAX实现的、专注于在加速器上实现高性能和并行化的高吞吐量粒子图像测速（PIV）合成图像生成器。其旨在通过比现有工具高数个数量级的图像对生成速度，来满足数据密集型强化学习流场估计方法的训练需求，并缩短开发用于实时PIV反馈的主动流体控制方法的迭代时间。


<details>
  <summary>Details</summary>
Motivation: 现有的粒子图像测速（PIV）方法需要大量数据，尤其是对于数据密集型的强化学习流场估计方法，以及在需要实时 PIV 反馈的主动流体控制研究中，开发快速流场估计方法时需要缩短迭代时间。因此，需要一个高性能、高吞吐量的合成图像生成器（如 SynthPix）来满足这些需求。

Method: 论文描述了 SynthPix 软件背后的主要思想和实现细节。它是一个在 JAX 中实现的、关注性能和并行性的合成图像生成器，能够支持与现有工具相同的配置参数，但具有更高的吞吐量。

Result: SynthPix 实现了与现有工具相同的配置参数支持，但在图像对生成吞吐量上实现了比现有工具高出数个数量级的提升，极大地加速了数据生成和方法开发迭代过程。

Conclusion: SynthPix 是一个针对 PIV 的高性能合成图像生成器，基于 JAX 实现，注重在加速器上的性能和并行性。它可以实现比现有工具高出数个数量级的图像对生成吞吐量，这对于训练数据密集型强化学习模型以及加速实时 PIV 反馈下的快速流场估计方法的开发至关重要。作者相信 SynthPix 对流体动力学社区具有重要价值。

Abstract: We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.

</details>


### [10] [Straggler Tolerant and Resilient DL Training on Homogeneous GPUs](https://arxiv.org/abs/2512.09685)
*Zeyu Zhang,Haiying Shen*

Main category: cs.DC

TL;DR: 这个论文与编译器、HLS、DSL、MLIR 不相关，与图处理不直接相关。它与**深度学习系统**相关。
**太长不看（TLDR）：**该论文研究了同构 GPU 深度学习训练中落后者（stragglers）的普遍性、原因（CPU 和带宽不平衡）以及现有异步 SGD 缓解方案的局限性。作者提出了一个名为 STAR 的深度学习训练系统，该系统引入了新的同步模式、基于启发式和 ML 的模式选择策略，以及主动的资源预防机制，以容忍和减轻落后者。在 AWS 上的评估显示，STAR 在保持同步 SGD 精度的情况下，相对于现有技术显著降低了 TTA（Time-To-Accuracy）。


<details>
  <summary>Details</summary>
Motivation: 尽管基于同构 GPU 的深度学习（DL）训练很流行，但人们对该场景下落后者的普遍性、起因、影响以及现有缓解方法的有效性知之甚少。现有研究不足以解决这些问题。本文旨在填补这一空白，通过提出一种名为 STAR 的系统来有效容忍和减轻 DL 训练中的落后者，从而显著降低 TTA 并保持同步 SGD 的收敛精度。

Method: 本文首先通过实验研究了落后者在同构 GPU 深度学习训练中的普遍性、原因（CPU 和带宽使用不平衡）以及影响，并分析了现有缓解方法（如异步 SGD）的局限性。在此基础上，提出了 STAR 系统。STAR 的核心机制包括：1）新的同步模式，通过对工作节点分组进行参数更新；2）选择最佳同步模式的启发式和 ML 方法，以最小化 TTA；3）主动资源预防机制，通过避免对 CPU 和带宽资源过载，从而防止落后者，尤其是在参数服务器（PS）分配和梯度传输中。最后，通过在 AWS 上进行跟踪驱动的评估来验证 STAR 的性能。

Result: 研究结果表明，落后者在同构 GPU 深度学习训练中仍然普遍存在，主要是由于 CPU 和带宽使用不平衡。现有的缓解方法（如从 SSGD 切换到 ASGD）可能不能改善 TTA，甚至会因为更高的资源消耗而产生更多的落后者。本文提出的 STAR 系统通过新的同步模式、智能模式选择（启发式和 ML）以及主动的资源预防策略，在 AWS 上对 PS 架构的 TTA 降低了 48% 到 84%，对 All-Reduce 架构的 TTA 降低了 51% 到 70%，并保持了 SSGD 的收敛精度。

Conclusion: 本文分析了在同构 GPU 深度学习训练中，现有解决方案在解决落后者问题上的局限性，并提出了 STAR 系统及其核心机制（新的同步模式、启发式和 ML 模式选择以及主动资源预防），证明了 STAR 在 AWS 上的实际性能优势。因此，STAR 是一个有效容忍和减轻深度学习训练中落后者问题的系统。

Abstract: Despite the popularity of homogeneous GPU-based deep learning (DL) training, the prevalence, causes and impact of stragglers and the effectiveness of existing straggler mitigation approaches are still not well understood in this scenario due to limited research on these questions. To fill this gap, we conducted comprehensive experiments and found that stragglers remain widespread due to CPU and bandwidth usage imbalances. Additionally, existing mitigation methods that switch from synchronous stochastic gradient descent (SSGD) to asynchronous SGD (ASGD) may not improve Time-To-Accuracy (TTA) and can even generate more stragglers due to its higher resource consumption. To address these newly found problems, we propose the Straggler Tolerant And Resilient DL training system (STAR). STAR includes new synchronization modes that group workers for each parameter updating. It has a heuristic and an ML method to choose the optimal synchronization mode for minimizing TTA, and reallocates resources to support the selected mode while minimizing the impact on co-located jobs. Moreover, it proactively prevents stragglers by avoiding overloading the CPU and bandwidth resources in allocating PSs (which consume high CPU and bandwidth) and in gradient transmission. Our trace-driven evaluation on AWS shows that STAR generates 48-84% and 51-70% lower TTA than state-of-the-art systems in the PS and all-reduce architectures, respectively, while maintaining the converged accuracy of SSGD. The code for STAR is open-sourced.

</details>


### [11] [Recoverable Lock-Free Locks](https://arxiv.org/abs/2512.09710)
*Hagit Attiya,Panagiota Fatourou,Eleftherios Kosmas,Yuanhao Wei*

Main category: cs.DC

TL;DR: 相关性：该论文与DSL、图处理、MLIR、编译器或HLS不直接相关。它属于并发编程/操作系统、并行计算或系统软件领域，聚焦于并发控制机制（锁、无锁、可恢复性）的转换。

太长不读（TLDR）摘要：本文提出了首个能将基于锁的实现转换为**同时具有无锁（Lock-Free）和可恢复性**的新型转换方法。它通过替换锁的获取和释放操作，支持嵌套锁，并在不牺牲原实现正确性的前提下确保系统可从故障中恢复。


<details>
  <summary>Details</summary>
Motivation: 首次提出一种能够同时实现无锁和可恢复性的转换机制。

Method: 提出了一种从基于锁的实现开始的转换方法，该方法提供了可恢复的、无锁的替代方案来处理锁的获取和释放操作。

Result: 该转换支持嵌套锁以提升通用性，并能在保证被应用（基于锁）实现正确性的前提下，实现可恢复性。

Conclusion: 本文介绍了第一个同时引入无锁（lock-freedom）和可恢复性（recoverability）的转换方法。该转换将基于锁的实现作为起点，提供了一种可恢复、无锁的方式来替代锁的获取和释放操作。这种转换支持嵌套锁以增强通用性，并能确保可恢复性，同时不损害其所应用的基于锁的实现的正确性。

Abstract: This paper presents the first transformation that introduces both lock-freedom and recoverability. Our transformation starts with a lock-based implementation, and provides a recoverable, lock-free substitution to lock acquire and lock release operations. The transformation supports nested locks for generality and ensures recoverability without jeopardising the correctness of the lock-based implementation it is applied on.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [12] [Almost-Optimal Approximation Algorithms for Global Minimum Cut in Directed Graphs](https://arxiv.org/abs/2512.09080)
*Ron Mosenzon*

Main category: cs.DS

TL;DR: This paper is not related to DSL or MLIR or HLS or compiler. The paper is related to **graph processing** (minimum cut problems in directed graphs).

TLDR: The paper presents new randomized $(1+ε)$-approximation algorithms for finding the global minimum edge-cut and vertex-cut in directed weighted graphs. These algorithms achieve an almost-optimal running time of $O\left(m^{1+o(1)}/ε\right)$, significantly improving upon previous results. The techniques build on and extend existing frameworks, and a novel black-box reduction is developed to relate the global minimum vertex-cut problem to its rooted variant.


<details>
  <summary>Details</summary>
Motivation: 本文的动机是寻找有向加权图（包括边加权图的全局最小边割和点加权图的全局最小点割）的更快速的近似算法。已有的最快算法在最小边割问题上的运行时间为 $\tilde{O}(\min\{n^2/\epsilon^2,m^{1+o(1)}\sqrt{n}\})$，在最小点割问题上为 $\tilde{O}(n^2/\epsilon^2)$。本文旨在改进这些复杂度，达到更接近最优的 $O(m^{1+o(1)}/\epsilon)$ 运行时间。同时，本文还希望将其结果扩展到带根版本的最小割问题，并为全局最小点割问题提供更通用的归约方法，解决之前仅适用于更受限设置（如单位点权）的归约方法的局限性。

Method: 本文开发了新的求解有向加权图的全局最小边割和全局最小点割的 $(1+ε)$-近似随机算法。算法借鉴并扩展了 Chuzhoy 等人（SODA 2026） 为解决无权有向图最小点割问题引入的框架。此外，为了解决全局最小点割问题，本文提出了一个新的黑盒归约方法，将该问题归约到其带根版本。

Result: 本文提出了求解有向加权图全局最小边割和全局最小点割的 $(1+ε)$-近似随机算法，其运行时间为 $O\left(m^{1+o(1)}/ε\right)$，在多项式有界权重下，对于常数 $ε>0$，达到了近乎最优的 $O\left(m^{1+o(1)}\right)$ 复杂度。这显著优于之前最快的算法。此外，这些结果可以扩展到带根版本的最小边割和最小点割问题。本文还提出了一个 novel 的黑盒归约方法，将全局最小点割问题归约到其带根版本，这在以前仅适用于更受限的设置（如单位点权）。

Conclusion: 本文介绍了求解有向图全局最小边割和全局最小点割的 $(1+ε)$-近似算法。这些算法在多项式有界权重下，运行时间复杂度达到了近乎最优的 $O(m^{1+o(1)}/\epsilon)$。此外，本文还提出了一个将全局最小点割问题归约到其带根版本的新的黑盒归约方法，为解决更一般的图切割问题提供了新的工具和思路。

Abstract: We develop new $(1+ε)$-approximation algorithms for finding the global minimum edge-cut in a directed edge-weighted graph, and for finding the global minimum vertex-cut in a directed vertex-weighted graph. Our algorithms are randomized, and have a running time of $O\left(m^{1+o(1)}/ε\right)$ on any $m$-edge $n$-vertex input graph, assuming all edge/vertex weights are polynomially-bounded. In particular, for any constant $ε>0$, our algorithms have an almost-optimal running time of $O\left(m^{1+o(1)}\right)$. The fastest previously-known running time for this setting, due to (Cen et al., FOCS 2021), is $\tilde{O}\left(\min\left\{n^2/ε^2,m^{1+o(1)}\sqrt{n}\right\}\right)$ for Minimum Edge-Cut, and $\tilde{O}\left(n^2/ε^2\right)$ for Minimum Vertex-Cut. Our results further extend to the rooted variants of the Minimum Edge-Cut and Minimum Vertex-Cut problems, where the algorithm is additionally given a root vertex $r$, and the goal is to find a minimum-weight cut separating any vertex from the root $r$. In terms of techniques, we build upon and extend a framework that was recently introduced by (Chuzhoy et al., SODA 2026) for solving the Minimum Vertex-Cut problem in unweighted directed graphs. Additionally, in order to obtain our result for the Global Minimum Vertex-Cut problem, we develop a novel black-box reduction from this problem to its rooted variant. Prior to our work, such reductions were only known for more restricted settings, such as when all vertex-weights are unit.

</details>


### [13] [Dynamic Graph Coloring: Sequential, Parallel, and Distributed](https://arxiv.org/abs/2512.09218)
*Mohsen Ghaffari,Jaehyun Koo*

Main category: cs.DS

TL;DR: 本文涉及图处理（Graph Processing）。
该论文提出了一个简单的随机算法，用于在图经历边插入和删除（动态图）时高效地维护 $(\Delta+1)$ 染色。该算法提供了一个统一的框架，在顺序设定中，它以 $O(1)$ 预期的最坏情况时间处理每个更新，改进了以往的摊还结果。在并行设定中，它以每个更新 $O(1)$ 预期工作量和 $\text{poly}(\log n)$ 深度处理任意大小的更新批次。在分布式设定中，它在每次更新后以 $O(\log n)$ 轮和 $O(1)$ 预期的消息/计算复杂度快速恢复正确的着色。


<details>
  <summary>Details</summary>
Motivation: 动态图着色是一个重要的理论和应用问题，要求在图结构发生变化（边插入/删除）时，保持图的有效着色。以前的工作（如 Bhattacharya 等人的 SODA'18、TALG 2022）已经取得了 $O(1)$ 摊还时间或 $O(\log \Delta)$ 摊还时间的结果，但缺乏一个统一的框架来高效地处理顺序、并行和分布式模型中的批量更新，特别是缺乏在最坏情况下具有 $O(1)$ 预期时间复杂度的算法。本文的动机是设计一个简单、高效的随机算法，提供一个统一的框架，同时在所有模型中改进或匹配现有最佳结果，特别是在顺序模型中实现 $O(1)$ 预期的最坏情况时间复杂度和在并行/分布式模型中高效处理批量更新。

Method: 本文提出了一种“简单”的随机算法（SODA'18 工作的变体，但具有更强的界限），该算法利用其同时处理多个更新的能力，构建了一个统一的框架，特别适用于并行和分布式模型。核心方法是基于随机化的维护策略，旨在在更新发生时保持图的 $(\Delta+1)$ 染色。

Result: - **顺序设定：** 算法以 $O(1)$ 预期的最坏情况时间处理每个更新，匹配并提高了现有结果（如 Henzinger 和 Peng 的 TALG 2022），后者仅实现了摊还结果。
- **并行设定：** 算法以每个更新 $O(1)$ 预期的工作量处理任意大小的更新批次，并以 $\text{poly}(\log n)$ 的深度（高概率）完成。这被认为是现有结果的“理想”并行化。
- **分布式设定：** 算法在每次插入和删除后仅导致至多 $O(1)$ 个节点变为未着色，并在 $O(\log n)$ 轮内（高概率）收敛到一个完整的、正确的着色。重要的是，算法的预期消息复杂度为 $O(1)$，每次更新的计算复杂度也为 $O(1)$。

Conclusion: 本文提出了一个统一的随机算法框架，用于在图经历边插入和删除更新时（即动态图）高效地维护一个 $(\Delta+1)$ 染色。该算法在顺序、并行和分布式模型中都取得了最先进的结果，尤其是在顺序模型中实现了每更新 $O(1)$ 预期的最坏情况时间复杂度，并在并行和分布式模型中展示了高效的批量更新处理能力和较低的通信/计算开销，为动态图着色问题提供了一个统一且高效的解决方案。

Abstract: We present a simple randomized algorithm that can efficiently maintain a $(Δ+1)$ coloring as the graph undergoes edge insertion and deletion updates, where $Δ$ denotes an upper bound on the maximum degree. A key advantage is the algorithm's ability to process many updates simultaneously, which makes it naturally adaptable to the parallel and distributed models. Concretely, it gives a unified framework across the models, leading to the following results:
  - In the sequential setting, the algorithm processes each update in $O(1)$ expected time, worst-case. This matches and strengthens the results of Henzinger and Peng [TALG 2022] and Bhattacharya et al. [TALG 2022], who achieved an $O(1)$ bound but amortized (in expectation and with high probability, respectively), whose work was an improvement of the $O(\log Δ)$ expected amortized bound of Bhattacharya et al. [SODA'18].
  - In the parallel setting, the algorithm processes each (arbitrary size) batch of updates using $O(1)$ work per update in the batch in expectation, and in $\text{poly}(\log n)$ depth with high probability. This is, in a sense, an ideal parallelization of the above results.
  - In the distributed setting, the algorithm can maintain a coloring of the network graph as (potentially many) edges are added or deleted. The maintained coloring is always proper; it may become partial upon updates, i.e., some nodes may temporarily lose their colors, but quickly converges to a full, proper coloring. Concretely, each insertion and deletion causes at most $O(1)$ nodes to become uncolored, but this is resolved within $O(\log n)$ rounds with high probability (e.g., in the absence of further updates nearby--the precise guarantee is stronger, but technical). Importantly, the algorithm incurs only $O(1)$ expected message complexity and computation per update.

</details>
