{"id": "2512.21596", "categories": ["cs.PL", "cs.FL", "cs.LG", "cs.LO", "cs.SC"], "pdf": "https://arxiv.org/pdf/2512.21596", "abs": "https://arxiv.org/abs/2512.21596", "authors": ["Peixin Wang", "Jianhao Bai", "Min Zhang", "C. -H. Luke Ong"], "title": "Quantitative Verification of Omega-regular Properties in Probabilistic Programming", "comment": null, "summary": "Probabilistic programming provides a high-level framework for specifying statistical models as executable programs with built-in randomness and conditioning. Existing inference techniques, however, typically compute posterior distributions over program states at fixed time points, most often at termination, thereby failing to capture the temporal evolution of probabilistic behaviors. We introduce temporal posterior inference (TPI), a new framework that unifies probabilistic programming with temporal logic by computing posterior distributions over execution traces that satisfy omega-regular specifications, conditioned on possibly temporal observations. To obtain rigorous quantitative guarantees, we develop a new method for computing upper and lower bounds on the satisfaction probabilities of omega-regular properties. Our approach decomposes Rabin acceptance conditions into persistence and recurrence components and constructs stochastic barrier certificates that soundly bound each component. We implement our approach in a prototype tool, TPInfer, and evaluate it on a suite of benchmarks, demonstrating effective and efficient inference over rich temporal properties in probabilistic models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e0e **DSL (Probabilistic Programming)**\u3001**Graph Processing (Temporal evolution/traces)**\u3001**Compiler (Executable programs)**\u3001**HLS (N/A)**\u3001**MLIR (N/A)** \u76f8\u5173\u3002\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u65f6\u95f4\u540e\u9a8c\u63a8\u7406\u201d\uff08TPI\uff09\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u5c06\u6982\u7387\u7f16\u7a0b\u4e0e\u65f6\u95f4\u903b\u8f91\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u8ba1\u7b97\u6ee1\u8db3 $\\omega$-regular \u89c4\u8303\u7684\u7a0b\u5e8f\u6267\u884c\u8f68\u8ff9\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u6982\u7387\u884c\u4e3a\u65f6\u95f4\u6f14\u5316\u7684\u95ee\u9898\u3002\u4e3a\u63d0\u4f9b\u4e25\u683c\u7684\u5b9a\u91cf\u4fdd\u8bc1\uff0cTPI \u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3 Rabin \u63a5\u53d7\u6761\u4ef6\u5e76\u6784\u5efa\u968f\u673a\u969c\u788d\u8bc1\u4e66\u6765\u8ba1\u7b97 $\\omega$-regular \u5c5e\u6027\u6ee1\u8db3\u6982\u7387\u7684\u4e0a\u754c\u548c\u4e0b\u754c\u3002\u8be5\u65b9\u6cd5\u5728\u539f\u578b\u5de5\u5177 TPInfer \u4e2d\u5f97\u5230\u5b9e\u73b0\uff0c\u5e76\u88ab\u8bc1\u660e\u5728\u6982\u7387\u6a21\u578b\u7684\u65f6\u95f4\u5c5e\u6027\u63a8\u7406\u4e2d\u662f\u6709\u6548\u4e14\u9ad8\u6548\u7684\u3002", "motivation": "\u73b0\u6709\u7684\u6982\u7387\u7f16\u7a0b\u63a8\u7406\u6280\u672f\u901a\u5e38\u53ea\u8ba1\u7b97\u56fa\u5b9a\u65f6\u95f4\u70b9\uff08\u901a\u5e38\u662f\u7a0b\u5e8f\u7ec8\u6b62\u65f6\uff09\u7a0b\u5e8f\u72b6\u6001\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u56e0\u6b64\u65e0\u6cd5\u6355\u6349\u6982\u7387\u884c\u4e3a\u7684\u65f6\u95f4\u6f14\u5316\u3002\u8fd9\u79cd\u5c40\u9650\u6027\u4fc3\u4f7f\u4f5c\u8005\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5bf9\u7a0b\u5e8f\u6267\u884c\u7684**\u65f6\u95f4\u5c5e\u6027**\u8fdb\u884c\u63a8\u7406\u7684\u65b0\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u65f6\u95f4\u540e\u9a8c\u63a8\u7406\uff08TPI\uff09\u6846\u67b6\uff0c\u5b83\u5c06\u6982\u7387\u7f16\u7a0b\u4e0e\u65f6\u95f4\u903b\u8f91\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u8ba1\u7b97\u5728\u53ef\u80fd\u7684\u65f6\u95f4\u89c2\u6d4b\u6761\u4ef6\u4e0b\u6ee1\u8db3 $\\omega$-regular \u89c4\u8303\u7684\u6267\u884c\u8f68\u8ff9\u7684\u540e\u9a8c\u5206\u5e03\u3002\u4e3a\u4e86\u83b7\u5f97\u4e25\u683c\u7684\u5b9a\u91cf\u4fdd\u8bc1\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u79cd\u8ba1\u7b97 $\\omega$-regular \u5c5e\u6027\u6ee1\u8db3\u6982\u7387\u4e0a\u754c\u548c\u4e0b\u754c\u7684\u65b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5c06 Rabin \u63a5\u53d7\u6761\u4ef6\u5206\u89e3\u4e3a\u6301\u7eed\u6027\uff08persistence\uff09\u548c\u5faa\u73af\uff08recurrence\uff09\u7ec4\u4ef6\uff0c\u5e76\u6784\u5efa\u968f\u673a\u969c\u788d\u8bc1\u4e66\u6765\u53ef\u9760\u5730\u7ea6\u675f\u6bcf\u4e2a\u7ec4\u4ef6\u3002", "result": "\u4f5c\u8005\u5728\u539f\u578b\u5de5\u5177 TPInfer \u4e2d\u5b9e\u73b0\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5bf9\u6982\u7387\u6a21\u578b\u4e2d\u7684\u4e30\u5bcc\u65f6\u95f4\u5c5e\u6027\u8fdb\u884c\u6709\u6548\u4e14\u9ad8\u6548\u7684\u63a8\u7406\u3002\u8ba1\u7b97 $\\omega$-regular \u5c5e\u6027\u6ee1\u8db3\u6982\u7387\u4e0a\u754c\u548c\u4e0b\u754c\u7684\u65b0\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u4e25\u683c\u7684\u5b9a\u91cf\u4fdd\u8bc1\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u65f6\u95f4\u540e\u9a8c\u63a8\u7406\uff08TPI\uff09\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u8ba1\u7b97\u6ee1\u8db3 $\\omega$-regular \u89c4\u8303\u5e76\u5728\u53ef\u80fd\u7684\u65f6\u95f4\u89c2\u6d4b\u6761\u4ef6\u4e0b\u7f16\u7a0b\u6267\u884c\u8f68\u8ff9\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6982\u7387\u7f16\u7a0b\u63a8\u7406\u6280\u672f\u65e0\u6cd5\u6355\u6349\u6982\u7387\u884c\u4e3a\u65f6\u95f4\u6f14\u5316\u7684\u95ee\u9898\u3002TPI \u4e3a\u6982\u7387\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u6355\u6349\u4e30\u5bcc\u65f6\u95f4\u5c5e\u6027\u7684\u6709\u6548\u4e14\u9ad8\u6548\u7684\u63a8\u7406\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u65b0\u7684\u6709\u754c\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u5b9a\u91cf\u4fdd\u8bc1\u3002"}}
{"id": "2512.21340", "categories": ["cs.DC", "cs.DB", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21340", "abs": "https://arxiv.org/abs/2512.21340", "authors": ["Dimitrios Amaxilatis", "Themistoklis Sarantakos", "Nikolaos Tsironis", "Souvik Sengupta", "Kostas Ramantas", "Jhofre Ojeda"], "title": "Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum", "comment": null, "summary": "Smart cities are increasingly adopting data-centric architectures to enhance the efficiency, sustainability, and resilience of urban services.", "AI": {"tldr": "\u672c\u6587\u4e0eDSL\u3001\u56fe\u5904\u7406\u3001MLIR\u3001\u7f16\u8bd1\u5668\u3001HLS\u7b49\u6280\u672f\u9886\u57df\u65e0\u5173\u3002\n\n\u667a\u6167\u57ce\u5e02\u6b63\u91c7\u7eb3\u6570\u636e\u4e2d\u5fc3\u67b6\u6784\uff0c\u4ee5\u589e\u5f3a\u57ce\u5e02\u670d\u52a1\u7684\u6548\u7387\u3001\u53ef\u6301\u7eed\u6027\u548c\u6062\u590d\u529b\u3002", "motivation": "\u667a\u6167\u57ce\u5e02\u5bfb\u6c42\u63d0\u5347\u57ce\u5e02\u670d\u52a1\u7684\u6548\u7387\u3001\u53ef\u6301\u7eed\u6027\u548c\u6062\u590d\u529b\uff0c\u4ee5\u6b64\u4fc3\u8fdb\u8fdb\u6b65\u548c\u53d1\u5c55\u3002\u56e0\u6b64\uff0c\u91c7\u7528\u6570\u636e\u4e2d\u5fc3\u67b6\u6784\u6210\u4e3a\u9a71\u52a8\u8fd9\u4e9b\u63d0\u5347\u7684\u5173\u952e\u52a8\u673a\u3002", "method": "\u62bd\u8c61\u5c42\u9762\u4e0a\uff0c\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u91c7\u7528\u6570\u636e\u4e2d\u5fc3\u67b6\u6784\u6765\u652f\u6301\u667a\u6167\u57ce\u5e02\u7684\u53d1\u5c55\u3002\u8fd9\u79cd\u67b6\u6784\u7684\u5b9e\u65bd\u5c06\u662f\u63d0\u5347\u57ce\u5e02\u670d\u52a1\u6548\u7387\u7684\u5173\u952e\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5728\u667a\u6167\u57ce\u5e02\u4e2d\u91c7\u7528\u6570\u636e\u4e2d\u5fc3\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u57ce\u5e02\u670d\u52a1\u7684\u6548\u7387\u3001\u53ef\u6301\u7eed\u6027\u548c\u6062\u590d\u529b\u7684\u589e\u5f3a\u3002\u5177\u4f53\u7684\u589e\u5f3a\u7a0b\u5ea6\u548c\u5f71\u54cd\u8303\u56f4\u672a\u5728\u6458\u8981\u4e2d\u8be6\u7ec6\u8bf4\u660e\u3002", "conclusion": "\u8003\u8651\u5230\u6458\u8981\u7b80\u77ed\u4e14\u7f3a\u4e4f\u5177\u4f53\u7ec6\u8282\uff0c\u5bf9\u4e8e\u57ce\u5e02\u670d\u52a1\u7ba1\u7406\u6548\u7387\u3001\u53ef\u6301\u7eed\u6027\u548c\u6062\u590d\u529b\u7684\u63d0\u5347\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6570\u636e\u4e2d\u5fc3\u67b6\u6784\u7684\u5177\u4f53\u5b9e\u73b0\u65b9\u5f0f\u53ca\u5176\u5bf9\u667a\u6167\u57ce\u5e02\u5404\u4e2a\u9886\u57df\u7684\u5f71\u54cd\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5e94\u7740\u91cd\u4e8e\u63a2\u7d22\u5982\u4f55\u6700\u4f18\u5316\u5730\u5229\u7528\u8fd9\u4e9b\u67b6\u6784\uff0c\u4ee5\u5e94\u5bf9\u667a\u6167\u57ce\u5e02\u53d1\u5c55\u4e2d\u6240\u9762\u4e34\u7684\u6311\u6218\u3002"}}
{"id": "2512.21499", "categories": ["cs.DS", "cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.21499", "abs": "https://arxiv.org/abs/2512.21499", "authors": ["Christian Janos Lebeda", "Aleksandar Nikolov", "Haohua Tang"], "title": "Weighted Fourier Factorizations: Optimal Gaussian Noise for Differentially Private Marginal and Product Queries", "comment": null, "summary": "We revisit the task of releasing marginal queries under differential privacy with additive (correlated) Gaussian noise. We first give a construction for answering arbitrary workloads of weighted marginal queries, over arbitrary domains. Our technique is based on releasing queries in the Fourier basis with independent noise with carefully calibrated variances, and reconstructing the marginal query answers using the inverse Fourier transform. We show that our algorithm, which is a factorization mechanism, is exactly optimal among all factorization mechanisms, both for minimizing the sum of weighted noise variances, and for minimizing the maximum noise variance. Unlike algorithms based on optimizing over all factorization mechanisms via semidefinite programming, our mechanism runs in time polynomial in the dataset and the output size. This construction recovers results of Xiao et al. [Neurips 2023] with a simpler algorithm and optimality proof, and a better running time.\n  We then extend our approach to a generalization of marginals which we refer to as product queries. We show that our algorithm is still exactly optimal for this more general class of queries. Finally, we show how to embed extended marginal queries, which allow using a threshold predicate on numerical attributes, into product queries. We show that our mechanism is almost optimal among all factorization mechanisms for extended marginals, in the sense that it achieves the optimal (maximum or average) noise variance up to lower order terms.", "AI": {"tldr": "This paper is related to **DSL** (marginal queries can be considered as a domain-specific language for statistical queries) and **Compiler** (mechanisms for query answering can be seen as a form of compilation/transformation of queries). The paper revisits differentially private release of marginal queries with Gaussian noise. It proposes a factorization mechanism based on the Fourier basis which is exactly optimal among all factorization mechanisms for weighted marginal and product queries, runs in polynomial time, and achieves near-optimal performance for extended marginal queries.", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u662f\u91cd\u65b0\u5ba1\u89c6\u5728\u5dee\u5206\u9690\u79c1\u4e0b\u53d1\u5e03\u5e26\u6709\u52a0\u6027\uff08\u76f8\u5173\uff09\u9ad8\u65af\u566a\u58f0\u7684\u8fb9\u9645\u67e5\u8be2\u95ee\u9898\uff0c\u65e8\u5728\u627e\u5230\u4e00\u79cd\u66f4\u901a\u7528\u3001\u66f4\u7b80\u5355\u3001\u8fd0\u884c\u66f4\u5feb\u4e14\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u7684\u673a\u5236\u6765\u5904\u7406\u4efb\u610f\u5de5\u4f5c\u8d1f\u8f7d\u7684\u52a0\u6743\u8fb9\u9645\u67e5\u8be2\uff0c\u5e76\u5c06\u5176\u63a8\u5e7f\u5230\u66f4\u4e00\u822c\u7684\u67e5\u8be2\u7c7b\u578b\uff08\u5982\u4e58\u79ef\u67e5\u8be2\u548c\u6269\u5c55\u8fb9\u9645\u67e5\u8be2\uff09\u3002\u4f5c\u8005\u65e8\u5728\u4e3a\u73b0\u6709\u5de5\u4f5c\uff08\u5982Xiao et al. [Neurips 2023]\uff09\u63d0\u4f9b\u4e00\u4e2a\u66f4\u4f18\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u5728\u5dee\u5206\u9690\u79c1\u4e0b\u53d1\u5e03\u5e26\u6709\u52a0\u6027\uff08\u76f8\u5173\uff09\u9ad8\u65af\u566a\u58f0\u7684\u8fb9\u9645\u67e5\u8be2\u95ee\u9898\u3002\u5177\u4f53\u65b9\u6cd5\u662f\uff1a\u5728\u5085\u91cc\u53f6\u57fa\u4e2d\uff0c\u4f7f\u7528\u5177\u6709\u7ecf\u8fc7\u4ed4\u7ec6\u6821\u51c6\u7684\u65b9\u5dee\u7684\u72ec\u7acb\u566a\u58f0\u53d1\u5e03\u67e5\u8be2\uff0c\u7136\u540e\u4f7f\u7528\u9006\u5085\u91cc\u53f6\u53d8\u6362\u91cd\u5efa\u8fb9\u9645\u67e5\u8be2\u7684\u7b54\u6848\u3002\u8be5\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63a8\u5e7f\u5230\u4e58\u79ef\u67e5\u8be2\u548c\u6269\u5c55\u8fb9\u9645\u67e5\u8be2\u3002", "result": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5085\u91cc\u53f6\u57fa\u7684\u5206\u89e3\u673a\u5236\uff0c\u8be5\u673a\u5236\u88ab\u8bc1\u660e\u5728\u6240\u6709\u5206\u89e3\u673a\u5236\u4e2d\u662f\u5b8c\u5168\u6700\u4f18\u7684\uff0c\u65e0\u8bba\u662f\u5728\u6700\u5c0f\u5316\u52a0\u6743\u566a\u58f0\u65b9\u5dee\u4e4b\u548c\u8fd8\u662f\u6700\u5c0f\u5316\u6700\u5927\u566a\u58f0\u65b9\u5dee\u65b9\u9762\u3002\u8be5\u673a\u5236\u7684\u8fd0\u884c\u65f6\u95f4\u662f\u6570\u636e\u96c6\u5927\u5c0f\u548c\u8f93\u51fa\u5927\u5c0f\u7684\u591a\u9879\u5f0f\u65f6\u95f4\uff0c\u4f18\u4e8e\u4f9d\u8d56\u534a\u5b9a\u89c4\u5212\u7684\u73b0\u6709\u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u6062\u590d\u4e86Xiao et al. [Neurips 2023]\u7684\u7ed3\u679c\uff0c\u4f46\u4f7f\u7528\u4e86\u66f4\u7b80\u5355\u7684\u7b97\u6cd5\u548c\u66f4\u4f18\u7684\u8fd0\u884c\u65f6\u95f4\u3002\u6b64\u5916\uff0c\u8be5\u7b97\u6cd5\u5bf9\u4e8e\u63a8\u5e7f\u7684\u4e58\u79ef\u67e5\u8be2\u4ecd\u7136\u662f\u5b8c\u5168\u6700\u4f18\u7684\u3002\u5bf9\u4e8e\u6269\u5c55\u8fb9\u9645\u67e5\u8be2\uff0c\u8be5\u673a\u5236\u8fbe\u5230\u4e86\u8fd1\u4e4e\u6700\u4f18\u7684\u6027\u80fd\uff0c\u4ec5\u5728\u4f4e\u9636\u9879\u4e0a\u6709\u6240\u4e0d\u540c\u3002", "conclusion": "\u672c\u6587\u91cd\u65b0\u63a2\u8ba8\u4e86\u5728\u5dee\u5206\u9690\u79c1\u4e0b\u4f7f\u7528\u52a0\u6027\uff08\u76f8\u5173\uff09\u9ad8\u65af\u566a\u58f0\u53d1\u5e03\u8fb9\u9645\u67e5\u8be2\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u7684\u901a\u7528\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u88ab\u8bc1\u660e\u5728\u52a0\u6743\u8fb9\u9645\u67e5\u8be2\u548c\u4e58\u79ef\u67e5\u8be2\u4e0a\uff0c\u5747\u662f\u6240\u6709\u5206\u89e3\u673a\u5236\u4e2d\u6700\u4f18\u7684\uff0c\u5e76\u4e14\u8fd0\u884c\u65f6\u95f4\u662f\u591a\u9879\u5f0f\u7ea7\u522b\u7684\u3002\u5bf9\u4e8e\u6269\u5c55\u8fb9\u9645\u67e5\u8be2\uff0c\u8be5\u7b97\u6cd5\u4e5f\u8fbe\u5230\u4e86\u8fd1\u4e4e\u6700\u4f18\u7684\u6027\u80fd\u3002\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u66f4\u7b80\u5355\u3001\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u7ed3\u679c\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2512.22066", "categories": ["cs.AR", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.22066", "abs": "https://arxiv.org/abs/2512.22066", "authors": ["Hannah Atmer", "Yuan Yao", "Thiemo Voigt", "Stefanos Kaxiras"], "title": "Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling", "comment": null, "summary": "Energy consumption dictates the cost and environmental impact of deploying Large Language Models. This paper investigates the impact of on-chip SRAM size and operating frequency on the energy efficiency and performance of LLM inference, focusing on the distinct behaviors of the compute-bound prefill and memory-bound decode phases. Our simulation methodology combines OpenRAM for energy modeling, LLMCompass for latency simulation, and ScaleSIM for systolic array operational intensity. Our findings show that total energy use is predominantly determined by SRAM size in both phases, with larger buffers significantly increasing static energy due to leakage, which is not offset by corresponding latency benefits. We quantitatively explore the memory-bandwidth bottleneck, demonstrating that while high operating frequencies reduce prefill latency, their positive impact on memory-bound decode latency is capped by the external memory bandwidth. Counter-intuitively, high compute frequency can reduce total energy by reducing execution time and consequently decreasing static energy consumption more than the resulting dynamic power increase. We identify an optimal hardware configuration for the simulated workload: high operating frequencies (1200MHz-1400MHz) and a small local buffer size of 32KB to 64KB. This combination achieves the best energy-delay product, balancing low latency with high energy efficiency. Furthermore, we demonstrate how memory bandwidth acts as a performance ceiling, and that increasing compute frequency only yields performance gains up to the point where the workload becomes memory-bound. This analysis provides concrete architectural insights for designing energy-efficient LLM accelerators, especially for datacenters aiming to minimize their energy overhead.", "AI": {"tldr": "\u662f\uff0c\u8be5\u8bba\u6587\u4e0e\u4ee5\u4e0b\u9886\u57df\u76f8\u5173\uff1a\u7f16\u8bd1\u5668\uff08\u52a0\u901f\u5668\u8bbe\u8ba1\u3001\u80fd\u8017\u4f18\u5316\uff09\u3001DSL/Graph Processing/MLIR\uff08\u8be5\u8bba\u6587\u5173\u6ce8\u7684\u662fLLM\u63a8\u7406\u52a0\u901f\u5668\u7684\u786c\u4ef6\u8bbe\u8ba1\u4e0e\u4f18\u5316\uff0c\u867d\u7136\u4e0d\u662f\u76f4\u63a5\u5173\u4e8e\u7f16\u8bd1\u5668\u524d\u7aef\u3001DSL\u6216MLIR\uff0c\u4f46\u5176\u76ee\u6807\u662f\u4f18\u5316\u786c\u4ef6\u6267\u884c\u6548\u7387\uff0c\u8fd9\u4e0e\u9ad8\u6027\u80fd\u8ba1\u7b97\u548c\u7f16\u8bd1\u5668\u540e\u7aef\u4f18\u5316\u7d27\u5bc6\u76f8\u5173\uff09\u3002\n\u592a\u957f\u4e0d\u770b\u6458\u8981\uff1a\u672c\u6587\u7814\u7a76\u4e86SRAM\u5927\u5c0f\u548c\u5de5\u4f5c\u9891\u7387\u5bf9LLM\u63a8\u7406\uff08\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\uff09\u80fd\u6548\u548c\u6027\u80fd\u7684\u5f71\u54cd\u3002\u53d1\u73b0SRAM\u5927\u5c0f\u4e3b\u5bfc\u603b\u80fd\u8017\uff0c\u8f83\u5927SRAM\u589e\u52a0\u9759\u6001\u80fd\u8017\u3002\u9ad8\u9891\u7387\u5728\u5185\u5b58\u74f6\u9888\u524d\u80fd\u964d\u4f4e\u5ef6\u8fdf\u548c\u80fd\u8017\uff08\u901a\u8fc7\u51cf\u5c11\u9759\u6001\u80fd\u8017\uff09\u3002\u6700\u4f73\u914d\u7f6e\u662f\u9ad8\u9891\u7387\uff081200-1400MHz\uff09\u548c\u5c0f\u7f13\u51b2\u533a\uff0832KB-64KB\uff09\uff0c\u5b9e\u73b0\u4e86\u6700\u4f73\u80fd\u8017\u5ef6\u8fdf\u79ef\uff08EDP\uff09\u3002\u7814\u7a76\u5f3a\u8c03\u5185\u5b58\u5e26\u5bbd\u662f\u6027\u80fd\u4e0a\u9650\uff0c\u4e3aLLM\u52a0\u901f\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u67b6\u6784\u89c1\u89e3\u3002", "motivation": "\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u80fd\u8017\u662f\u5176\u6210\u672c\u548c\u73af\u5883\u5f71\u54cd\u7684\u5173\u952e\u56e0\u7d20\u3002\u672c\u6587\u7684\u52a8\u673a\u662f\u7cfb\u7edf\u5730\u7814\u7a76\u7247\u4e0aSRAM\u5927\u5c0f\u548c\u5de5\u4f5c\u9891\u7387\u8fd9\u4e24\u4e2a\u5173\u952e\u786c\u4ef6\u53c2\u6570\u5bf9LLM\u63a8\u7406\u7684\u80fd\u6548\u548c\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5206\u522b\u63a2\u8ba8\u8ba1\u7b97\u5bc6\u96c6\u578b\uff08compute-bound\uff09\u7684\u9884\u586b\u5145\uff08prefill\uff09\u9636\u6bb5\u548c\u5185\u5b58\u5bc6\u96c6\u578b\uff08memory-bound\uff09\u7684\u89e3\u7801\uff08decode\uff09\u9636\u6bb5\u7684\u4e0d\u540c\u884c\u4e3a\u3002\u76ee\u7684\u662f\u63d0\u4f9b\u5177\u4f53\u7684\u67b6\u6784\u6d1e\u5bdf\uff0c\u4ee5\u8bbe\u8ba1\u8282\u80fd\u7684LLM\u52a0\u901f\u5668\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u7ed3\u5408\u591a\u79cd\u6a21\u62df\u5de5\u5177\u7684\u4eff\u771f\u65b9\u6cd5\uff1a\u4f7f\u7528OpenRAM\u8fdb\u884c\u80fd\u8017\u5efa\u6a21\uff1b\u4f7f\u7528LLMCompass\u8fdb\u884c\u5ef6\u8fdf\u4eff\u771f\uff1b\u4f7f\u7528ScaleSIM\u8fdb\u884c\u8109\u52a8\u9635\u5217\uff08systolic array\uff09\u7684\u8fd0\u884c\u5f3a\u5ea6\u5206\u6790\u3002\u901a\u8fc7\u8fd9\u79cd\u591a\u5de5\u5177\u7ed3\u5408\u7684\u4eff\u771f\u65b9\u6cd5\uff0c\u8be6\u7ec6\u5206\u6790\u4e86\u4e0d\u540c\u786c\u4ef6\u914d\u7f6e\u4e0b\u7684\u80fd\u8017\u548c\u6027\u80fd\u8868\u73b0\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff1a\n1. **SRAM\u5927\u5c0f\u5bf9\u80fd\u8017\u7684\u4e3b\u5bfc\u4f5c\u7528\uff1a** \u603b\u80fd\u8017\u4e3b\u8981\u7531SRAM\u5927\u5c0f\u51b3\u5b9a\uff0c\u65e0\u8bba\u662f\u5728\u9884\u586b\u5145\u8fd8\u662f\u89e3\u7801\u9636\u6bb5\u3002\u8f83\u5927\u7684SRAM\u7f13\u51b2\u533a\u4f1a\u663e\u8457\u589e\u52a0\u7531\u4e8e\u6cc4\u6f0f\u4ea7\u751f\u7684\u9759\u6001\u80fd\u8017\uff0c\u4e14\u4e0d\u80fd\u901a\u8fc7\u76f8\u5e94\u7684\u5ef6\u8fdf\u6539\u5584\u6765\u62b5\u6d88\u3002\n2. **\u9ad8\u9891\u7387\u7684\u5fae\u5999\u5f71\u54cd\uff1a**\n    * \u63d0\u9ad8\u5de5\u4f5c\u9891\u7387\u80fd\u51cf\u5c11\u9884\u586b\u5145\u9636\u6bb5\u7684\u5ef6\u8fdf\u3002\n    * \u63d0\u9ad8\u9891\u7387\u5bf9\u5185\u5b58\u5bc6\u96c6\u578b\u89e3\u7801\u9636\u6bb5\u5ef6\u8fdf\u7684\u79ef\u6781\u5f71\u54cd\u53d7\u5230\u5916\u90e8\u5185\u5b58\u5e26\u5bbd\u7684\u9650\u5236\u3002\n    * **\u53cd\u76f4\u89c9\u7684\u53d1\u73b0\uff1a** \u9ad8\u8ba1\u7b97\u9891\u7387\u53ef\u4ee5\u901a\u8fc7\u51cf\u5c11\u6267\u884c\u65f6\u95f4\uff0c\u4ece\u800c\u51cf\u5c11\u9759\u6001\u80fd\u8017\u7684\u6d88\u8017\uff0c\u5176\u51cf\u5c11\u91cf\u5927\u4e8e\u52a8\u6001\u529f\u8017\u7684\u589e\u52a0\u91cf\uff0c\u53cd\u800c\u964d\u4f4e\u603b\u80fd\u8017\u3002\n3. **\u6700\u4f73\u914d\u7f6e\uff1a** \u6700\u4f73\u7684\u786c\u4ef6\u914d\u7f6e\u662f\u9ad8\u5de5\u4f5c\u9891\u7387\uff081200MHz-1400MHz\uff09\u548c\u5c0f\u7684\u672c\u5730\u7f13\u51b2\u533a\uff0832KB-64KB\uff09\u3002\n4. **\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\uff1a** \u5185\u5b58\u5e26\u5bbd\u662f\u6027\u80fd\u7684\u4e0a\u9650\uff0c\u63d0\u9ad8\u8ba1\u7b97\u9891\u7387\u53ea\u6709\u5728\u5de5\u4f5c\u8d1f\u8f7d\u4ecd\u662f\u8ba1\u7b97\u5bc6\u96c6\u578b\u65f6\u624d\u4ea7\u751f\u6027\u80fd\u589e\u76ca\uff1b\u4e00\u65e6\u8fbe\u5230\u5185\u5b58\u74f6\u9888\uff0c\u6027\u80fd\u5c06\u4e0d\u518d\u63d0\u9ad8\u3002", "conclusion": "\u672c\u6587\u7684\u7ed3\u8bba\u662f\uff0c\u5bf9\u4e8e\u6240\u6a21\u62df\u7684LLM\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5b58\u5728\u4e00\u4e2a\u6700\u4f73\u7684\u786c\u4ef6\u914d\u7f6e\uff0c\u5373\u9ad8\u5de5\u4f5c\u9891\u7387\uff081200MHz-1400MHz\uff09\u548c\u5c0f\u7684\u672c\u5730\u7f13\u51b2\u533a\u5927\u5c0f\uff0832KB-64KB\uff09\u3002\u8fd9\u79cd\u914d\u7f6e\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u80fd\u8017\u5ef6\u8fdf\u79ef\uff08EDP\uff09\uff0c\u5e73\u8861\u4e86\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u80fd\u6548\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u5185\u5b58\u5e26\u5bbd\u662f\u6027\u80fd\u7684\u4e0a\u9650\uff0c\u8d85\u51fa\u5185\u5b58\u74f6\u9888\u540e\uff0c\u63d0\u9ad8\u8ba1\u7b97\u9891\u7387\u4e0d\u4f1a\u5e26\u6765\u6027\u80fd\u63d0\u5347\u3002\u8fd9\u4e9b\u5177\u4f53\u7684\u67b6\u6784\u89c1\u89e3\u5bf9\u4e8e\u8bbe\u8ba1\u6570\u636e\u4e2d\u5fc3\u4e2d\u8282\u80fd\u7684LLM\u52a0\u901f\u5668\u5177\u6709\u91cd\u8981\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2512.21671", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.21671", "abs": "https://arxiv.org/abs/2512.21671", "authors": ["Sebastian Forster", "Gramoz Goranci", "Ali Momeni"], "title": "Fully Dynamic Spectral Sparsification for Directed Hypergraphs", "comment": "STACS 2026", "summary": "There has been a surge of interest in spectral hypergraph sparsification, a natural generalization of spectral sparsification for graphs. In this paper, we present a simple fully dynamic algorithm for maintaining spectral hypergraph sparsifiers of \\textit{directed} hypergraphs. Our algorithm achieves a near-optimal size of $O(n^2 / \\varepsilon ^2 \\log ^7 m)$ and amortized update time of $O(r^2 \\log ^3 m)$, where $n$ is the number of vertices, and $m$ and $r$ respectively upper bound the number of hyperedges and the rank of the hypergraph at any time.\n  We also extend our approach to the parallel batch-dynamic setting, where a batch of any $k$ hyperedge insertions or deletions can be processed with $O(kr^2 \\log ^3 m)$ amortized work and $O(\\log ^2 m)$ depth. This constitutes the first spectral-based sparsification algorithm in this setting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e0e\u56fe\u5904\u7406\u76f8\u5173\uff0c\u56e0\u4e3a\u5b83\u5904\u7406\u4e86\u8d85\u56fe\uff08\u56fe\u7684\u63a8\u5e7f\uff09\u7684\u8c31\u7a00\u758f\u5316\u95ee\u9898\u3002\n\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u8fd1\u4e4e\u6700\u4f18\u7684\u5168\u52a8\u6001\u7b97\u6cd5\uff0c\u7528\u4e8e\u7ef4\u62a4\u6709\u5411\u8d85\u56fe\u7684\u8c31\u7a00\u758f\u5316\u5668\uff0c\u5176\u7a00\u758f\u5316\u5668\u5927\u5c0f\u4e3a $O(n^2 / \\varepsilon ^2 \\log ^7 m)$\uff0c\u644a\u9500\u66f4\u65b0\u65f6\u95f4\u4e3a $O(r^2 \\log ^3 m)$\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u88ab\u6269\u5c55\u5230\u5e76\u884c\u6279\u5904\u7406\u52a8\u6001\u8bbe\u7f6e\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u8c31\u7a00\u758f\u5316\u7684\u7b97\u6cd5\uff0c\u80fd\u4ee5 $O(kr^2 \\log ^3 m)$ \u7684\u644a\u9500\u5de5\u4f5c\u91cf\u548c $O(\\log ^2 m)$ \u6df1\u5ea6\u5904\u7406\u4e00\u6279 $k$ \u4e2a\u8d85\u8fb9\u66f4\u65b0\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u5bf9\u8c31\u8d85\u56fe\u7a00\u758f\u5316\uff0c\u5373\u56fe\u8c31\u7a00\u758f\u5316\u7684\u81ea\u7136\u63a8\u5e7f\uff0c\u8868\u73b0\u51fa\u6d53\u539a\u7684\u5174\u8da3\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u6709\u5411\u8d85\u56fe\u7684\u8c31\u7a00\u758f\u5316\u4ee5\u53ca\u52a8\u6001\u7ef4\u62a4\u7a00\u758f\u5316\u5668\u7684\u95ee\u9898\u4ecd\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6709\u5411\u8d85\u56fe\u7684\u5168\u52a8\u6001\u7b97\u6cd5\uff0c\u7528\u4e8e\u7ef4\u62a4\u8c31\u7a00\u758f\u5316\u5668\u3002\u8be5\u7b97\u6cd5\u91c7\u7528\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u5c06\u5176\u6269\u5c55\u5230\u4e86\u5e76\u884c\u6279\u5904\u7406\u52a8\u6001\uff08batch-dynamic\uff09\u8bbe\u7f6e\uff0c\u8fd9\u662f\u9996\u6b21\u5728\u8c31\u7a00\u758f\u5316\u9886\u57df\u91c7\u7528\u6b64\u8bbe\u7f6e\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u5168\u52a8\u6001\u7b97\u6cd5\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u7a00\u758f\u5316\u5668\u5927\u5c0f $O(n^2 / \\varepsilon ^2 \\log ^7 m)$ \u548c $O(r^2 \\log ^3 m)$ \u7684\u644a\u9500\u66f4\u65b0\u65f6\u95f4\u3002\u5728\u5e76\u884c\u6279\u5904\u7406\u52a8\u6001\u8bbe\u7f6e\u4e2d\uff0c\u8be5\u7b97\u6cd5\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u8c31\u7684\u7a00\u758f\u5316\u7b97\u6cd5\uff0c\u80fd\u591f\u4ee5 $O(kr^2 \\log ^3 m)$ \u7684\u644a\u9500\u5de5\u4f5c\u91cf\u548c $O(\\log ^2 m)$ \u7684\u6df1\u5ea6\u5904\u7406\u4efb\u610f $k$ \u4e2a\u8d85\u8fb9\u63d2\u5165\u6216\u5220\u9664\u6279\u5904\u7406\u64cd\u4f5c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6709\u5411\u8d85\u56fe\u8c31\u7a00\u758f\u5316\u7684\u7b80\u5355\u5168\u52a8\u6001\u7b97\u6cd5\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u5230\u5e76\u884c\u6279\u5904\u7406\u52a8\u6001\u8bbe\u7f6e\uff0c\u4e3a\u8c31\u7a00\u758f\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u5904\u7406\u5927\u89c4\u6a21\u52a8\u6001\u8d85\u56fe\u6570\u636e\u65f6\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2512.21487", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21487", "abs": "https://arxiv.org/abs/2512.21487", "authors": ["Xinglin Pan", "Shaohuai Shi", "Wenxiang Lin", "Yuxin Wang", "Zhenheng Tang", "Wei Wang", "Xiaowen Chu"], "title": "Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism", "comment": null, "summary": "The mixture-of-experts (MoE) architecture scales model size with sublinear computational increase but suffers from memory-intensive inference due to KV caches and sparse expert activation. Recent disaggregated expert parallelism (DEP) distributes attention and experts to dedicated GPU groups but lacks support for shared experts and efficient task scheduling, limiting performance.\n  We propose FinDEP, a fine-grained task scheduling algorithm for DEP that maximizes task overlap to improve MoE inference throughput. FinDEP introduces three innovations: 1) partitioning computation/communication into smaller tasks for fine-grained pipelining, 2) formulating a scheduling optimization supporting variable granularity and ordering, and 3) developing an efficient solver for this large search space.\n  Experiments on four GPU systems with DeepSeek-V2 and Qwen3-MoE show FinDEP improves throughput by up to 1.61x over prior methods, achieving up to 1.24x speedup on a 32-GPU system.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e0e\u7f16\u8bd1\u5668\u76f8\u5173\uff0c\u56e0\u4e3a\u5b83\u6d89\u53ca\u9ad8\u6548\u7684\u4efb\u52a1\u8c03\u5ea6\u548c\u5e76\u884c\u4f18\u5316\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5f0f\u786c\u4ef6\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u63a8\u7406\u3002\n\n\u8be5\u8bba\u6587\u63d0\u51fa FinDEP\uff0c\u4e00\u4e2a\u9488\u5bf9\u5206\u5e03\u5f0f\u4e13\u5bb6\u5e76\u884c\uff08DEP\uff09\u7684\u7ec6\u7c92\u5ea6\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3 MoE \u6a21\u578b\u63a8\u7406\u65f6\u4efb\u52a1\u91cd\u53e0\u5ea6\u4f4e\u548c\u6027\u80fd\u53d7\u9650\u7684\u95ee\u9898\u3002FinDEP \u901a\u8fc7\u5c06\u8ba1\u7b97/\u901a\u4fe1\u5212\u5206\u4e3a\u66f4\u5c0f\u7684\u4efb\u52a1\u3001\u6784\u5efa\u7075\u6d3b\u7684\u8c03\u5ea6\u4f18\u5316\u6a21\u578b\u548c\u5f00\u53d1\u9ad8\u6548\u7684\u6c42\u89e3\u5668\uff0c\u6700\u5927\u5316\u4efb\u52a1\u91cd\u53e0\uff0c\u4ece\u800c\u5728 DeepSeek-V2 \u548c Qwen3-MoE \u7b49\u6a21\u578b\u4e0a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe 1.61 \u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684 MoE \u67b6\u6784\u867d\u7136\u80fd\u4ee5\u4e9a\u7ebf\u6027\u8ba1\u7b97\u589e\u957f\u6269\u5c55\u6a21\u578b\u89c4\u6a21\uff0c\u4f46\u7531\u4e8e\u952e\u503c\uff08KV\uff09\u7f13\u5b58\u548c\u7a00\u758f\u4e13\u5bb6\u6fc0\u6d3b\uff0c\u5bfc\u81f4\u63a8\u7406\u65f6\u7684\u5185\u5b58\u9700\u6c42\u5927\u3002\u6700\u8fd1\u7684\u5206\u5e03\u5f0f\u4e13\u5bb6\u5e76\u884c\uff08DEP\uff09\u5c06\u6ce8\u610f\u529b\u673a\u5236\u548c\u4e13\u5bb6\u5206\u914d\u7ed9\u4e13\u7528\u7684 GPU \u7ec4\uff0c\u4f46\u5b83\u7f3a\u4e4f\u5bf9\u5171\u4eab\u4e13\u5bb6\u548c\u9ad8\u6548\u4efb\u52a1\u8c03\u5ea6\u7684\u652f\u6301\uff0c\u4ece\u800c\u9650\u5236\u4e86\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u7ec6\u7684\u8c03\u5ea6\u65b9\u6cd5\u6765\u63d0\u9ad8 MoE \u63a8\u7406\u7684\u541e\u5410\u91cf\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86 FinDEP\uff0c\u4e00\u4e2a\u9488\u5bf9 DEP \u7684\u7ec6\u7c92\u5ea6\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u6700\u5927\u5316\u4efb\u52a1\u91cd\u53e0\u6765\u63d0\u9ad8 MoE \u63a8\u7406\u7684\u541e\u5410\u91cf\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u5c06\u8ba1\u7b97\u548c\u901a\u4fe1\u5212\u5206\u4e3a\u66f4\u5c0f\u7684\u4efb\u52a1\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6d41\u6c34\u7ebf\uff1b2\uff09\u5efa\u7acb\u4e00\u4e2a\u652f\u6301\u53ef\u53d8\u7c92\u5ea6\u548c\u6392\u5e8f\u7684\u8c03\u5ea6\u4f18\u5316\u6a21\u578b\uff1b3\uff09\u5f00\u53d1\u4e00\u4e2a\u9ad8\u6548\u7684\u6c42\u89e3\u5668\u6765\u89e3\u51b3\u5927\u578b\u641c\u7d22\u7a7a\u95f4\u95ee\u9898\u3002", "result": "\u5728\u56db\u4e2a GPU \u7cfb\u7edf\u4e0a\u4f7f\u7528 DeepSeek-V2 \u548c Qwen3-MoE \u6a21\u578b\u7684\u5b9e\u9a8c\u4e2d\uff0cFinDEP \u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c06\u541e\u5410\u91cf\u63d0\u9ad8\u4e86\u9ad8\u8fbe 1.61 \u500d\uff0c\u5e76\u5728\u4e00\u4e2a 32-GPU \u7cfb\u7edf\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe 1.24 \u500d\u7684\u52a0\u901f\u3002", "conclusion": "FinDEP\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u4efb\u52a1\u8c03\u5ea6\u548c\u6d41\u6c34\u7ebf\u5316\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f MoE \u63a8\u7406\u4e2d\u4efb\u52a1\u91cd\u53e0\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u3002\u5176\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5bf9\u8ba1\u7b97\u548c\u901a\u4fe1\u4efb\u52a1\u7684\u7ec6\u7c92\u5ea6\u5212\u5206\u3001\u7075\u6d3b\u7684\u8c03\u5ea6\u4f18\u5316\u6a21\u578b\u4ee5\u53ca\u9ad8\u6548\u7684\u6c42\u89e3\u5668\u3002FinDEP \u7684\u63d0\u51fa\u5bf9\u4e8e\u5927\u89c4\u6a21 MoE \u6a21\u578b\u7684\u90e8\u7f72\u548c\u9ad8\u6548\u63a8\u7406\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2512.21980", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.21980", "abs": "https://arxiv.org/abs/2512.21980", "authors": ["A. I. Perminov"], "title": "A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication", "comment": null, "summary": "This paper presents a new state-of-the-art algorithm for exact $3\\times3$ matrix multiplication over general non-commutative rings, achieving a rank-23 scheme with only 58 scalar additions. This improves the previous best additive complexity of 60 additions without a change of basis. The result was discovered through an automated search combining ternary-restricted flip-graph exploration with greedy intersection reduction for common subexpression elimination. The resulting scheme uses only coefficients from $\\{-1, 0, 1\\}$, ensuring both efficiency and portability across arbitrary fields. The total scalar operation count is reduced from 83 to 81.", "AI": {"tldr": "\u4e0e DSL\u3001\u56fe\u5904\u7406\u3001MLIR\u3001\u7f16\u8bd1\u5668\u6216 HLS **\u65e0\u5173**\u3002\n\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97 $3 \\times 3$ \u975e\u4ea4\u6362\u73af\u4e0a\u77e9\u9635\u4e58\u6cd5\u7684\u6700\u5148\u8fdb\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4e09\u5143\u9650\u5236\u7684\u7ffb\u8f6c\u56fe\u63a2\u7d22\u548c\u8d2a\u5a6a\u4ea4\u96c6\u7ea6\u51cf\u7684\u81ea\u52a8\u5316\u641c\u7d22\uff0c\u5728\u4fdd\u6301 23 \u79e9\u7684\u524d\u63d0\u4e0b\uff0c\u5c06\u52a0\u6cd5\u590d\u6742\u5ea6\u4ece 60 \u6b21\u964d\u4f4e\u5230 58 \u6b21\uff0c\u5c06\u603b\u6807\u91cf\u64cd\u4f5c\u6570\u4ece 83 \u6b21\u964d\u4f4e\u5230 81 \u6b21\u3002\u65b0\u65b9\u6848\u4ec5\u4f7f\u7528 $\\{-1, 0, 1\\}$ \u7cfb\u6570\uff0c\u786e\u4fdd\u4e86\u9ad8\u6548\u6027\u548c\u53ef\u79fb\u690d\u6027\u3002", "motivation": "\u4f18\u5316\u7684\u76ee\u6807\u662f\u5bfb\u627e\u8ba1\u7b97\u7cbe\u786e $3 \\times 3$ \u975e\u4ea4\u6362\u73af\u4e0a\u77e9\u9635\u4e58\u6cd5\u7684\u66f4\u5feb\u901f\u7b97\u6cd5\uff0c\u5177\u4f53\u6765\u8bf4\uff0c\u662f\u5728\u4fdd\u6301 23 \u79e9\uff08\u5373 23 \u6b21\u4e58\u6cd5\uff09\u7684\u524d\u63d0\u4e0b\uff0c**\u964d\u4f4e\u52a0\u6cd5\u590d\u6742\u5ea6**\u548c**\u603b\u6807\u91cf\u64cd\u4f5c\u6570**\uff0c\u4ece\u800c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u79fb\u690d\u6027\u3002\u539f\u5148\u6700\u4f73\u65b9\u6848\u7684\u52a0\u6cd5\u590d\u6742\u5ea6\u4e3a 60 \u6b21\uff0c\u672c\u6587\u65e8\u5728\u8d85\u8d8a\u8be5\u8bb0\u5f55\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u81ea\u52a8\u5316\u641c\u7d22\u53d1\u73b0\u65b0\u7684 $3 \\times 3$ \u77e9\u9635\u4e58\u6cd5\u65b9\u6848\u3002\u8be5\u641c\u7d22\u65b9\u6cd5\u7ed3\u5408\u4e86**\u4e09\u5143\u9650\u5236\u7684\u7ffb\u8f6c\u56fe\u63a2\u7d22 (ternary-restricted flip-graph exploration)** \u548c\u7528\u4e8e\u516c\u5171\u5b50\u8868\u8fbe\u5f0f\u6d88\u9664\u7684**\u8d2a\u5a6a\u4ea4\u96c6\u7ea6\u51cf (greedy intersection reduction)**\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u7cfb\u7edf\u5730\u63a2\u7d22\u5e76\u4f18\u5316\u4e58\u6cd5\u65b9\u6848\u4e2d\u7684\u52a0\u6cd5\u64cd\u4f5c\u3002\u6700\u7ec8\u7684\u65b9\u6848\u4f7f\u7528\u4e86\u4ec5\u5305\u542b $\\{-1, 0, 1\\}$ \u7684\u7cfb\u6570\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7cbe\u786e $3 \\times 3$ \u975e\u4ea4\u6362\u73af\u4e0a\u77e9\u9635\u4e58\u6cd5\u65b9\u6848\uff1a\n\n1.  **\u79e9 (Rank):** \u4fdd\u6301\u4e86 23 \u79e9\uff08\u5373 23 \u6b21\u4e58\u6cd5\uff09\u3002\n2.  **\u52a0\u6cd5\u590d\u6742\u5ea6 (Additive Complexity):** \u8fbe\u5230 58 \u6b21\u6807\u91cf\u52a0\u6cd5\uff0c\u4f18\u4e8e\u6b64\u524d\u6700\u4f73\u7684 60 \u6b21\u3002\n3.  **\u603b\u6807\u91cf\u64cd\u4f5c\u6570 (Total Scalar Operation Count):** \u4ece 83 \u6b21\u964d\u4f4e\u5230 81 \u6b21\u3002\n4.  **\u7cfb\u6570\u7279\u6027:** \u65b9\u6848\u4e2d\u4ec5\u4f7f\u7528 $\\{-1, 0, 1\\}$ \u4e2d\u7684\u7cfb\u6570\uff0c\u6781\u5927\u63d0\u9ad8\u4e86\u8de8\u4efb\u610f\u57df\u7684**\u6548\u7387**\u548c**\u53ef\u79fb\u690d\u6027**\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 $3 \\times 3$ \u77e9\u9635\u4e58\u6cd5\u5feb\u901f\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u975e\u4ea4\u6362\u73af\u4e0a\u5b9e\u73b0\u4e86 23 \u79e9\u7684\u65b9\u6848\uff0c\u5e76\u5c06\u52a0\u6cd5\u590d\u6742\u5ea6\u964d\u4f4e\u5230 58 \u6b21\uff0c\u603b\u6807\u91cf\u64cd\u4f5c\u6570\u964d\u4f4e\u5230 81 \u6b21\u3002\u8be5\u65b9\u6848\u4f7f\u7528\u4ec5\u5305\u542b $\\{-1, 0, 1\\}$ \u7cfb\u6570\u7684\u4e58\u6cd5\u56e0\u5b50\uff0c\u4fdd\u8bc1\u4e86\u8de8\u4efb\u610f\u57df\u7684\u6548\u7387\u548c\u53ef\u79fb\u690d\u6027\u3002\u8fd9\u4e00\u6539\u8fdb\u901a\u8fc7\u7ed3\u5408\u4e09\u5143\u9650\u5236\u7684\u7ffb\u8f6c\u56fe\u63a2\u7d22\u548c\u7528\u4e8e\u516c\u5171\u5b50\u8868\u8fbe\u5f0f\u6d88\u9664\u7684\u8d2a\u5a6a\u4ea4\u96c6\u7ea6\u51cf\u7684\u81ea\u52a8\u5316\u641c\u7d22\u65b9\u6cd5\u53d1\u73b0\u3002"}}
{"id": "2512.21571", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21571", "abs": "https://arxiv.org/abs/2512.21571", "authors": ["Hui Guo", "Qihang Zheng", "Chenghai Huo", "Dongliang Guo", "Haoqi Yang", "Yang Zhang"], "title": "nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures", "comment": null, "summary": "The efficient deployment of large language models (LLMs) is hindered by memory architecture heterogeneity, where traditional compilers suffer from fragmented workflows and high adaptation costs. We present nncase, an open-source, end-to-end compilation framework designed to unify optimization across diverse targets. Central to nncase is an e-graph-based term rewriting engine that mitigates the phase ordering problem, enabling global exploration of computation and data movement strategies. The framework integrates three key modules: Auto Vectorize for adapting to heterogeneous computing units, Auto Distribution for searching parallel strategies with cost-aware communication optimization, and Auto Schedule for maximizing on-chip cache locality. Furthermore, a buffer-aware Codegen phase ensures efficient kernel instantiation. Evaluations show that nncase outperforms mainstream frameworks like MLC LLM and Intel IPEX on Qwen3 series models and achieves performance comparable to the hand-optimized llama.cpp on CPUs, demonstrating the viability of automated compilation for high-performance LLM deployment. The source code is available at https://github.com/kendryte/nncase.", "AI": {"tldr": "This paper is related to compiler and DSL. More specifically, it introduces an end-to-end compilation framework named nncase for efficient deployment of LLMs on diverse hardware. The core of nncase is an e-graph-based term rewriting engine and three key optimization modules (Auto Vectorize, Auto Distribution, Auto Schedule). nncase outperforms MLC LLM and Intel IPEX and achieves performance comparable to the hand-optimized llama.cpp, validating the viability of automated compilation for high-performance LLM deployment.", "motivation": "\u9ad8\u6548\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53d7\u5230\u5185\u5b58\u67b6\u6784\u5f02\u6784\u6027\u7684\u963b\u788d\u3002\u4f20\u7edf\u7684\u7f16\u8bd1\u5668\u5de5\u4f5c\u6d41\u7a0b\u96f6\u788e\uff0c\u9002\u5e94\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u6709\u6548\u5e94\u5bf9\u8fd9\u79cd\u5f02\u6784\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u7aef\u5230\u7aef\u3001\u7edf\u4e00\u7684\u7f16\u8bd1\u6846\u67b6\u6765\u4f18\u5316\u8de8\u591a\u6837\u5316\u76ee\u6807\uff08\u5f02\u6784\u786c\u4ef6\uff09\u7684 LLM \u90e8\u7f72\u3002", "method": "nncase \u6846\u67b6\u5305\u542b\u4e00\u4e2a\u57fa\u4e8e e-graph \u7684\u9879\u91cd\u5199\u5f15\u64ce\uff0c\u7528\u4e8e\u5168\u5c40\u63a2\u7d22\u8ba1\u7b97\u548c\u6570\u636e\u79fb\u52a8\u7b56\u7565\uff0c\u4ee5\u51cf\u8f7b\u76f8\u4f4d\u6392\u5e8f\u95ee\u9898\u3002\u5b83\u96c6\u6210\u4e86\u4e09\u4e2a\u5173\u952e\u4f18\u5316\u6a21\u5757\uff1a1. **\u81ea\u52a8\u5411\u91cf\u5316 (Auto Vectorize)**\uff1a\u9002\u5e94\u5f02\u6784\u8ba1\u7b97\u5355\u5143\u30022. **\u81ea\u52a8\u5206\u5e03 (Auto Distribution)**\uff1a\u641c\u7d22\u5e76\u884c\u7b56\u7565\uff0c\u5e76\u8fdb\u884c\u6210\u672c\u611f\u77e5\u7684\u901a\u4fe1\u4f18\u5316\u30023. **\u81ea\u52a8\u8c03\u5ea6 (Auto Schedule)**\uff1a\u6700\u5927\u5316\u7247\u4e0a\u7f13\u5b58\u5c40\u90e8\u6027\u3002\u6b64\u5916\uff0c\u4e00\u4e2a\u7f13\u51b2\u533a\u611f\u77e5\u7684\u4ee3\u7801\u751f\u6210 (Codegen) \u9636\u6bb5\u786e\u4fdd\u4e86\u9ad8\u6548\u7684\u5185\u6838\u5b9e\u4f8b\u5316\u3002", "result": "\u5728 Qwen3 \u7cfb\u5217\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cnncase \u7684\u6027\u80fd\u8d85\u8d8a\u4e86\u4e3b\u6d41\u6846\u67b6\uff0c\u5982 MLC LLM \u548c Intel IPEX\u3002\u5728 CPU \u4e0a\uff0cnncase \u7684\u6027\u80fd\u4e0e\u7ecf\u8fc7\u624b\u52a8\u4f18\u5316\u7684 llama.cpp \u76f8\u5f53\u3002\u8fd9\u8bc1\u660e\u4e86 nncase \u5728\u5b9e\u73b0\u9ad8\u6027\u80fd LLM \u81ea\u52a8\u5316\u90e8\u7f72\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "nncase \u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u3001\u5f00\u6e90\u7684\u7f16\u8bd1\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u7684\u4f18\u5316\u6a21\u5757\uff08\u81ea\u52a8\u5411\u91cf\u5316\u3001\u81ea\u52a8\u5206\u5e03\u3001\u81ea\u52a8\u8c03\u5ea6\uff09\u548c\u4e00\u4e2a\u57fa\u4e8e e-graph \u7684\u9879\u91cd\u5199\u5f15\u64ce\uff0c\u89e3\u51b3\u4e86 LLM \u90e8\u7f72\u4e2d\u5185\u5b58\u67b6\u6784\u5f02\u6784\u6027\u548c\u4f20\u7edf\u7f16\u8bd1\u5668\u7684\u5c40\u9650\u6027\u3002nncase \u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u4e3b\u6d41\u6846\u67b6\uff08\u5982 MLC LLM \u548c Intel IPEX\uff09\uff0c\u8fbe\u5230\u4e86\u4e0e\u624b\u5199\u4f18\u5316 llama.cpp \u76f8\u5f53\u7684\u6c34\u5e73\uff0c\u8bc1\u660e\u4e86\u81ea\u52a8\u5316\u7f16\u8bd1\u5728\u9ad8\u6027\u80fd LLM \u90e8\u7f72\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.21967", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2512.21967", "abs": "https://arxiv.org/abs/2512.21967", "authors": ["Deniz Elbek", "Kamer Kaya"], "title": "BLEST: Blazingly Efficient BFS using Tensor Cores", "comment": "13 pages, 3 figures, 4 tables, 3 algorithms, 46 references", "summary": "Breadth-First Search (BFS) is a fundamental graph kernel that underpins a wide range of applications. While modern GPUs provide specialised Matrix-Multiply-Accumulate (MMA) units, e.g., Tensor Cores (TC), with extremely high throughput, they target dense operations, making it non-trivial to exploit them for irregular, unstructured graph computations. In particular, fully utilising them for a BFS requires an efficient mapping of the edge operations onto TCs while avoiding redundancy, load imbalance, and synchronisation. We present BLEST, a TC-accelerated framework that reformulates the pull-based BFS pipeline around a bitmap-oriented structure and a carefully engineered execution layout. BLEST introduces Binarised Virtual Slice Sets (BVSS) to enforce warp-level load balancing and to eliminate frontier-oblivious work assignment. To improve both memory efficiency and update locality across diverse graphs, we apply two complementary graph reordering strategies: a compression-oriented ordering for social-like graphs and a bandwidth-reducing ordering for non-social graphs. At the compute level, we develop a batched SpMSpV multiplication pattern that uses the bitwise TC tiles to handle dot products without wasting output entries, thereby reducing the number of required MMA calls. Finally, BLEST combines kernel fusion with a lazy vertex update scheme to reduce host-side synchronisation, mitigate atomic overheads, and improve cache locality. Experiments show that BLEST delivers, on average, $3.58\\times$, $4.64\\times$ and $4.9\\times$ speedup over BerryBees, Gunrock, and GSWITCH, respectively, across a broad set of real-world graphs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e0e\u56fe\u5904\u7406\uff08\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u4f5c\u4e3a\u6838\u5fc3\u56fe\u6838\uff09\u548c\u7f16\u8bd1\u5668/\u786c\u4ef6\u52a0\u901f\uff08\u5229\u7528GPU Tensor Cores\u8fdb\u884c\u52a0\u901f\u548c\u6027\u80fd\u4f18\u5316\uff09\u76f8\u5173\u3002\n\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\uff08BFS\uff09\u662f\u56fe\u8ba1\u7b97\u7684\u57fa\u7840\u5185\u6838\uff0c\u4f46\u5c06\u5b83\u6709\u6548\u5730\u6620\u5c04\u5230\u73b0\u4ee3GPU\u7684\u9ad8\u541e\u5410\u91cfTensor Cores\uff08TC\uff09\u4e0a\u9762\u4e34\u8d1f\u8f7d\u5747\u8861\u3001\u5197\u4f59\u548c\u540c\u6b65\u7684\u6311\u6218\u3002BLEST\u662f\u4e00\u4e2aTC\u52a0\u901f\u7684\u6846\u67b6\uff0c\u5b83\u5c06\u62c9\u5f0fBFS\u6d41\u6c34\u7ebf\u56f4\u7ed5\u4f4d\u56fe\u7ed3\u6784\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6267\u884c\u5e03\u5c40\u8fdb\u884c\u91cd\u6784\u3002\u5b83\u5f15\u5165\u4e86Binarised Virtual Slice Sets (BVSS)\u4ee5\u5728warp\u7ea7\u522b\u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\uff0c\u5e76\u4f7f\u7528\u4e24\u79cd\u56fe\u91cd\u6392\u5e8f\u7b56\u7565\u6765\u4f18\u5316\u5185\u5b58\u6548\u7387\u548c\u5c40\u90e8\u6027\u3002\u5728\u8ba1\u7b97\u5c42\u9762\uff0cBLEST\u5f00\u53d1\u4e86\u4e00\u79cd\u6279\u5904\u7406\u7684\u7a00\u758f\u77e9\u9635-\u7a00\u758f\u5411\u91cf\u4e58\u6cd5\u6a21\u5f0f\uff0c\u8be5\u6a21\u5f0f\u5229\u7528\u4f4d\u64cd\u4f5cTC\u74e6\u7247\u6765\u51cf\u5c11MMA\u8c03\u7528\u3002\u901a\u8fc7\u5185\u6838\u878d\u5408\u548c\u5ef6\u8fdf\u9876\u70b9\u66f4\u65b0\uff0cBLEST\u51cf\u5c11\u4e86\u540c\u6b65\u5f00\u9500\u3001\u539f\u5b50\u64cd\u4f5c\u5f00\u9500\u5e76\u63d0\u9ad8\u4e86\u7f13\u5b58\u5c40\u90e8\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cBLEST\u5728\u771f\u5b9e\u56fe\u4e2d\u6bd4\u73b0\u6709\u7684\u9ad8\u6027\u80fd\u6846\u67b6\u5e73\u5747\u5feb3.58\u500d\u52304.9\u500d\u3002", "motivation": "\u73b0\u4ee3GPU\uff0c\u5982NVIDIA\uff0c\u63d0\u4f9b\u4e86\u4e13\u7528\u7684\u5927\u541e\u5410\u91cf\u77e9\u9635\u4e58\u79ef\u7d2f\u52a0\uff08MMA\uff09\u5355\u5143\uff08\u5982Tensor Cores, TC\uff09\uff0c\u8fd9\u4e9b\u5355\u5143\u4e3b\u8981\u9488\u5bf9\u5bc6\u96c6\u64cd\u4f5c\u3002\u7136\u800c\uff0c\u56fe\u8ba1\u7b97\u4e2d\u7684\u6838\u5fc3\u7b97\u6cd5\uff0c\u5982\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\uff08BFS\uff09\uff0c\u672c\u8d28\u4e0a\u662f\u4e0d\u89c4\u5219\u4e14\u975e\u7ed3\u6784\u5316\u7684\u7a00\u758f\u64cd\u4f5c\u3002\u5982\u4f55\u6709\u6548\u5730\u5c06BFS\u7b49\u4e0d\u89c4\u5219\u56fe\u8ba1\u7b97\u6620\u5c04\u5230TC\u7b49\u5bc6\u96c6\u786c\u4ef6\u5355\u5143\u4e0a\uff0c\u540c\u65f6\u907f\u514d\u5197\u4f59\u3001\u8d1f\u8f7d\u4e0d\u5747\u8861\u548c\u540c\u6b65\u5f00\u9500\uff0c\u662f\u63d0\u9ad8\u56fe\u5904\u7406\u6027\u80fd\u7684\u5173\u952e\u6311\u6218\u3002\u8be5\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u5229\u7528TC\u7684\u5353\u8d8a\u541e\u5410\u91cf\u6765\u52a0\u901f\u56fe\u8ba1\u7b97\uff0c\u514b\u670d\u7a00\u758f\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "BLEST\u662f\u4e00\u4e2aTC\u52a0\u901f\u7684\u6846\u67b6\uff0c\u5b83\u56f4\u7ed5\u4f4d\u56fe\uff08bitmap-oriented\uff09\u7ed3\u6784\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6267\u884c\u5e03\u5c40\u91cd\u65b0\u6784\u5efa\u4e86\u62c9\u5f0f\uff08pull-based\uff09BFS\u6d41\u6c34\u7ebf\u3002\u5173\u952e\u65b9\u6cd5\u5305\u62ec\uff1a\n1. **Binarised Virtual Slice Sets (BVSS)**\uff1a\u5f15\u5165\u8be5\u673a\u5236\u4ee5\u5728warp\u7ea7\u522b\u5f3a\u5236\u8fdb\u884c\u8d1f\u8f7d\u5747\u8861\uff0c\u5e76\u6d88\u9664\u5bf9\u524d\u6cbf\uff08frontier\uff09\u4e0d\u654f\u611f\u7684\u5de5\u4f5c\u5206\u914d\u3002\n2. **\u56fe\u91cd\u6392\u5e8f\u7b56\u7565**\uff1a\u5e94\u7528\u4e24\u79cd\u4e92\u8865\u7684\u91cd\u6392\u5e8f\u7b56\u7565\u4ee5\u63d0\u9ad8\u5185\u5b58\u6548\u7387\u548c\u66f4\u65b0\u5c40\u90e8\u6027\uff1a\u9762\u5411\u538b\u7f29\u7684\u91cd\u6392\u5e8f\uff08\u9002\u7528\u4e8e\u793e\u4ea4\u56fe\uff09\u548c\u51cf\u5c11\u5e26\u5bbd\u7684\u91cd\u6392\u5e8f\uff08\u9002\u7528\u4e8e\u975e\u793e\u4ea4\u56fe\uff09\u3002\n3. **\u6279\u5904\u7406 SpMSpV \u4e58\u6cd5\u6a21\u5f0f**\uff1a\u5f00\u53d1\u4e86\u4e00\u79cd\u4f7f\u7528\u4f4d\u64cd\u4f5cTC\u74e6\u7247\uff08bitwise TC tiles\uff09\u7684\u6279\u5904\u7406\u7a00\u758f\u77e9\u9635-\u7a00\u758f\u5411\u91cf\u4e58\u6cd5\u6a21\u5f0f\uff0c\u4ee5\u5728\u4e0d\u6d6a\u8d39\u8f93\u51fa\u6761\u76ee\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u70b9\u79ef\uff0c\u4ece\u800c\u51cf\u5c11\u6240\u9700\u7684MMA\u8c03\u7528\u6b21\u6570\u3002\n4. **\u5185\u6838\u878d\u5408\u4e0e\u5ef6\u8fdf\u9876\u70b9\u66f4\u65b0**\uff1a\u7ed3\u5408\u5185\u6838\u878d\u5408\u548c\u5ef6\u8fdf\u9876\u70b9\u66f4\u65b0\u65b9\u6848\uff0c\u4ee5\u51cf\u5c11\u4e3b\u673a\u7aef\u540c\u6b65\u3001\u51cf\u8f7b\u539f\u5b50\u64cd\u4f5c\u5f00\u9500\u5e76\u63d0\u9ad8\u7f13\u5b58\u5c40\u90e8\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBLEST\u5728\u5e7f\u6cdb\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u5bf9\u4e8e\u73b0\u6709\u5148\u8fdb\u7684\u56fe\u5904\u7406\u6846\u67b6\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u5177\u4f53\u800c\u8a00\uff0c\u4e0e BerryBees \u76f8\u6bd4\uff0c\u5e73\u5747\u52a0\u901f\u6bd4\u8fbe\u5230 3.58 \u500d\uff1b\u4e0e Gunrock \u76f8\u6bd4\uff0c\u5e73\u5747\u52a0\u901f\u6bd4\u8fbe\u5230 4.64 \u500d\uff1b\u4e0e GSWITCH \u76f8\u6bd4\uff0c\u5e73\u5747\u52a0\u901f\u6bd4\u8fbe\u5230 4.9 \u500d\u3002", "conclusion": "BLEST\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u4ee5\u4f4d\u56fe\u4e3a\u5bfc\u5411\u7684\u62c9\u5f0fBFS\u6d41\u6c34\u7ebf\uff0c\u5e76\u7cbe\u5fc3\u8bbe\u8ba1\u6267\u884c\u5e03\u5c40\uff0c\u5c06\u56fe\u8ba1\u7b97\u7684\u9ad8\u6548\u6027\u4e0eGPU\u5f20\u91cf\u6838\u5fc3\uff08TC\uff09\u7684\u9ad8\u541e\u5410\u91cf\u76f8\u7ed3\u5408\u3002\u5b83\u5728\u5404\u79cd\u5b9e\u9645\u56fe\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e73\u5747\u52a0\u901f\u6bd4\u5206\u522b\u8fbe\u5230 BerryBees \u7684 3.58 \u500d\u3001Gunrock \u7684 4.64 \u500d\u548c GSWITCH \u7684 4.9 \u500d\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5229\u7528\u73b0\u4ee3GPU\u786c\u4ef6\u8fdb\u884c\u56fe\u7b97\u6cd5\u52a0\u901f\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.21884", "categories": ["cs.DC", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.21884", "abs": "https://arxiv.org/abs/2512.21884", "authors": ["Tingyang Sun", "Ting He", "Bo Ji", "Parimal Parag"], "title": "Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models", "comment": null, "summary": "Large language models have demonstrated extraordinary performance in many AI tasks but are expensive to use, even after training, due to their requirement of high-end GPUs. Recently, a distributed system called PETALS was developed to lower the barrier for deploying LLMs by splitting the model blocks across multiple servers with low-end GPUs distributed over the Internet, which was much faster than swapping the model parameters between the GPU memory and other cheaper but slower local storage media. However, the performance of such a distributed system critically depends on the resource allocation, and how to do so optimally remains unknown. In this work, we present the first systematic study of the resource allocation problem in distributed LLM inference, with focus on two important decisions: block placement and request routing. Our main results include: experimentally validated performance models that can predict the inference performance under given block placement and request routing decisions, a formulation of the offline optimization of block placement and request routing as a mixed integer linear programming problem together with the NP-hardness proof and a polynomial-complexity algorithm with guaranteed performance, and an adaptation of the offline algorithm for the online setting with the same performance guarantee under bounded load. Through both experiments and experimentally-validated simulations, we have verified that the proposed solution can substantially reduce the inference time compared to the state-of-the-art solution in diverse settings with geographically-distributed servers. As a byproduct, we have also developed a light-weighted CPU-only simulator capable of predicting the performance of distributed LLM inference on GPU servers, which can evaluate large deployments and facilitate future research for researchers with limited GPU access.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e0e\u7f16\u8bd1\u5668\u3001HLS\u3001DSL\u3001\u6216 MLIR \u65e0\u5173\u3002\u5b83\u4e0e\u56fe\u5904\u7406\uff08Graph Processing\uff09\u65e0\u76f4\u63a5\u5173\u8054\uff0c\u4f46\u6d89\u53ca\u5206\u5e03\u5f0f\u7cfb\u7edf\u3001\u6a21\u578b\u5757\uff08\u56fe\u7ed3\u6784\u6570\u636e\u7684\u4e00\u79cd\u8868\u793a\uff09\u7684\u653e\u7f6e\u548c\u901a\u4fe1\u4f18\u5316\uff0c\u5982\u679c\u5c06LLM\u6a21\u578b\u7ed3\u6784\u7406\u89e3\u4e3a\u4e00\u79cd\u8d85\u5927\u89c4\u6a21\u7684\u8ba1\u7b97\u56fe\uff0c\u90e8\u5206\u6a21\u578b\u5757\u5230GPU\u7684\u6620\u5c04\u548c\u901a\u4fe1\u53ef\u4ee5\u88ab\u6cdb\u5316\u7684\u89c6\u4e3a\u56fe\u5904\u7406\u548c\u4f18\u5316\u95ee\u9898\u7684\u4e00\u90e8\u5206\uff0c\u56e0\u6b64\u5728\u5e7f\u4e49\u4e0a\u4e0e\u56fe\u5904\u7406\u6709\u4e00\u4e01\u70b9\u5173\u8054\uff0c\u4e0d\u8fc7\u5b83\u4e3b\u8981\u7684\u6838\u5fc3\u662f**\u5206\u5e03\u5f0f\u7cfb\u7edf\uff08Distributed System\uff09**\u548c**\u8d44\u6e90\u8c03\u5ea6/\u7ba1\u7406\uff08Resource Scheduling/Management\uff09**\u3002\n\n\u603b\u7ed3\uff1a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u6210\u672c\u9ad8\u6602\uff0cPETALS\u7b49\u5206\u5e03\u5f0f\u7cfb\u7edf\u901a\u8fc7\u5c06\u6a21\u578b\u5757\u5206\u6563\u5230\u4f4e\u7aefGPU\u4e0a\u90e8\u7f72\uff0c\u4f46\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u8d44\u6e90\u5206\u914d\u3002\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u5206\u5e03\u5f0fLLM\u63a8\u7406\u4e2d\u7684\u5757\u653e\u7f6e\u548c\u8bf7\u6c42\u8def\u7531\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7ecf\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u6027\u80fd\u6a21\u578b\u3001NP-\u96be\u6027\u8bc1\u660e\uff0c\u4ee5\u53ca\u4e00\u79cd\u5177\u6709\u6027\u80fd\u4fdd\u8bc1\u7684\u591a\u9879\u5f0f\u590d\u6742\u5ea6\u79bb\u7ebf\u548c\u5728\u7ebf\u4f18\u5316\u7b97\u6cd5\u3002\u5b9e\u9a8c\u548c\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u80fd\u663e\u8457\u7f29\u77ed\u63a8\u7406\u65f6\u95f4\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u7eafCPU\u6a21\u62df\u5668\u7528\u4e8e\u6027\u80fd\u9884\u6d4b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u6210\u672c\u9ad8\u6602\uff0c\u5373\u4f7f\u5728\u8bad\u7ec3\u540e\u4e5f\u9700\u8981\u9ad8\u7aef GPU\u3002\u73b0\u6709\u7684 PETALS \u7b49\u5206\u5e03\u5f0f\u7cfb\u7edf\u901a\u8fc7\u5c06\u6a21\u578b\u5757\u5206\u914d\u5230\u4e92\u8054\u7f51\u4e0a\u5206\u5e03\u7684\u4f4e\u7aef GPU \u4e0a\u4ee5\u964d\u4f4e\u90e8\u7f72\u95e8\u69db\uff0c\u4f46\u8fd9\u5e26\u6765\u4e86\u4e25\u5cfb\u7684\u8d44\u6e90\u5206\u914d\u6311\u6218\uff1a\u5982\u4f55\u6700\u4f18\u5730\u8fdb\u884c\u6a21\u578b\u5757\u653e\u7f6e\u548c\u8bf7\u6c42\u8def\u7531\uff0c\u4ee5\u907f\u514d\u7cfb\u7edf\u6027\u80fd\u53d7\u5176\u5173\u952e\u4f9d\u8d56\u6027\u7684\u9650\u5236\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u7814\u7a76\u5206\u5e03\u5f0f LLM \u63a8\u7406\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u4ee5\u4f18\u5316\u6027\u80fd\u3002", "method": "\u672c\u6587\u9996\u5148\u63d0\u51fa\u4e86\u7ecf\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u6027\u80fd\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u7ed9\u5b9a\u5757\u653e\u7f6e\u548c\u8bf7\u6c42\u8def\u7531\u51b3\u7b56\u4e0b\u7684\u63a8\u7406\u6027\u80fd\u3002\u57fa\u4e8e\u6b64\u6a21\u578b\uff0c\u5c06\u79bb\u7ebf\u4f18\u5316\u95ee\u9898\u8868\u8ff0\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4e86\u5176 NP-\u96be\u6027\uff0c\u7136\u540e\u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709\u6027\u80fd\u4fdd\u8bc1\u7684\u591a\u9879\u5f0f\u590d\u6742\u5ea6\u7b97\u6cd5\u3002\u6700\u540e\uff0c\u5c06\u79bb\u7ebf\u7b97\u6cd5\u8c03\u6574\u7528\u4e8e\u5728\u7ebf\u8bbe\u7f6e\uff0c\u5728\u6709\u754c\u8d1f\u8f7d\u4e0b\u5177\u6709\u76f8\u540c\u7684\u6027\u80fd\u4fdd\u8bc1\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u7eaf CPU \u6a21\u62df\u5668\uff0c\u7528\u4e8e\u9884\u6d4b\u5206\u5e03\u5f0f LLM \u63a8\u7406\u7684\u6027\u80fd\u3002", "result": "\u4e3b\u8981\u6210\u679c\u5305\u62ec\uff1a1) \u5b9e\u9a8c\u9a8c\u8bc1\u7684\u6027\u80fd\u6a21\u578b\uff0c\u80fd\u591f\u9884\u6d4b\u7ed9\u5b9a\u5757\u653e\u7f6e\u548c\u8bf7\u6c42\u8def\u7531\u51b3\u7b56\u4e0b\u7684\u63a8\u7406\u6027\u80fd\uff1b2) \u9996\u6b21\u5c06\u5757\u653e\u7f6e\u548c\u8bf7\u6c42\u8def\u7531\u7684\u79bb\u7ebf\u4f18\u5316\u95ee\u9898\u516c\u5f0f\u5316\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4e86\u5176 NP-\u96be\u6027\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709\u4fdd\u8bc1\u6027\u80fd\u7684\u591a\u9879\u5f0f\u590d\u6742\u5ea6\u7b97\u6cd5\uff1b3) \u5c06\u79bb\u7ebf\u7b97\u6cd5\u5e94\u7528\u4e8e\u5728\u7ebf\u8bbe\u7f6e\uff0c\u5728\u6709\u754c\u8d1f\u8f7d\u4e0b\u5177\u6709\u76f8\u540c\u7684\u6027\u80fd\u4fdd\u8bc1\u3002\u901a\u8fc7\u5b9e\u9a8c\u548c\u4eff\u771f\u9a8c\u8bc1\uff0c\u6240\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u76f8\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6848\u80fd\u591f\u663e\u8457\u7f29\u77ed\u5728\u5730\u57df\u5206\u5e03\u5f0f\u670d\u52a1\u5668\u4e0a\u7684\u63a8\u7406\u65f6\u95f4\u3002\u6b64\u5916\uff0c\u526f\u4ea7\u54c1\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u7eaf CPU \u7684\u6a21\u62df\u5668\uff0c\u53ef\u9884\u6d4b\u5206\u5e03\u5f0f LLM \u63a8\u7406\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u5206\u5e03\u5f0f LLM \u63a8\u7406\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7ecf\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u6027\u80fd\u6a21\u578b\u3001\u79bb\u7ebf\u548c\u5728\u7ebf\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u5728\u5b9e\u9a8c\u548c\u4eff\u771f\u4e2d\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6848\u76f8\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6848\u80fd\u663e\u8457\u7f29\u77ed\u63a8\u7406\u65f6\u95f4\u3002\u8fd9\u8868\u660e\u4e86\u901a\u8fc7\u4f18\u5316\u8d44\u6e90\u5206\u914d\uff0c\u53ef\u4ee5\u514b\u670d\u5206\u5e03\u5f0f LLM \u63a8\u7406\u7cfb\u7edf\u7684\u6027\u80fd\u74f6\u9888\uff0c\u964d\u4f4e\u90e8\u7f72\u6210\u672c\uff0c\u540c\u65f6\u7814\u7a76\u6240\u5f00\u53d1\u7684\u8f7b\u91cf\u7ea7\u6a21\u62df\u5668\u53ef\u4ee5\u6709\u6548\u652f\u6301\u672a\u6765\u7684\u7814\u7a76\u548c\u90e8\u7f72\u8bc4\u4f30\u3002"}}
{"id": "2512.22036", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22036", "abs": "https://arxiv.org/abs/2512.22036", "authors": ["Zhuoran Zhu", "Chunyang Zhu", "Hao Lin", "Xu Fu", "Yiming Zhou", "Quanlu Zhang", "Zhenhua Li", "Feng Qian", "Chao Yu", "Boxun Li", "Guohao Dai", "Yu Wang"], "title": "FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion", "comment": null, "summary": "Large-scale Mixture-of-Experts (MoE) models rely on \\emph{expert parallelism} for efficient training and inference, which splits experts across devices and necessitates distributed data shuffling to route each token to its assigned experts. However, existing communication libraries handle this shuffling poorly; its overhead can account for over half of end-to-end runtime. We present FUSCO, an MoE-friendly communication library that achieves efficient and lightweight data shuffling through fused data transformation and communication, based on the key observation that MoE's expert-major data layout conflicts with the device-major layout expected by communication operations. FUSCO captures the fine-grained data layout, which is then interpreted by a pipelined communication engine that performs the required shuffling efficiently along the communication path. Lightweight planning and load-balancing mechanisms complement the engine by eliminating redundant communication and dispersing traffic. Evaluations on representative benchmarks illustrate that FUSCO achieves up to 3.84$\\times$ and 2.01$\\times$ speedups over NCCL and DeepEP (the state-of-the-art MoE communication library), respectively. In end-to-end MoE tasks, compared to NCCL and DeepEP, FUSCO reduces the training latency by 1.17-1.39$\\times$ and 1.10-1.19$\\times$, and lowers the first-token generation latency in inference by 1.09-1.25$\\times$ and 1.06-1.16$\\times$.", "AI": {"tldr": "\u6d89\u53ca\u9886\u57df\uff1a\u7f16\u8bd1\u5668/\u901a\u4fe1\u5e93\u3001\u56fe\u5904\u7406\uff08\u6570\u636e\u8def\u7531\uff09\u3001DSL\uff08\u65e0\uff09\u3002MoE \u6a21\u578b\u4f9d\u8d56\u4e13\u5bb6\u5e76\u884c\uff0c\u9700\u8981\u9ad8\u6548\u7684\u6570\u636e\u6df7\u6d17\u3002\u4f46\u73b0\u6709\u901a\u4fe1\u5e93\u5904\u7406\u8fd9\u79cd\u6df7\u6d17\u6548\u7387\u4f4e\uff0c\u5f00\u9500\u5927\u3002FUSCO \u662f\u4e00\u4e2a MoE \u53cb\u597d\u7684\u901a\u4fe1\u5e93\uff0c\u901a\u8fc7\u878d\u5408\u6570\u636e\u8f6c\u6362\u548c\u901a\u4fe1\uff0c\u89e3\u51b3\u4e86 MoE \u4e13\u5bb6\u4e3b\u5bfc\u5e03\u5c40\u4e0e\u901a\u4fe1\u5e93\u8bbe\u5907\u4e3b\u5bfc\u5e03\u5c40\u7684\u51b2\u7a81\u3002\u5b83\u4f7f\u7528\u6d41\u6c34\u7ebf\u901a\u4fe1\u5f15\u64ce\u3001\u8f7b\u91cf\u7ea7\u89c4\u5212\u548c\u8d1f\u8f7d\u5e73\u8861\u673a\u5236\uff0c\u6709\u6548\u6267\u884c\u6df7\u6d17\u3002FUSCO \u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4 NCCL \u548c DeepEP \u5feb 3.84 \u500d\u548c 2.01 \u500d\uff0c\u5728\u7aef\u5230\u7aef MoE \u4efb\u52a1\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u5927\u89c4\u6a21\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u4e3a\u4e86\u9ad8\u6548\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u4f9d\u8d56\u4e8e\u201c\u4e13\u5bb6\u5e76\u884c\u201d\uff0c\u9700\u8981\u5206\u5e03\u5f0f\u6570\u636e\u6df7\u6d17\u5c06\u6bcf\u4e2a\u4ee4\u724c\u8def\u7531\u5230\u5176\u6307\u5b9a\u7684\u4e13\u5bb6\u3002\u7136\u800c\uff0c\u73b0\u6709\u901a\u4fe1\u5e93\u5904\u7406\u8fd9\u79cd\u6df7\u6d17\u64cd\u4f5c\u6548\u7387\u4f4e\u4e0b\uff0c\u5176\u5f00\u9500\u53ef\u4ee5\u5360\u5230\u6574\u4f53\u8fd0\u884c\u65f6\u95f4\u7684\u4e00\u534a\u4ee5\u4e0a\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u4e13\u4e3a MoE \u8bbe\u8ba1\u7684\u3001\u80fd\u591f\u9ad8\u6548\u5904\u7406\u6570\u636e\u6df7\u6d17\u7684\u901a\u4fe1\u5e93\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u5e93\u5728\u5904\u7406 MoE \u4e13\u5bb6\u4e3b\u5bfc\u6570\u636e\u5e03\u5c40\u51b2\u7a81\u65f6\u7684\u6027\u80fd\u74f6\u9888\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86 FUSCO\uff0c\u8fd9\u662f\u4e00\u4e2a MoE \u53cb\u597d\u7684\u901a\u4fe1\u5e93\u3002FUSCO \u7684\u6838\u5fc3\u65b9\u6cd5\u662f\u57fa\u4e8e MoE \u6a21\u578b\u7684\u201c\u4e13\u5bb6\u4e3b\u5bfc\u201d\u6570\u636e\u5e03\u5c40\u4e0e\u73b0\u6709\u901a\u4fe1\u64cd\u4f5c\u6240\u9700\u7684\u201c\u8bbe\u5907\u4e3b\u5bfc\u201d\u5e03\u5c40\u4e4b\u95f4\u7684\u51b2\u7a81\u89c2\u5bdf\uff0c\u901a\u8fc7\u878d\u5408\u6570\u636e\u8f6c\u6362\u548c\u901a\u4fe1\u6765\u5b9e\u73b0\u9ad8\u6548\u8f7b\u91cf\u7ea7\u7684\u6570\u636e\u6df7\u6d17\u3002\u5177\u4f53\u5b9e\u65bd\u4e0a\uff0cFUSCO \u6355\u83b7\u7ec6\u7c92\u5ea6\u6570\u636e\u5e03\u5c40\uff0c\u7136\u540e\u7531\u6d41\u6c34\u7ebf\u901a\u4fe1\u5f15\u64ce\u8fdb\u884c\u89e3\u91ca\uff0c\u5728\u901a\u4fe1\u8def\u5f84\u4e0a\u6709\u6548\u5730\u6267\u884c\u6240\u9700\u7684\u6df7\u6d17\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea7\u89c4\u5212\u548c\u8d1f\u8f7d\u5e73\u8861\u673a\u5236\u6765\u6d88\u9664\u5197\u4f59\u901a\u4fe1\u5e76\u5206\u6563\u6d41\u91cf\u3002", "result": "FUSCO \u5728\u4ee3\u8868\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u4e0e NCCL \u76f8\u6bd4\uff0cFUSCO \u5b9e\u73b0\u4e86\u9ad8\u8fbe 3.84 \u500d\u7684\u52a0\u901f\uff1b\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684 MoE \u901a\u4fe1\u5e93 DeepEP \u76f8\u6bd4\uff0cFUSCO \u5b9e\u73b0\u4e86\u9ad8\u8fbe 2.01 \u500d\u7684\u52a0\u901f\u3002\u5728\u7aef\u5230\u7aef MoE \u4efb\u52a1\u4e2d\uff0c\u4e0e NCCL \u548c DeepEP \u76f8\u6bd4\uff0cFUSCO \u5206\u522b\u5c06\u8bad\u7ec3\u5ef6\u8fdf\u964d\u4f4e\u4e86 1.17-1.39 \u500d\u548c 1.10-1.19 \u500d\uff0c\u5e76\u5c06\u63a8\u7406\u4e2d\u7684\u9996\u4e2a\u4ee4\u724c\u751f\u6210\u5ef6\u8fdf\u964d\u4f4e\u4e86 1.09-1.25 \u500d\u548c 1.06-1.16 \u500d\u3002", "conclusion": "FUSCO \u901a\u8fc7\u878d\u5408\u6570\u636e\u8f6c\u6362\u548c\u901a\u4fe1\u7684\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd MoE \u53cb\u597d\u7684\u3001\u9ad8\u6548\u4e14\u8f7b\u91cf\u7ea7\u7684\u6570\u636e\u6df7\u6d17\u673a\u5236\u3002\u5b83\u89e3\u51b3\u4e86 MoE \u56fa\u6709\u7684\u4e13\u5bb6\u4e3b\u5bfc\u6570\u636e\u5e03\u5c40\u4e0e\u73b0\u6709\u901a\u4fe1\u5e93\u671f\u671b\u7684\u8bbe\u5907\u4e3b\u5bfc\u5e03\u5c40\u4e4b\u95f4\u7684\u51b2\u7a81\u95ee\u9898\u3002FUSCO \u663e\u8457\u51cf\u5c11\u4e86\u5927\u89c4\u6a21 MoE \u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u7279\u522b\u662f\u5728 MoE \u6a21\u578b\u7684\u4e13\u5bb6\u5e76\u884c\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
