<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DS](#cs.DS) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2512.15766)
*Yijie Zhi,Yayu Cao,Jianhua Dai,Xiaoyang Han,Jingwen Pu,Qingran Wu,Sheng Cheng,Ming Cai*

Main category: cs.PL

TL;DR: 该论文与编译器和循环优化相关。LOOPRAG是一种新的检索增强生成（RAG）框架，用于指导大型语言模型（LLM）进行有效的循环优化，特别是在静态控制部分。它通过参数驱动的循环属性、平衡相似性和多样性的循环感知检索算法，以及基于反馈的迭代机制来克服LLM在循环优化中易出错和次优的问题。通过在PolyBench、TSVC和LORE上的评估，LOOPRAG显著超越了传统编译器和基础LLM，实现了高达14.34倍的加速比。


<details>
  <summary>Details</summary>
Motivation: 尽管循环变换是广泛使用的优化技术，但选择最佳的循环变换组合仍然具有挑战性，部分原因是成本建模的复杂性。虽然最近的研究探索了LLM在代码优化中的潜力，但作者观察到LLM在有效的循环变换优化方面经常表现不佳，容易出错或产生次优结果。因此，提出LOOPRAG框架旨在通过指导LLM来弥补这一差距，以实现有效的循环优化。

Method: LOOPRAG是一种新的检索增强生成（RAG）框架，用于指导LLM对静态控制部分执行有效的循环优化。该方法包括：1. 引入参数驱动方法，利用循环属性触发各种循环变换，生成多样化且合法的示例代码作为演示源。2. 提出一种基于循环特征的“循环感知”算法，用于检索最具信息量的演示，平衡代码检索的相似性和多样性。3. 引入基于反馈的迭代机制，包含编译、测试和性能结果作为反馈来指导LLM，以增强代码的正确性和效率。4. 对每个优化的代码进行变异、覆盖和差异测试以进行等价性检查。

Result: 在PolyBench、TSVC和LORE基准套件上的评估结果显示，LOOPRAG相对于基础编译器（GCC-Graphite, Clang-Polly, Perspective和ICX）获得了显著的平均加速比：PolyBench高达11.20倍，TSVC高达14.34倍，LORE高达9.29倍。同时，相对于基础LLM（DeepSeek和GPT-4），加速比也高达：PolyBench 11.97倍，TSVC 5.61倍，LORE 11.59倍。

Conclusion: LOOPRAG通过引入参数驱动的循环属性、平衡相似性和多样性的循环感知检索算法，以及基于反馈的迭代机制，成功地使大型语言模型能够更有效地进行循环优化，从而在多个基准测试中显著超越了传统的编译器和基础LLM。这为代码优化领域中LLM的应用开辟了新的道路。

Abstract: Loop transformations are semantics-preserving optimization techniques, widely used to maximize objectives such as parallelism. Despite decades of research, applying the optimal composition of loop transformations remains challenging due to inherent complexities, including cost modeling for optimization objectives. Recent studies have explored the potential of Large Language Models (LLMs) for code optimization. However, our key observation is that LLMs often struggle with effective loop transformation optimization, frequently leading to errors or suboptimal optimization, thereby missing opportunities for performance improvements. To bridge this gap, we propose LOOPRAG, a novel retrieval-augmented generation framework designed to guide LLMs in performing effective loop optimization on Static Control Part. We introduce a parameter-driven method to harness loop properties, which trigger various loop transformations, and generate diverse yet legal example codes serving as a demonstration source. To effectively obtain the most informative demonstrations, we propose a loop-aware algorithm based on loop features, which balances similarity and diversity for code retrieval. To enhance correct and efficient code generation, we introduce a feedback-based iterative mechanism that incorporates compilation, testing and performance results as feedback to guide LLMs. Each optimized code undergoes mutation, coverage and differential testing for equivalence checking. We evaluate LOOPRAG on PolyBench, TSVC and LORE benchmark suites, and compare it against compilers (GCC-Graphite, Clang-Polly, Perspective and ICX) and representative LLMs (DeepSeek and GPT-4). The results demonstrate average speedups over base compilers of up to 11.20$\times$, 14.34$\times$, and 9.29$\times$ for PolyBench, TSVC, and LORE, respectively, and speedups over base LLMs of up to 11.97$\times$, 5.61$\times$, and 11.59$\times$.

</details>


### [2] [Automated Formalization of Probabilistic Requirements from Structured Natural Language](https://arxiv.org/abs/2512.15788)
*Anastasia Mavridou,Marie Farrell,Gricel Vázquez,Tom Pressburger,Timothy E. Wang,Radu Calinescu,Michael Fisher*

Main category: cs.PL

TL;DR: 这个论文与编译器相关，它是一种用于将结构化自然语言需求转化为形式化逻辑公式的工具，可以看作是一种高级的语言处理和转换。
自主和自适应系统中的不确定性使得需求规约非常困难，尤其是在需要形式化分析的概率需求方面。由于要求开发人员直接用复杂的概率逻辑编写需求不切实际且容易出错，本文扩展了 NASA FRET 工具，使其能够支持结构化自然语言的概率需求规约，并开发了一种形式化、组合式和自动化的方法，将这些结构化需求翻译成概率时态逻辑公式。通过提供自动验证框架和形式化证明，确保了生成公式的正确性。这使得自主和自适应系统的形式化分析更加实用且不易出错。


<details>
  <summary>Details</summary>
Motivation: 在软件密集型系统中集成自主和自适应行为对软件开发构成重大挑战，尤其是在安全关键和任务关键系统中，因为需要明确捕捉环境或决策过程中的不确定性。用概率结构来捕捉影响这些系统的不确定性需求规约非常困难。要求开发人员直接使用复杂的概率逻辑等形式化语言来编写需求是不现实且容易出错的。

Method: 扩展了 NASA FRET 工具以支持结构化自然语言的概率需求规约。开发了一种自动方法，用于将这些结构化自然语言需求转化（翻译）为概率时态逻辑公式。提出了形式化、组合式和自动化的翻译方法，并提供了自动化的验证框架和形式化证明来提高对转化结果的信任。

Result: 成功扩展了 FRET 工具，使其能够支持使用结构化自然语言规约无歧义且正确的概率需求，并能自动将这些需求翻译成概率时态逻辑公式。所提出的翻译方法是形式化、组合式和自动化的。通过自动验证框架和形式化证明，证明了所生成公式的良构性和符合预期语义，增加了对形式化结果的信任。

Conclusion: 本文通过扩展 NASA 的 FRET 工具，使其支持结构化自然语言的概率需求规约，并将这些需求自动翻译成概率时态逻辑公式，从而使自主和自适应系统中的形式化分析更加实用且不易出错。通过提供自动验证框架和形式化证明，确保了生成公式的正确性。

Abstract: Integrating autonomous and adaptive behavior into software-intensive systems presents significant challenges for software development, as uncertainties in the environment or decision-making processes must be explicitly captured. These challenges are amplified in safety- and mission-critical systems, which must undergo rigorous scrutiny during design and development. Key among these challenges is the difficulty of specifying requirements that use probabilistic constructs to capture the uncertainty affecting these systems. To enable formal analysis, such requirements must be expressed in precise mathematical notations such as probabilistic logics. However, expecting developers to write requirements directly in complex formalisms is unrealistic and highly error-prone. We extend the structured natural language used by NASA's Formal Requirement Elicitation Tool (FRET) with support for the specification of unambiguous and correct probabilistic requirements, and develop an automated approach for translating these requirements into logical formulas. We propose and develop a formal, compositional, and automated approach for translating structured natural-language requirements into formulas in probabilistic temporal logic. To increase trust in our formalizations, we provide assurance that the generated formulas are well-formed and conform to the intended semantics through an automated validation framework and a formal proof. The extended FRET tool enables developers to specify probabilistic requirements in structured natural language, and to automatically translate them into probabilistic temporal logic, making the formal analysis of autonomous and adaptive systems more practical and less error-prone.

</details>


### [3] [A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning](https://arxiv.org/abs/2512.15816)
*Daragh King,Vasileios Koutavas,Laura Kovacs*

Main category: cs.PL

TL;DR: 本文与编译器相关。它探讨了使用神经符号方法（NeuroInv）生成循环不变量，这是程序验证中的一个重要组成部分。NeuroInv 结合了 LLM 和霍尔逻辑进行不变量推导和优化，并使用验证工具的反例进行修复。在包含 150 个 Java 程序的基准测试中，NeuroInv 取得了 99.5% 的成功率，显著优于其他方法，并显示出处理更复杂多循环程序的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 循环不变量的生成仍然是自动化程序验证中的关键瓶颈。尽管最近的工作开始探索在这一领域使用大型语言模型（LLMs），但这些方法往往缺乏可靠和结构化的方法论，并且很少参考现有的程序验证理论。本文旨在提供一个结合 LLM 和程序验证理论的、结构化的方法来解决这一瓶颈。

Method: NeuroInv 包含两个关键模块：（1）神经推理模块：利用大型语言模型（LLMs）和霍尔逻辑（Hoare logic），通过反向链式最弱前置条件推理（backward-chaining weakest precondition reasoning）来推导和优化候选不变量。（2）验证引导的符号模块：利用来自 OpenJML 的反例（counterexamples）迭代地修复不变量。该方法在 150 个 Java 程序的综合基准测试上进行了评估，包括单循环和多循环、多个数组、随机分支和带噪声的代码段，并额外引入了 10 个更大的多循环程序的困难基准。

Result: NeuroInv 在包含单循环和多循环、多个数组、随机分支和带噪声代码段的 150 个 Java 程序的综合基准测试中，取得了 99.5% 的成功率，显著优于其他被评估的方法。此外，在包含 10 个大型多循环程序的困难基准测试上，NeuroInv 的表现证明了其可以扩展到更复杂的验证场景。

Conclusion: 本文提出的 NeuroInv 是一种神经符号方法，通过结合 LLM 和形式验证理论，弥补了先前 LLM 方法在结构化和理论依据上的不足。NeuroInv 在包含单循环和多循环的复杂基准测试中取得了 99.5% 的成功率，并在更复杂的验证场景中展示了其可扩展性，显著优于其他方法，表明了将 LLM 与程序验证理论相结合生成循环不变量的有效性。

Abstract: Loop invariant generation remains a critical bottleneck in automated program verification. Recent work has begun to explore the use of Large Language Models (LLMs) in this area, yet these approaches tend to lack a reliable and structured methodology, with little reference to existing program verification theory. This paper presents NeuroInv, a neurosymbolic approach to loop invariant generation. NeuroInv comprises two key modules: (1) a neural reasoning module that leverages LLMs and Hoare logic to derive and refine candidate invariants via backward-chaining weakest precondition reasoning, and (2) a verification-guided symbolic module that iteratively repairs invariants using counterexamples from OpenJML. We evaluate NeuroInv on a comprehensive benchmark of 150 Java programs, encompassing single and multiple (sequential) loops, multiple arrays, random branching, and noisy code segments. NeuroInv achieves a $99.5\%$ success rate, substantially outperforming the other evaluated approaches. Additionally, we introduce a hard benchmark of $10$ larger multi-loop programs (with an average of $7$ loops each); NeuroInv's performance in this setting demonstrates that it can scale to more complex verification scenarios.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [LOG.io: Unified Rollback Recovery and Data Lineage Capture for Distributed Data Pipelines](https://arxiv.org/abs/2512.16038)
*Eric Simon,Renato B. Hoffmann,Lucas Alf,Dalvan Griebler*

Main category: cs.DC

TL;DR: 该论文与**图处理（分布式数据管道）**和**编译器/HLS（编程模型和性能优化）**相关。

**太长不看版 (TL;DR):** 本文介绍了 LOG.io，这是一个专为无服务器分布式数据管道设计的解决方案，用于**正确的日志回滚恢复和细粒度数据沿袭捕获**。它支持**通用编程模型**，具有**非阻塞**恢复和**动态扩展**能力。与 Flink 的 ABS 协议相比，当存在**拖慢操作符且吞吐量适中**时，LOG.io 在恢复上表现更优；在其他情况下，虽然 ABS 更好，但 LOG.io 可以通过**数据并行化**显著减少开销。此外，捕获**事件级数据沿袭的开销微不足道**（小于 1.5%）。


<details>
  <summary>Details</summary>
Motivation: 在分布式数据管道中，面临着如何实现**正确的故障恢复**（回滚恢复）和**细粒度的数据沿袭捕获**的挑战，尤其是在无服务器可扩展架构和需要支持通用编程模型（如非确定性操作符、外部交互）的场景中。传统的恢复协议（如 ABS）在某些场景下（例如存在拖慢操作符）可能效率不高，并且缺乏细粒度的数据沿袭能力。LOG.io 的动机是提供一个综合、高效且灵活的解决方案来解决这些问题，同时保持对其他活动操作符的非阻塞性和管道的可扩展性。

Method: 该论文介绍了 LOG.io，这是一个用于分布式数据管道的综合解决方案，旨在实现正确的**回滚恢复**和**细粒度数据沿袭捕获**。它为无服务器可扩展架构量身定制，使用基于**日志**的回滚恢复协议。LOG.io 支持一个**通用编程模型**，包括非确定性操作符、与外部系统的交互以及任意自定义代码。其恢复机制是**非阻塞**的，允许故障操作符独立恢复，利用**数据并行化**，并支持在管道执行期间**动态扩展**操作符。性能评估是在 SAP Data Intelligence 系统中进行的，LOG.io 与最初在 Flink 中实现的 ABS（异步屏障快照）协议进行了比较。

Result: 性能评估在 SAP Data Intelligence 系统中进行，LOG.io 与 ABS 协议进行了比较：1. **存在拖慢操作符且事件吞吐量适中**（例如，每 100 毫秒 1 个事件）时，LOG.io 在正常处理期间与 ABS 性能**一样好**，但在恢复期间**优于** ABS。2. **在其他情况下**，ABS 在正常处理和恢复方面都**优于** LOG.io。然而，在这种情况下，**数据并行化**可以**在很大程度上减少 LOG.io 的开销**，而 ABS 则没有改进。3. 细粒度（事件级别和任意两个操作符之间）**数据沿袭捕获的开销很小**，在所有实验中都**低于 1.5%**。

Conclusion: LOG.io 是一种在分布式数据管道中实现正确回滚恢复和细粒度数据沿袭捕获的综合解决方案，尤其适用于无服务器可扩展架构。它采用基于日志的回滚恢复协议，支持通用的编程模型（包括非确定性操作符、外部系统交互和任意自定义代码），并且具有非阻塞特性，允许故障操作符独立恢复而不中断其他操作符。实验表明，在存在拖慢操作符且事件吞吐量适中（例如，每 100 毫秒 1 个事件）的情况下，LOG.io 在正常处理和恢复期间的表现与 ABS 相当或优于 ABS。在其他情况下，ABS 表现更好，但数据并行化可以显著减少 LOG.io 的开销而 ABS 没有改进。此外，LOG.io 捕获数据沿袭的开销非常小。这项研究为分布式数据管道的可靠性和可观测性提供了一种高效的替代方案。

Abstract: This paper introduces LOG.io, a comprehensive solution designed for correct rollback recovery and fine-grain data lineage capture in distributed data pipelines. It is tailored for serverless scalable architectures and uses a log-based rollback recovery protocol. LOG.io supports a general programming model, accommodating non-deterministic operators, interactions with external systems, and arbitrary custom code. It is non-blocking, allowing failed operators to recover independently without interrupting other active operators, thereby leveraging data parallelization, and it facilitates dynamic scaling of operators during pipeline execution. Performance evaluations, conducted within the SAP Data Intelligence system, compare LOG.io with the Asynchronous Barrier Snapshotting (ABS) protocol, originally implemented in Flink. Our experiments show that when there are straggler operators in a data pipeline and the throughput of events is moderate (e.g., 1 event every 100 ms), LOG.io performs as well as ABS during normal processing and outperforms ABS during recovery. Otherwise, ABS performs better than LOG.io for both normal processing and recovery. However, we show that in these cases, data parallelization can largely reduce the overhead of LOG.io while ABS does not improve. Finally, we show that the overhead of data lineage capture, at the granularity of the event and between any two operators in a pipeline, is marginal, with less than 1.5% in all our experiments.

</details>


### [5] [MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services](https://arxiv.org/abs/2512.16056)
*Lingfeng Tang,Daoping Zhang,Junjie Chen,Peihao Huang,Feng Jin,Chengguang Xu,Yuxin Chen,Feiqiang Sun,Guo Chen*

Main category: cs.DC

TL;DR: 该论文与编译器（compiler）相关，因为它涉及到LLM服务的性能优化和系统级的数据传输机制，这在广义上属于编译和系统优化的范畴，尽管它更侧重于**系统和硬件**层面的优化。本文提出多路径内存访问（MMA）方案，利用服务器内多路径连接（如PCIe和NVLink）实现GPU与主机内存间高效数据传输。MMA通过动态库注入，无需代码修改即可部署，实现245GB/s的传输带宽（4.62倍加速）。端到端测试表明，MMA将LLM服务的TTFT缩短1.14x-2.38x，模型切换延迟降低1.12x-2.48x。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）的性能受到PCIe带宽的限制，尤其是对于前缀缓存获取和模型切换等场景。虽然服务器内部理论上存在GPU与主机内存之间的多路径数据传输能力，但异构协议（如PCIe和NVLink）的限制导致实际带宽仅限于单个PCIe链路，造成服务器内部带宽的未充分利用。因此，需要一种能够充分利用多路径带宽来加速GPU与主机内存之间数据传输的方案。

Method: 提出并实现了多路径内存访问（Multipath Memory Access, MMA）方案。MMA利用服务器内的多路径连接（如PCle和NVLink）来实现GPU和主机内存之间的高效数据传输，打破了传统单路径PCIe带宽的限制。该方案通过动态库注入的方式实现，无需修改LLM应用程序代码即可部署。

Result: 在测试环境中，MMA将GPU与内存之间的数据传输带宽提升至245 GB/s，相比原生单路径带宽实现了4.62倍的加速。端到端评估结果显示，MMA将LLM服务的首个Token生成时间（TTFT）缩短了1.14倍至2.38倍，并将vLLM休眠模式下的模型切换延迟降低了1.12倍至2.48倍。

Conclusion: MMA通过动态库注入的方式实现了GPU与主机内存之间的高效多路径数据传输，显著提高了数据传输带宽，并在LLM服务中降低了TTFT和模型切换延迟，证明了其在解决LLM性能瓶颈方面的有效性。

Abstract: The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.

</details>


### [6] [Cold-Start Anti-Patterns and Refactorings in Serverless Systems: An Empirical Study](https://arxiv.org/abs/2512.16066)
*Syed Salauddin Mohammad Tariq,Foyzul Hassan,Amiangshu Bosu,Probir Roy*

Main category: cs.DC

TL;DR: 本文不涉及DSL、图处理、MLIR、编译器或HLS。
TLDR: Serverless计算中的冷启动延迟是一个性能瓶颈。本文通过分析开源Serverless系统的问题报告，导出了反模式和诊断挑战的分类，并提出了SCABENCH基准测试和INITSCOPE分析框架。INITSCOPE显著提高了对冷启动问题的定位准确性和诊断效率，促进了Serverless设计中以性能为导向的冷启动缓解实践。


<details>
  <summary>Details</summary>
Motivation: Serverless计算的冷启动延迟是主要的性能瓶颈。作者认为以前的工作将缓解措施视为黑盒优化，而本文旨在将冷启动作为一个对开发者可见的设计问题来研究。

Method: 研究了81份开源Serverless系统的问题报告，导出了初始化反模式、修复策略和诊断挑战的分类法。在此基础上，引入了SCABENCH可复现基准测试和INITSCOPE轻量级分析框架（连接代码加载和执行）。将INITSCOPE与现有工具进行了比较，并进行了开发者研究。

Result: 在SCABENCH上，INITSCOPE将定位准确性提高了高达40%，与现有工具相比，诊断工作量减少了64%。开发者研究表明，其任务准确性更高，诊断速度更快。这些结果促进了在Serverless设计中采用以证据驱动、性能感知的方法来缓解冷启动。

Conclusion: 本文通过深入研究冷启动问题，提出了SCABENCH基准测试和INITSCOPE轻量级分析框架，提高了冷启动问题的诊断效率和准确性，有助于推动在Serverless设计中采用以证据驱动、性能感知的方法来缓解冷启动。

Abstract: Serverless computing simplifies deployment and scaling, yet cold-start latency remains a major performance bottleneck. Unlike prior work that treats mitigation as a black-box optimization, we study cold starts as a developer-visible design problem. From 81 adjudicated issue reports across open-source serverless systems, we derive taxonomies of initialization anti-patterns, remediation strategies, and diagnostic challenges spanning design, packaging, and runtime layers. Building on these insights, we introduce SCABENCH, a reproducible benchmark, and INITSCOPE, a lightweight analysis framework linking what code is loaded with what is executed. On SCABENCH, INITSCOPE improved localization accuracy by up to 40% and reduced diagnostic effort by 64% compared with prior tools, while a developer study showed higher task accuracy and faster diagnosis. Together, these results advance evidence-driven, performance-aware practices for cold-start mitigation in serverless design. Availability: The research artifact is publicly accessible for future studies and improvements.

</details>


### [7] [Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference](https://arxiv.org/abs/2512.16134)
*Jian Tian,Shuailong Li,Yang Cao,Wenbo Cui,Minghan Zhu,Wenkang Wu,Jianming Zhang,Yanpeng Wang,Zhiwen Xiao,Zhenyu Hou,Dou Shen*

Main category: cs.DC

TL;DR: 相关性：本文与编译器、DSL、MLIR或HLS无关。它与图处理不直接相关，但与**LLM服务和调度（属于广义上的编译/系统优化范畴）**相关。/
太长不看：为了优化大规模DP+EP分布式LLM服务中固有的高同步成本和调度效率问题，本文提出了一种名为错峰批处理调度（SBS）的新机制。SBS通过时间解耦地缓冲请求形成最优批次，并结合负载感知全局分配策略来消除内部排队气泡，最终在生产环境中将首令牌延迟（TTFT）降低了30%-40%，并提高了15%-20%的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）服务的演进，复杂的分布式架构（如P/D分离、大规模DP+EP范式）带来了与传统部署不同的调度挑战。传统调度程序将实例视为黑盒，而DP+EP架构具有高昂的内部同步成本。作者指出，在这种系统中，立即请求分发（immediate request dispatching）会导致严重的引擎内部排队和并行化气泡（parallelization bubbles），从而降低首令牌生成时间（Time-to-First-Token, TTFT）。因此，需要一种新的调度机制来解决这些问题。

Method: 本文提出了错峰批处理调度（Staggered Batch Scheduling, SBS）机制，作为解决DP+EP架构中调度挑战的核心方法。SBS通过有意缓冲请求来形成最优执行批次，实现了请求的“时间解耦”，从而消除了系统内部的排队气泡（queuing bubbles）。此外，利用缓冲带来的调度窗口，论文引入了负载感知全局分配（Load-Aware Global Allocation）策略，用于平衡预填充（Prefill）和解码（Decode）阶段的计算负载，特别是跨DP单元的负载。

Result: 所提出的系统部署在一个服务Deepseek-V3的生产H800集群上，与最先进的即时调度基线相比，首令牌生成时间（TTFT）减少了30%至40%，吞吐量提高了15%至20%。

Conclusion: 本文提出的错峰批处理调度（SBS）机制，通过引入时间解耦和负载感知全局分配策略，有效解决了大规模分布式LLM服务中DP+EP架构固有的高内部同步成本和调度挑战。实验结果显示，该方法在不牺牲吞吐量的前提下，大幅优化了TTFT，并在生产环境中取得了显著的性能提升。

Abstract: The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.

</details>


### [8] [AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research](https://arxiv.org/abs/2512.16455)
*Ignacio Heredia,Álvaro López García,Germán Moltó,Amanda Calatrava,Valentin Kozlov,Alessandro Costantini,Viet Tran,Mario David,Daniel San Martín,Marcin Płóciennik,Marta Obregón Ruiz,Saúl Fernandez,Judith Sáinz-Pardo Díaz,Miguel Caballer,Caterina Alarcón Marín,Stefan Dlugolinsky,Martin Šeleng,Lisana Berberi,Khadijeh Alibabaei,Borja Esteban Sanchis,Pedro Castro,Giacinto Donvito,Diego Aguirre,Sergio Langarita,Vicente Rodriguez,Leonhard Duda,Andrés Heredia Canales,Susana Rebolledo Ruiz,João Machado,Giang Nguyen,Fernando Aguilar Gómez,Jaime Díez*

Main category: cs.DC

TL;DR: 该论文与编译器、HLS、DSL、MLIR 无关，但与**图处理（Graph Processing）**无直接关系，与**机器学习（ML/AI）**强相关（涉及AI模型、ML生命周期、联邦学习等）和**编译器/基础设施（Compiler/Infrastructure）**相关（涉及构建一个联邦计算平台，抽象底层基础设施）。/
本文描述了一个联邦计算平台，专门用于支持科学工作负载中的AI应用，通过整合跨越地理分散的e-Infrastructures资源，在一个统一的服务目录中提供从模型开发、训练（包括联邦学习）到多样化部署的完整机器学习生命周期支持，同时强调工具的可追溯性和模型的可重复性，以降低科研社区采用AI的门槛。


<details>
  <summary>Details</summary>
Motivation: 科学工作负载中的人工智能应用需要一个高效、可重复且易于访问的计算平台。由于底层基础设施（e-Infrastructures）通常是分散的，这为AI模型的开发、训练和部署带来了复杂性。因此，该平台的动机是建立一个专用的联邦计算平台，用于支持科学AI，提供一致、透明的部署和全生命周期的机器学习支持，同时确保操作的可追溯性和模型的可重复性。

Method: 本文描述的是一个联邦计算平台，其方法论重点在于集成和统一分布式资源，以支持科学工作负载中的人工智能应用。具体方法包括：1. **联邦化部署**：将分散的e-Infrastructures（电子基础设施）整合起来，提供统一、透明的访问。2. **服务目录**：提供涵盖完整机器学习生命周期的一站式服务，包括模型开发、训练（提供GPU、标注工具、实验跟踪、联邦学习）、和部署（覆盖云连续体的多种选项）。3. **工具集成**：提供可追溯性和可重复性工具，并集成不同的AI模型提供者、数据集和存储资源，以连接更广泛的机器学习生态系统。

Result: 所描述的平台成功地提供了一个联邦计算解决方案，专门用于科学AI工作负载。其成果包括：1. **统一访问**：实现了对分散的e-Infrastructures的一致、透明访问。2. **全周期支持**：提供了从模型开发到训练再到部署的完整ML服务目录。3. **增强功能**：集成了GPU资源、联邦学习支持、实验跟踪、以及模型可追溯性和可重复性工具。4. **生态系统连接**：能够整合不同的AI模型提供者、数据集和存储资源。5. **高可定制性**：降低了外部社区的采用门槛。

Conclusion: 本文描述的AI平台通过提供统一的接入点、全面的服务目录、支持全机器学习生命周期以及强调可追溯性和可重复性，解决了科学工作负载中AI应用的复杂性和碎片化问题。它的联邦式架构和可定制性也确保了平台能够适应多种科研社区的需求并易于集成。

Abstract: In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.

</details>


### [9] [Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems](https://arxiv.org/abs/2512.16473)
*En-Ming Huang,Li-Shang Lin,Chun-Yi Lee*

Main category: cs.DC

TL;DR: 该论文涉及 **compiler** (inference framework / hardware utilization) 和 **graph processing** (MoE structure can be viewed as a sparse graph-like structure where data flows through selected experts)。
**太长不读（TLDR）:** 针对消费级硬件上 MoE 模型内存需求高和传统卸载延迟大的问题，本文提出了一种新颖的 CPU-GPU 协作推理框架，通过在 GPU 上建立专家缓存以减少数据传输，并利用 CPU 多线程处理缓存未命中，从而提高了 MoE 模型在单请求推理场景中的性能和硬件利用率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的计算需求高，尤其是在消费级硬件上部署面临挑战。尽管专家混合（MoE）模型通过选择性激活参数子集提高了效率，但最先进的 MoE 模型仍需要超出典型消费级 GPU 容量的大量内存。传统的 CPU-GPU 卸载方法由于引入了延迟而限制了推理性能。因此，该研究的动机是开发一种高效的 CPU-GPU 协作推理框架，以克服内存限制和传输延迟，使得大型 MoE 模型能够在消费级硬件上进行高效的单请求推理。

Method: 该论文提出了一种新颖的 CPU-GPU 协作推理框架。核心方法是在 GPU 上设计了一个专家缓存机制，用于存储常用的专家模型参数以减少数据传输。对于缓存未命中的情况，计算任务将卸载到 CPU 上，并利用 CPU 的多线程优化进行高效处理。这种设计旨在通过减少 CPU 和 GPU 之间的数据传输量，克服传统卸载方法带来的延迟限制，从而提高 MoE 模型在消费级硬件上的推理性能。

Result: 该框架的评估结果显示了性能提升。该研究强调了 CPU-GPU 协作的潜力，能够在消费级系统上最大化硬件利用率，特别是在单请求推理场景中，以解决大型 MoE 模型在内存受限环境中的高效部署问题。

Conclusion: 该论文提出了一种新颖的 CPU-GPU 协作推理框架，通过在 GPU 上引入专家缓存机制，并结合 CPU 的多线程优势来处理缓存未命中，有效降低了 MoE 模型在消费级硬件上进行单请求推理时的内存和带宽限制。实验结果表明该框架在性能上有所提升，并强调了 CPU-GPU 协作在最大化硬件利用率方面的潜力，从而促进了大型 MoE 模型在资源受限环境中的部署。

Abstract: Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [Workload Characterization for Branch Predictability](https://arxiv.org/abs/2512.15827)
*FNU Vikas,Paul Gratz,Daniel Jiménez*

Main category: cs.AR

TL;DR: This content has not passed the compliance test and has been hidden.


<details>
  <summary>Details</summary>
Motivation: 分支预测是提高指令级并行性（ILP）的关键技术。准确的分支预测能减少错误路径上的指令执行，从而提高性能并降低能耗。然而，现有的分支预测器（如 TAGE 和感知机）的性能对工作负载的特性非常敏感。本文的动机是希望通过提出一种新的工作负载特征分析方法和相关指标，来深入理解和量化不同工作负载对现代分支预测器准确性的影响，进而为分支预测器的设计和改进提供指导。

Method: 本文提出了一种分支预测的工作负载特征分析方法，并引入了两个新的工作负载驱动的分支预测准确性指标：**分支工作集大小**（branch working set size）和**分支可预测性**（branch predictability）。通过将每条轨迹的这两种特性与现代分支预测方案（如 TAGE 和感知机）的错误预测率进行比较，对 2,451 条工作负载轨迹进行了分类和分析。分支工作集被定义为最常出现的分支上下文（包括分支地址、全局和局部历史）的集合。

Result: 本文提出了分支工作集大小和分支可预测性这两个新指标，并证明它们与现代分支预测器（如 TAGE 和感知机）的错误预测率高度相关。通过对 2,451 条工作负载轨迹的分析，将其分类为七种分支工作集大小和九种可预测性类别。研究结果揭示了预测准确性的来源，并确定了现代分支预测器偏爱的工作负载类别。

Conclusion: 本文介绍了评估分支预测准确性的新方法和指标，并对现代分支预测器进行了工作负载特征分析。通过分析分支工作集的大小和可预测性与预测器准确率之间的关系，这项工作为改进未来分支预测器的设计提供了指导。

Abstract: Conditional branch prediction predicts the likely direction of a conditional branch instruction to support ILP extraction. Branch prediction is a pattern recognition problem that learns mappings between a context to the branch outcome. An accurate predictor reduces the number of instructions executed on the wrong path resulting in an improvement of performance and energy consumption. In this paper, we present a workload characterization methodology for branch prediction. We propose two new workload-driven branch prediction accuracy identifiers -- branch working set size and branch predictability. These parameters are highly correlated with misprediction rates of modern branch prediction schemes (e.g. TAGE and perceptron). We define the branch working set of a trace as a group of most frequently occurring branch contexts, i.e. the 3-part tuple of branch address, and associated global and local history. We analyze the branch working set's size and predictability on a per-trace basis to study its relationship with a modern branch predictor's accuracy. We have characterized 2,451 workload traces into seven branch working set size and nine predictability categories after analyzing their branch behavior. We present further insights into the source of prediction accuracy and favored workload categories for modern branch predictors.

</details>


### [11] [Full System Architecture Modeling for Wearable Egocentric Contextual AI](https://arxiv.org/abs/2512.16045)
*Vincent T. Lee,Tanfer Alan,Sung Kim,Ecenur Ustun,Amr Suleiman,Ajit Krisshna,Tim Balbekov,Armin Alaghi,Richard Newcombe*

Main category: cs.AR

TL;DR: This paper is related to **Compiler** and **HLS** by discussing system-level design and optimization for a complex wearable device, which often involves specialized hardware and compilation flows for efficiency. The *system modeling* and *design space exploration* are closely related to how compiler/HLS tools map high-level functions to power/performance-efficient implementations (though the paper does not explicitly detail the compiler structure, the full-system power analysis directly dictates the needs for aggressive optimization by specialized tools). The paper is also broadly related to **edge device/system design** which often leverages efficient **MLIR**-like intermediate representations or specific graph processing techniques for ML tasks. / The paper analyzes the complete system architecture of a wearable contextual AI system (Aria2) and the lessons learned from its design space exploration. Key findings show that no single component dominates system power, necessitating end-to-end full system modeling and long-range design decisions to achieve power efficiency for future all-day wearable AI devices, emphasizing that optimizations must consider the overall system context (Amdahl's law for power).


<details>
  <summary>Details</summary>
Motivation: 构建下一代面向人类的计算需要全天候、具备空间感知能力的可穿戴设备来捕捉自我中心视觉和功能性基本信息，并以此构建和维护用户的“个人语境”。将这种个人语境与先进的生成式AI结合，可以解锁强大的新一代情境化AI个人助理和应用。然而，由于系统的复杂性和严格的功耗限制（受限于电池和重量），设计支持这种语境化AI的可穿戴系统是一个巨大的挑战，因此需要研究如何指导此类系统的设计。

Method: 这项研究方法上是提供一个具体的穿戴式语境化人工智能系统（Aria2）的完整系统架构视图，并通过系统建模和设计空间探索的过程来提取经验教训，从而指导未来此类系统的设计。它着重于端到端的全系统视角分析功耗分布和设计局限性。

Result: 研究提供了一个完整的穿戴式语境化AI系统（Aria2）的系统架构，并得出了关键结果：端到端的全系统模型视图对于此类系统至关重要，因为没有单一组件或类别绝对主导系统功耗。这意味着长期的设计决策和功耗优化必须在完整的系统语境下进行，以避免受限于其他系统瓶颈（如功耗领域的阿姆达尔定律或瓶颈转移）。

Conclusion: 这篇论文提供了一个用于构建全天候、可穿戴、语境化人工智能系统（如 Aria2）的系统架构视图和设计经验。核心结论是，对于此类复杂且功耗受限的系统，必须采用端到端的全系统建模和设计方法，因为系统功耗并非由单一组件主导，优化必须在整体系统语境下进行。未来的工作需要持续关注跨组件的协同设计和功耗优化，以最终实现全天候可用的设备。

Abstract: The next generation of human-oriented computing will require always-on, spatially-aware wearable devices to capture egocentric vision and functional primitives (e.g., Where am I? What am I looking at?, etc.). These devices will sense an egocentric view of the world around us to observe all human- relevant signals across space and time to construct and maintain a user's personal context. This personal context, combined with advanced generative AI, will unlock a powerful new generation of contextual AI personal assistants and applications. However, designing a wearable system to support contextual AI is a daunting task because of the system's complexity and stringent power constraints due to weight and battery restrictions. To understand how to guide design for such systems, this work provides the first complete system architecture view of one such wearable contextual AI system (Aria2), along with the lessons we have learned through the system modeling and design space exploration process. We show that an end-to-end full system model view of such systems is vitally important, as no single component or category overwhelmingly dominates system power. This means long-range design decisions and power optimizations need to be made in the full system context to avoid running into limits caused by other system bottlenecks (i.e., Amdahl's law as applied to power) or as bottlenecks change. Finally, we reflect on lessons and insights for the road ahead, which will be important toward eventually enabling all-day, wearable, contextual AI systems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [12] [Improved Lower Bounds for Privacy under Continual Release](https://arxiv.org/abs/2512.15981)
*Bardiya Aryanfard,Monika Henzinger,David Saulpic,A. R. Sricharan*

Main category: cs.DS

TL;DR: 涉及领域：本文涉及差分隐私（Differential Privacy）、图处理（Graph Processing）、数据流算法（Streaming Algorithms）和算法下界证明（Lower Bounds）。总结：本文研究了差分隐私下持续发布演变数据集统计信息的机制的误差下界。在事件级设置中，作者证明了针对仅插入图问题（如最大匹配、$k$-核等）的加性误差存在第一个多项式下界，相较于先前结果是指数级改进。研究还表明，允许小的乘性近似能显著降低误差。在项目级设置中，作者证明了大量图问题的持续机制的乘性误差和加性误差的乘积存在多项式下界。这些结果为理解持续设置中保证差分隐私所需的内在误差提供了新的见解。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究在差分隐私（DP）约束下持续发布不断演变数据集统计信息的机制的误差。特别是在图数据和流数据场景中，持续发布机制通常需要权衡隐私、实用性和效率。作者发现当前关于仅插入（insertions-only）图问题的持续发布机制的误差界限存在不足（例如Fichtenberger等人的多对数下界），并期望通过证明更强的多项式下界来更好地理解在这种具有局部隐私限制的流数据设置中，尤其是图问题上，保证差分隐私所需的内在误差。作者还希望探索乘性近似对于降低误差界限的作用。

Method: 本文主要通过证明下界来分析差分隐私持续发布机制的误差问题。在事件级设置中，作者构建了用于最大匹配、度直方图和$k$-核的实例，以证明加性误差的多项式下界。通过与$1$-Way-Marginals的规约，证明了同时范数估计的不可避免的多项式加性误差。随后，作者为同时范数估计提供了一个新的持续机制，允许小乘性近似。在项目级设置中，作者提出了一个新的关于1-Way-Marginals的乘性误差和加性误差乘积的下界，然后通过规约将其应用于大量的持续图问题。

Result: 在事件级设置中：1. 证明了仅插入图问题（最大匹配、度直方图和$k$-核）的加性误差存在第一个多项式下界，这相较于现有结果（如最大匹配和度直方图的多对数下界）是指数级的改进。2. 对$k$-核问题首次给出了持续发布机制的下界。3. 发现对于最大匹配和$k$-核，允许小的乘性近似可以将加性误差降低到多对数级别。4. 证明了仅插入设置下同时范数估计的不可避免的多项式加性误差。5. 提出了第一个具有多对数加性误差和$(1+\zeta)$乘性近似的持续机制，用于同时估计所有单调对称范数。在项目级设置中：1. 证明了大量图问题的持续机制的乘性误差和加性误差的乘积存在多项式下界。2. 提出了一个新的关于1-Way-Marginals的乘性误差和加性误差乘积的下界，概括了Hardt和Talwar、Bun等人的工作。这些是首个针对任何具有乘性误差的差分隐私持续发布机制的下界。

Conclusion: 本文研究了事件级和项目级差分隐私持续发布机制的下界。在事件级设置中，作者首次证明了针对仅插入图问题（如最大匹配、度直方图和$k$-核）的加性误差存在多项式下界，这比现有结果有了显著的改进。作者还发现在允许小乘性近似的情况下，可以将最大匹配和$k$-核的加性误差降低到多对数级别。对于同时范数估计，作者给出了第一个具有多对数加性误差的持续机制，并带有$(1+\zeta)$乘性近似。在项目级设置中，作者证明了大量图问题的持续机制的乘性误差和加性误差的乘积存在多项式下界，并为此提出了一个新的关于1-Way-Marginals的下界。这些结果为理解在持续发布设置下保证差分隐私所需的误差提供了重要的新见解。

Abstract: We study the problem of continually releasing statistics of an evolving dataset under differential privacy. In the event-level setting, we show the first polynomial lower bounds on the additive error for insertions-only graph problems such as maximum matching, degree histogram and $k$-core. This is an exponential improvement on the polylogarithmic lower bounds of Fichtenberger et al.[ESA 2021] for the former two problems, and are the first continual release lower bounds for the latter. Our results run counter to the intuition that the difference between insertions-only vs fully dynamic updates causes the gap between polylogarithmic and polynomial additive error. We show that for maximum matching and $k$-core, allowing small multiplicative approximations is what brings the additive error down to polylogarithmic.
  Beyond graph problems, our techniques also show that polynomial additive error is unavoidable for Simultaneous Norm Estimation in the insertions-only setting. When multiplicative approximations are allowed, we circumvent this lower bound by giving the first continual mechanism with polylogarithmic additive error under $(1+ζ)$ multiplicative approximations, for $ζ>0$, for estimating all monotone symmetric norms simultaneously.
  In the item-level setting, we show polynomial lower bounds on the product of the multiplicative and the additive error of continual mechanisms for a large range of graph problems. To the best of our knowledge, these are the first lower bounds for any differentially private continual release mechanism with multiplicative error. To obtain this, we prove a new lower bound on the product of multiplicative and additive error for 1-Way-Marginals, from which we reduce to continual graph problems. This generalizes the lower bounds of Hardt and Talwar[STOC 2010] and Bun et al.[STOC 2014] on the additive error for mechanisms with no multiplicative error.

</details>


### [13] [Instance Optimality in PageRank Centrality Estimation](https://arxiv.org/abs/2512.16087)
*Mikkel Thorup,Hanzhi Wang*

Main category: cs.DS

TL;DR: 该论文与图处理（PageRank 估计算法在有向图上的应用）相关。所研究的自适应 PageRank 估计算法的实例最优性取决于图的度数分布，对于稀疏图和度数受限图，该算法是实例最优的（至多相差一个多项式对数因子），但对于度数大多等于 $n$ 的图则不然。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是研究一个简单的、经典的估计顶点 PageRank 中心性的算法的自适应变体，并确定其在不同图结构下的实例最优性，从而理解该算法的性能边界。

Method: 本文研究了一个经典 PageRank 估计算法的自适应变体在不同图结构下的实例最优性。方法是通过理论分析和反例来证明其在特定图类上的实例最优性，以及在其他图类上实例最优性的失效。

Result: 本文证明了所研究的 PageRank 估计算法的自适应变体，对于最大入度或出度至多为 $n$ 的常数部分的任何有向图，以及包含至多多项式对数个无界度顶点的图（包括所有 $\widetilde{O}(n)$ 边的稀疏图），在达到恒定相对误差和恒定概率的条件下，其时间复杂度在多项式对数因子内是实例最优的。同时，本文提供了一个反例，表明该算法对于度数大多等于 $n$ 的图不是实例最优的。

Conclusion: 本文研究了图的度数对算法实例最优性的影响。对于最大入度或出度最多为 $n$ 的常数部分、以及包含少许（对数级别）无界度节点的图（覆盖了所有 $\widetilde{O}(n)$ 边的稀疏图），本算法是实例最优的，最多相差一个多项式对数因子。但对于度数大多等于 $n$ 的图，该算法不再是实例最优的。

Abstract: We study an adaptive variant of a simple, classic algorithm for estimating a vertex's PageRank centrality within a constant relative error, with constant probability. We show that this algorithm is instance-optimal up to a polylogarithmic factor for any directed graph of order $n$ whose maximal in- and out-degrees are at most a constant fraction of $n$. The instance-optimality also extends to graphs in which up to a polylogarithmic number of vertices have unbounded degree, thereby covering all sparse graphs with $\widetilde{O}(n)$ edges. Finally, we provide a counterexample showing that the algorithm is not instance-optimal for graphs with degrees mostly equal to $n$.

</details>


### [14] [Conquering the Multiverse: The River Voting Method with Efficient Parallel Universe Tiebreaking](https://arxiv.org/abs/2512.16414)
*Jannes Malanowski*

Main category: cs.DS

TL;DR: 该论文与图处理（Graph Processing）和算法优化相关。图处理主要体现在 Ranked Pairs 和 River 投票方法都使用了加权优势图（weighted margin graph）来建模选举，并且计算 River 赢者的过程涉及对图上的边进行特定排序和图算法（如类 Prim 算法）的应用。
TLDR: Ranked Pairs 投票法在保证中立性（通过 Parallel Universe Tiebreaking, PUT）时计算赢者是 NP-难的。本文证明了新兴的 River 投票法与 PUT 结合可以在多项式时间内高效计算。通过引入半 River 图和类似 Prim 算法的变体实现这一目标，并将 River 算法的运行时间从 $\mathcal{O}(n^4)$ 优化到 $\mathcal{O}(n^2 \log n)$。


<details>
  <summary>Details</summary>
Motivation: 选举投票需要有一个公平的赢者决定标准。中立性（Neutrality）是公平选举的关键标准之一，要求所有选项得到平等对待。排名配对（Ranked Pairs, RP）是一种已建立的投票方法，但在处理平局时，如果使用保持中立性的平局打破方案（如并行宇宙平局打破，PUT），则计算赢者是 NP-难的，即计算上不可行。River 是最近引入的、与 RP 具有相似优点的投票方法，并且具有抵抗操纵等额外优势。因此，本文的动机是研究 River 投票方法与 PUT 结合时的计算可行性，旨在找到一种既能保持中立性又能高效计算赢者的方法。同时，也希望提升 River 算法本身的计算效率。

Method: 本文的贡献在于提出了一种新的计算 River+PUT 获胜者的方法，并证明了其多项式时间复杂度。具体方法是：1. 引入半 River 图（semi-River diagram），该图包含任意平局打破方案下可能出现在 River 图中的边。2. 基于半 River 图，利用一个经过特殊构造的边排序方案运行 River 算法来检查某个选项是否是赢者。3. 在半 River 图上，通过对每个选项应用 Prim 算法的一个变体来计算 River 获胜者。4. 此外，本文还提供了一种将 River 算法的运行时间从 $\mathcal{O}(n^4)$ 优化到 $\mathcal{O}(n^2 \log n)$ 的改进算法。

Result: 主要结果是证明了 River 投票方法与并行宇宙平局打破（PUT）结合可以在多项式最坏情况运行时间内（polynomial worst-case runtime）计算获胜者。具体而言，可以通过构造特殊的边排序来运行 River 算法，从而检查某个选项是否为 River PUT 赢者。这一过程通过引入半 River 图（semi-River diagram）实现，该图包含了任意平局打破下可能出现在 River 图中的边。在半 River 图上，通过对每个选项应用 Prim 算法的一个变体来计算 River 赢者。此外，本文还将 River 算法的朴素运行时间从 $\mathcal{O}(n^4)$ 改进到了 $\mathcal{O}(n^2 \log n)$，其中 $n$ 是选项的数量。

Conclusion: 本文分析了排名配对（Ranked Pairs, RP）和 River 两种投票方法在处理平局时对中立性（Neutrality）和计算可行性（Tractability）的影响。RP 在平局下无法同时保证中立性和可计算性。而 River 投票方法，特别是与并行宇宙平局打破（Parallel Universe Tiebreaking, PUT）结合时，可以在多项式时间内计算出赢者，解决了 RP+PUT 的 NP-难问题。通过引入半 River 图（semi-River diagram）并应用 Prim 算法的变体，实现了高效的计算。此外，本文还将 River 算法的运行时间从 $\mathcal{O}(n^4)$ 优化到 $\mathcal{O}(n^2 \log n)$，提升了其在实际应用中的效率。

Abstract: Democracy relies on making collective decisions through voting. In addition, voting procedures have further applications, for example in the training of artificial intelligence. An essential criterion for determining the winner of a fair election is that all alternatives are treated equally: this is called neutrality. The established Ranked Pairs voting method cannot simultaneously guarantee neutrality and be computationally tractable for election with ties. River, the recently introduced voting method, shares desirable properties with Ranked Pairs and has further advantages, such as a new property related to resistance against manipulation. Both Ranked Pairs and River use a weighted margin graph to model the election. Ties in the election can lead to edges of equal margin. To order the edges in such a case, a tiebreaking scheme must be employed. Many tiebreaks violate neutrality or other important properties. A tiebreaking scheme that preserves neutrality is Parallel Universe Tiebreaking (PUT). Ranked Pairs with PUT is NP-hard to compute.
  The main result of this thesis shows that River with PUT can be computed in polynomial worst-case runtime: We can check whether an alternative is a River PUT winner, by running River with a specially constructed ordering of the edges. To construct this ordering, we introduce the semi-River diagram which contains the edges that can appear in any River diagram for some arbitrary tiebreak. On this diagram we can compute the River winners, by applying a variant of Prims algorithm per alternative. Additionally, we give an algorithm improve the previous naive runtime of River from $\mathcal{O}(n^4)$ to $\mathcal{O}(n^2 \log n)$, where n is the number of alternatives.

</details>


### [15] [Fully Dynamic Algorithms for Chamfer Distance](https://arxiv.org/abs/2512.16639)
*Gramoz Goranci,Shaofeng Jiang,Peter Kiss,Eva Szilagyi,Qiaoyuan Yang*

Main category: cs.DS

TL;DR: 该论文与 **机器学习（Machine Learning）** 相关，因为它明确提到 Chamfer 距离作为 **损失函数** 在机器学习中应用，并且研究动态点云数据。

**太长不读（tldr）总结：**
Chamfer 距离是点云相似性的重要度量，在机器学习等领域需要对动态变化的数据集进行重复计算。本文提出了第一个在点集动态演化（插入/删除）时，高效维护 Chamfer 距离近似值的动态算法。该算法将 Chamfer 距离的维护问题转化为近似最近邻（ANN）搜索，理论上在 $\ell_p$ 范数下（$p \in \{1, 2\}$）实现了与现有 ANN 算法相当的时间复杂度界限，并被证明在实际数据集中表现良好。


<details>
  <summary>Details</summary>
Motivation: Chamfer 距离作为点云 dissimilarity 的度量标准被广泛使用，特别是在需要对动态变化数据集进行重复评估的实际应用中（例如，作为机器学习中的损失函数）。然而，现有方法在点集动态演化（通过点插入或删除）时，如何高效地维护该距离的近似值是一个未解决的问题。因此，本文的动机是为全动态设置下的 Chamfer 距离计算问题提供第一个动态算法。

Method: 本文提出了一种将 Chamfer 距离的动态维护问题转化为近似最近邻（ANN）搜索的动态算法。通过利用现有的 ANN 搜索界限，该算法可以实现对 Chamfer 距离的 $(1+\epsilon)$-近似或 $O(1/\epsilon)$-近似。

Result: 本文提出的算法取得了以下近似和时间复杂度（其中 $\tilde{O}$ 隐藏了对数因子）：
1. $(1+\epsilon)$-近似：$\tilde{O}(\epsilon^{-d})$ 更新时间。
2. $O(1/\epsilon)$-近似：$\tilde{O}(d n^{\epsilon^2} \epsilon^{-4})$ 更新时间。
实验证明，该方法在实际数据集上的性能与自然基线相比具有竞争力。

Conclusion: 本文首次提出了用于维护动态点集间 Chamfer 距离近似值的动态算法。该算法将问题转化为近似最近邻（ANN）搜索，并在理论上取得了与现有最先进的 ANN 算法相当的界限。在实际应用中，该方法也表现出竞争力，为处理动态点云数据中的相似性度量提供了一种高效的解决方案。

Abstract: We study the problem of computing Chamfer distance in the fully dynamic setting, where two set of points $A, B \subset \mathbb{R}^{d}$, each of size up to $n$, dynamically evolve through point insertions or deletions and the goal is to efficiently maintain an approximation to $\mathrm{dist}_{\mathrm{CH}}(A,B) = \sum_{a \in A} \min_{b \in B} \textrm{dist}(a,b)$, where $\textrm{dist}$ is a distance measure. Chamfer distance is a widely used dissimilarity metric for point clouds, with many practical applications that require repeated evaluation on dynamically changing datasets, e.g., when used as a loss function in machine learning. In this paper, we present the first dynamic algorithm for maintaining an approximation of the Chamfer distance under the $\ell_p$ norm for $p \in \{1,2 \}$. Our algorithm reduces to approximate nearest neighbor (ANN) search with little overhead. Plugging in standard ANN bounds, we obtain $(1+ε)$-approximation in $\tilde{O}(ε^{-d})$ update time and $O(1/ε)$-approximation in $\tilde{O}(d n^{ε^2} ε^{-4})$ update time. We evaluate our method on real-world datasets and demonstrate that it performs competitively against natural baselines.

</details>


### [16] [Learning Confidence Ellipsoids and Applications to Robust Subspace Recovery](https://arxiv.org/abs/2512.16875)
*Chao Gao,Liren Shan,Vaidehi Srinivas,Aravindan Vijayaraghavan*

Main category: cs.DS

TL;DR: 本文与**图处理/编译器/HLS/MLIR/DSL**不相关。
本文研究了在高维空间中寻找最小体积置信椭球的问题。由于高维情况下精确求解或获得非平凡近似是NP-hard的，作者提出了一种多项式时间算法。该算法利用最小体积包围椭球的原对偶结构和几何 Brascamp-Lieb 不等式，实现了在体积上有 $O(\beta^{\gamma d})$ 近似保证、概率覆盖率达到 $1-O(\alpha/\gamma)$ 的目标。此外，文章证明了这种近似依赖在新提出的硬度结果中是在指数范围内常数因子下必要的，并首次为鲁棒子空间恢复问题提供了具有近似保证的多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 在高维空间中，寻找最小体积置信椭球以估计分布的中心位置和分散度是一个经典且重要的问题。然而，当椭球的条件数 $\beta$ 趋于无穷大时，即使是非平凡的近似因子在体积上也是 NP-hard 的。椭球是一种高度表达性的置信集，因为它能捕捉分布中的相关性，并能近似任何凸集。因此，本文的动机是探索在体积近似保证的情况下，如何高效地找到具有有界条件数 $\beta$ 的置信椭球。这个问题在高维鲁棒估计和鲁棒子空间恢复中尤其关键。

Method: 本文提出了一种多项式时间算法，用于在高维空间中寻找置信椭球。该算法的核心是利用最小体积包围椭球（MVEE）的丰富原对偶结构和几何 Brascamp-Lieb 不等式。通过这种方法，算法可以找到一个椭球 $E$，其体积与具有有界条件数 $\beta$ 的最佳椭球相比，具有 $O(\beta^{\gamma d})$ 的乘性近似因子，并且覆盖至少 $1-O(\alpha/\gamma)$ 的概率质量。此外，研究还通过计算硬度结果来补充说明，证明了这种指数级的依赖关系可能是必要的。

Result: 本文提出的主要结果是一个多项式时间算法，它能找到一个椭球 $E$，该椭球的体积与最佳 $\beta$ 条件椭球的体积相比，在 $O(\beta^{\gamma d})$ 的乘性因子内，同时覆盖至少 $1-O(\alpha/\gamma)$ 的概率质量，其中 $\gamma < \alpha$。研究还通过一个计算硬度结果来佐证，$O(\beta^{\gamma d})$这种指数依赖是必要的（达到指数中的常数）。此外，这一算法还使得鲁棒子空间恢复问题在最坏情况实例上获得了第一个具有近似保证的多项式时间算法。

Conclusion: 本文研究了高维空间中寻找置信椭球的问题。它提出了一个在多项式时间内运行的算法，该算法在体积上具有$O(\beta^{\gamma d})$的近似保证，并覆盖至少$1-O(\alpha/\gamma)$的概率质量。该算法结合了最小体积包围椭球的对偶结构和几何 Brascamp-Lieb 不等式。这项工作为高维鲁棒估计和子空间恢复问题提供了第一个具有近似保证的多项式时间算法，为处理高维数据中的相关性和近似性问题奠定了基础。

Abstract: We study the problem of finding confidence ellipsoids for an arbitrary distribution in high dimensions. Given samples from a distribution $D$ and a confidence parameter $α$, the goal is to find the smallest volume ellipsoid $E$ which has probability mass $\Pr_{D}[E] \ge 1-α$. Ellipsoids are a highly expressive class of confidence sets as they can capture correlations in the distribution, and can approximate any convex set. This problem has been studied in many different communities. In statistics, this is the classic minimum volume estimator introduced by Rousseeuw as a robust non-parametric estimator of location and scatter. However in high dimensions, it becomes NP-hard to obtain any non-trivial approximation factor in volume when the condition number $β$ of the ellipsoid (ratio of the largest to the smallest axis length) goes to $\infty$. This motivates the focus of our paper: can we efficiently find confidence ellipsoids with volume approximation guarantees when compared to ellipsoids of bounded condition number $β$?
  Our main result is a polynomial time algorithm that finds an ellipsoid $E$ whose volume is within a $O(β^{γd})$ multiplicative factor of the volume of best $β$-conditioned ellipsoid while covering at least $1-O(α/γ)$ probability mass for any $γ< α$. We complement this with a computational hardness result that shows that such a dependence seems necessary up to constants in the exponent. The algorithm and analysis uses the rich primal-dual structure of the minimum volume enclosing ellipsoid and the geometric Brascamp-Lieb inequality. As a consequence, we obtain the first polynomial time algorithm with approximation guarantees on worst-case instances of the robust subspace recovery problem.

</details>
