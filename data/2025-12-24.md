<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.DS](#cs.DS) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [An Adaptive Distributed Stencil Abstraction for GPUs](https://arxiv.org/abs/2512.19851)
*Aditya Bhosale,Laxmikant Kale*

Main category: cs.DC

TL;DR: 该论文与编译器、DSL 和图处理相关。因为它提到了一个高性能 stencil DSL（特定领域语言）以及如何构建一个高性能计算的抽象来替代它，这涉及到软件抽象和性能优化方面，可能与编译和运行时系统有关。摘要：目前的 Python 科学计算工具链（如 NumPy）难以充分利用多节点超级计算机的性能，特别是对于硬件加速器和资源适应性的需求得不到满足。本文提出了一个使用 CharmTyles 和自适应 Charm++ 运行时构建的、针对多节点 GPU 上 stencil 计算的自适应、分布式抽象，该抽象具有 NumPy 风格的语法。实验证明，该抽象具有动态资源弹性，并且性能优于专门的 DSL 和通用的 NumPy 替代品。


<details>
  <summary>Details</summary>
Motivation: 当前的 Python 科学计算生态系统大多局限于单节点并行性，导致 NumPy 等高级原型设计与现代超级计算机上的高性能执行之间存在差距。同时，硬件加速器的普及和对能效的需求使得资源适应性成为一个关键要求，而传统的 HPC 抽象却过于僵化。因此，需要一个自适应的、分布式的抽象来解决这一问题。

Method: 本文通过利用基于自适应 Charm++ 运行时的 CharmTyles 框架，构建了一个用于多节点 GPU 上 stencil 计算的自适应、分布式抽象。该抽象采用了 NumPy 风格的语法以简化从原型到生产代码的迁移。通过动态重新调整正在运行的应用程序的节点数量，展示了其资源弹性，并对相关开销进行了性能分析。

Result: 本文提出的抽象实现了显著的性能改进，优于一个专门的高性能 stencil DSL 和一个通用的 NumPy 替代方案。此外，该抽象还展示了良好的资源弹性，能够动态地重新调整运行中的应用所使用的节点数量。性能分析部分量化了相关的开销。

Conclusion: 本文提出的自适应、分布式抽象为多节点 GPU 上的 stencil 计算提供了一个高性能且灵活的解决方案。通过利用 CharmTyles 和 Charm++ 运行时，结合 NumPy 风格的语法，该抽象成功地弥合了 Python 科学计算中的原型设计与超级计算机高性能执行之间的差距，并展示了在资源弹性和性能上的显著优势。

Abstract: The scientific computing ecosystem in Python is largely confined to single-node parallelism, creating a gap between high-level prototyping in NumPy and high-performance execution on modern supercomputers. The increasing prevalence of hardware accelerators and the need for energy efficiency have made resource adaptivity a critical requirement, yet traditional HPC abstractions remain rigid. To address these challenges, we present an adaptive, distributed abstraction for stencil computations on multi-node GPUs. This abstraction is built using CharmTyles, a framework based on the adaptive Charm++ runtime, and features a familiar NumPy-like syntax to minimize the porting effort from prototype to production code. We showcase the resource elasticity of our abstraction by dynamically rescaling a running application across a different number of nodes and present a performance analysis of the associated overheads. Furthermore, we demonstrate that our abstraction achieves significant performance improvements over both a specialized, high-performance stencil DSL and a generalized NumPy replacement.

</details>


### [2] [Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions](https://arxiv.org/abs/2512.19972)
*Pengchao Han,Xi Huang,Yi Fang,Guojun Han*

Main category: cs.DC

TL;DR: 该论文与MLIR、DSL、图处理、编译、HLS不直接相关，它属于机器学习/人工智能（AI）领域的研究，特别是关于**协作学习（Collaborative Learning）**和**知识蒸馏（Knowledge Distillation, KD）**。

总结：这篇综述文章全面回顾了协作学习中知识蒸馏（KD）的应用，并聚焦于记忆和知识在其中的关键作用。文章首先定义了KD中的记忆和知识，并解释了它们如何影响知识在分布式、层级式和去中心化协作模式中的提取、存储和共享。它特别强调了联邦学习（FL）等分布式学习模式中的任务异构性，以及模型、数据、资源和隐私问题。最终，文章对现有工作进行了分类，并讨论了挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有的协作学习范式，特别是其中的知识蒸馏（KD）技术，虽然有效，但对于“KD如何利用跨主体（agents）间的记忆和知识”这一底层机制缺乏深入的探索和理解。因此，本文旨在弥补这一差距，通过系统性地审视记忆和知识在协作学习中KD过程中的核心作用，从而为推进该领域的发展提供清晰的框架和洞察。

Method: 本文采用综述和分类分析的方法，全面回顾了协作学习中知识蒸馏（KD）的应用，并着重分析了记忆（Memory）和知识（Knowledge）在其中的作用。具体方法包括：1. **定义和分类**：明确定义KD过程中的记忆和知识，并进行分类；2. **探讨相互关系**：分析知识是如何在协作环境中被提取、存储和共享的；3. **模式分析**：考察分布式、层级式和去中心化等协作学习模式，特别关注分布式学习模式下的任务异构性（如FL、MADA、FML、FCL、FMTL、FKGE）；4. **异构性分析**：强调模型、数据、资源异构性及其隐私问题；5. **分类现有工作**：根据现有工作处理记忆和知识的方式对其进行分类；6. **挑战与展望**：讨论现有挑战并提出未来的研究方向。

Result: 本文提供了协作学习中知识蒸馏（KD）的全面综述，重点阐明了记忆和知识的作用。主要结果包括：对KD过程中记忆和知识的清晰定义和分类；揭示了知识在分布式、层级式和去中心化等协作学习模式中是如何被提取、存储和共享的；特别强调了分布式学习模式下的任务异构性（如FL、FML等）；分析和强调了模型、数据、资源异构性以及隐私问题；最后，根据现有工作处理记忆和知识的方式进行了分类，并提出了未来的研究方向。

Conclusion: 这篇综述论文通过对协作学习中知识蒸馏（KD）的记忆和知识作用的深入研究，为该领域未来的发展奠定了基础。它不仅清晰地定义和分类了关键概念，揭示了知识在协作环境中的提取、存储和共享机制，还提出了对现有挑战和未来研究方向的深刻见解。这对于推进大规模智能系统中高效、隐私保护的知识共享方法具有重要意义。

Abstract: Collaborative learning has emerged as a key paradigm in large-scale intelligent systems, enabling distributed agents to cooperatively train their models while addressing their privacy concerns. Central to this paradigm is knowledge distillation (KD), a technique that facilitates efficient knowledge transfer among agents. However, the underlying mechanisms by which KD leverages memory and knowledge across agents remain underexplored. This paper aims to bridge this gap by offering a comprehensive review of KD in collaborative learning, with a focus on the roles of memory and knowledge. We define and categorize memory and knowledge within the KD process and explore their interrelationships, providing a clear understanding of how knowledge is extracted, stored, and shared in collaborative settings. We examine various collaborative learning patterns, including distributed, hierarchical, and decentralized structures, and provide insights into how memory and knowledge dynamics shape the effectiveness of KD in collaborative learning. Particularly, we emphasize task heterogeneity in distributed learning pattern covering federated learning (FL), multi-agent domain adaptation (MADA), federated multi-modal learning (FML), federated continual learning (FCL), federated multi-task learning (FMTL), and federated graph knowledge embedding (FKGE). Additionally, we highlight model heterogeneity, data heterogeneity, resource heterogeneity, and privacy concerns of these tasks. Our analysis categorizes existing work based on how they handle memory and knowledge. Finally, we discuss existing challenges and propose future directions for advancing KD techniques in the context of collaborative learning.

</details>


### [3] [Scaling Point-based Differentiable Rendering for Large-scale Reconstruction](https://arxiv.org/abs/2512.20017)
*Hexu Zhao,Xiaoteng Liu,Xiwen Min,Jianhao Huang,Youming Deng,Yanfei Li,Ang Li,Jinyang Li,Aurojit Panda*

Main category: cs.DC

TL;DR: 部分与图处理（"Point-based"暗示了点云或点集，可以抽象为图结构）和编译器（系统设计和优化与编译器原理相关），以及高性能计算和分布式系统相关。Gaian是一个通用的分布式点云微分渲染（PBDR）训练系统，它通过统一API和优化数据局部性来大幅减少通信开销（高达91%），从而在多GPU和大规模场景下，将训练吞吐量提高1.50倍到3.71倍。


<details>
  <summary>Details</summary>
Motivation: 现有的点云微分渲染（PBDR）系统通常与特定的PBDR方法紧密耦合，并且由于数据局部性差，存在严重的通信开销，这阻碍了PBDR扩展到高分辨率和大型场景。因此，需要一个通用的、高效的分布式训练系统来解决这些问题。

Method: Gaian通过提供一个统一的API来支持现有的PBDR方法，同时公开丰富的数据访问信息。Gaian利用这些信息来优化数据局部性并大幅减少通信量。

Result: 通过实现和评估4种PBDR算法，Gaian在6个数据集和多达128个GPU上展现出高性能和资源效率。它成功地将通信量减少了至多91%，并将训练吞吐量提高了1.50倍至3.71倍。

Conclusion: Gaian是一个通用的分布式PBDR训练系统，它通过统一API支持多种现有PBDR方法，并通过利用丰富的数据访问信息来优化数据局部性和减少通信开销。这使得在扩展到高分辨率和大型场景时，PBDR训练的性能和资源效率得以显著提高。

Abstract: Point-based Differentiable Rendering (PBDR) enables high-fidelity 3D scene reconstruction, but scaling PBDR to high-resolution and large scenes requires efficient distributed training systems. Existing systems are tightly coupled to a specific PBDR method. And they suffer from severe communication overhead due to poor data locality. In this paper, we present Gaian, a general distributed training system for PBDR. Gaian provides a unified API expressive enough to support existing PBDR methods, while exposing rich data-access information, which Gaian leverages to optimize locality and reduce communication. We evaluated Gaian by implementing 4 PBDR algorithms. Our implementations achieve high performance and resource efficiency: across six datasets and up to 128 GPUs, it reduces communication by up to 91% and improves training throughput by 1.50x-3.71x.

</details>


### [4] [Population Protocols Revisited: Parity and Beyond](https://arxiv.org/abs/2512.20163)
*Leszek Gąsieniec,Tytus Grodzicki,Tomasz Jurdziński,Jakub Kowalski,Grzegorz Stachowiak*

Main category: cs.DC

TL;DR: 该论文与DSL、图处理、MLIR、编译器或HLS均无直接关联，它属于分布式计算和理论计算机科学领域，研究的是群体协议（Population Protocols）。
**太长不看版（TLDR）：** 针对群体协议中奇偶校验和同余计算（模运算）缺乏同时具备时间效率和空间效率的协议的空白，本文提出了一种整合群体权重、鲁棒时钟和异常检测的新计算范式。基于此范式，本文首次设计出了高效的奇偶校验和同余协议，它们的状态数和静默稳定时间均为 $O(\log^3 n)$，成功地弥补了这一理论空白。


<details>
  <summary>Details</summary>
Motivation: 现有的群体协议（Population Protocols）在诸如领导者选举和多数派计算等分布式计算中心的标准问题上已取得高效的解决方案，其中多数派计算与普雷斯堡算术中的谓词类型紧密相关。然而，对于在该算术框架中具有互补性的同余谓词（如奇偶校验），现有协议在实现时间效率和空间效率兼具方面存在显著空白。本文的动机正是为了弥补这一差距，通过探索奇偶校验问题并将其推广到模任意 $m$ 的同余计算，以期为这类互补问题提供高效的解决方案。

Method: 本文提出了一种新的群体计算范式，旨在设计高效、通用且鲁棒的多阶段稳定群体协议。该范式主要包含三个核心机制：1. **群体权重（Population Weights）**：一种鲁棒的度量机制；2. **鲁棒的时钟机制（Robust Clocking Mechanism）**；3. **高效的异常检测与切换机制（Efficient Anomaly Detection Coupled with a Switching Mechanism）**：该机制确保在出现异常时（通过切换到一种“慢但始终正确”的解决方案），协议仍然可以正确运行。基于此范式，作者成功设计了第一个同时具有时间效率和空间效率的奇偶校验和模 $m$ 同余协议。

Result: 本文首次提出了高效的奇偶校验（Parity）和同余（Congruence）群体协议。这些协议在两个关键性能指标上均实现了高效性：协议的**状态数**为 $O(\log^3 n)$，并且在**静默稳定时间**上达到了 $O(\log^3 n)$。这一成就展示了所提出的计算范式（集成群体权重、鲁棒时钟和异常检测）在通用高效群体协议设计上的有效性。此外，工作还讨论了权重系统在不同数据表示（一元和二元）之间隐式转换的能力及其对群体规模计算和表示等其他问题的潜在应用。

Conclusion: 本文通过引入新的计算范式，即结合了群体权重、时钟机制和异常检测，成功地弥补了现有群体协议在处理模运算（如奇偶校验）问题上的不足。该范式首次实现了时间效率和空间效率兼具的奇偶校验和同余协议，具体为 $O(\log^3 n)$ 的状态数和 $O(\log^3 n)$ 的静默稳定时间。这表明该方法是设计高效、通用、鲁棒的多阶段稳定群体协议的有效途径，同时也揭示了权重系统在不同数据表示（如一元和二元）之间转换的潜力，对群体计算中的其他问题（如群体规模的计算和表示）具有广泛的应用前景。

Abstract: For nearly two decades, population protocols have been extensively studied, yielding efficient solutions for central problems in distributed computing, including leader election, and majority computation, a predicate type in Presburger Arithmetic closely tied to population protocols. Surprisingly, no protocols have achieved both time- and space-efficiency for congruency predicates, such as parity computation, which are complementary in this arithmetic framework. This gap highlights a significant challenge in the field. To address this gap, we explore the parity problem, where agents are tasked with computing the parity of the given sub-population size. Then we extend the solution for parity to compute congruences modulo an arbitrary $m$.
  Previous research on efficient population protocols has focused on protocols that minimise both stabilisation time and state utilisation for specific problems. In contrast, this work slightly relaxes this expectation, permitting protocols to place less emphasis on full optimisation and more on universality, robustness, and probabilistic guarantees. This allows us to propose a novel computing paradigm that integrates population weights (or simply weights), a robust clocking mechanism, and efficient anomaly detection coupled with a switching mechanism (which ensures slow but always correct solutions). This paradigm facilitates universal design of efficient multistage stable population protocols. Specifically, the first efficient parity and congruence protocols introduced here use both $O(\log^3 n)$ states and achieve silent stabilisation in $O(\log^3 n)$ time. We conclude by discussing the impact of implicit conversion between unary and binary representations enabled by the weight system, with applications to other problems, including the computation and representation of (sub-)population sizes.

</details>


### [5] [Reaching Agreement Among Reasoning LLM Agents](https://arxiv.org/abs/2512.20184)
*Chaoyi Ruan,Yiliang Wang,Ziji Shi,Jialin Li*

Main category: cs.DC

TL;DR: 否，本论文内容与 DSL、图处理、MLIR、编译器或 HLS 无关。它主要涉及多智能体系统和共识协议。
总结：多智能体系统目前的编排方法存在效率和可靠性问题。本文提出了多智能体精化问题的形式化模型，并引入了 Aegean 这一基于共识的协议和实现引擎 Aegean-Serve，以保证多智能体推理的可靠性。实验结果显示，Aegean 在保持答案质量的同时，显著降低了推理延迟，并提供了可证明的安全性与活性保障。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体编排依赖于静态启发式工作流（如固定的循环限制和屏障同步），这些临时方法浪费计算资源，由于“拖油瓶”（stragglers）导致高延迟，并有固化暂时性协议的风险。作者认为可靠的多智能体推理需要一个类似于经典分布式共识问题的形式化基础。

Method: 本文首先提出了多智能体精化问题的形式化模型，包括正确性保证和智能体推理的形式化语义。然后，介绍了 Aegean 这一专为随机推理智能体设计的共识协议，用于解决多智能体精化问题。最后，在 Aegean-Serve 中实现了该协议，该服务引擎具有共识感知能力，允许在并发智能体执行中进行增量法定人数检测，从而实现早期终止。

Result: Aegean 协议在提供可证明的安全性和活性保证的同时，将延迟降低了 1.2 至 20 倍，而答案质量仅维持在 2.5% 的损失范围内。在本地 GPU 部署和商业 API 提供商上的持续收益验证了基于共识的编排在不牺牲正确性的前提下消除了“拖油瓶”延迟。

Conclusion: 多智能体系统需要一个正式的基础来保证可靠性，而 Aegean 正是一个解决多智能体精化问题的共识协议。它通过增量法定人数检测实现了早期终止，并在保证正确性的同时显著降低了延迟。这项工作为可靠的多智能体推理提供了一个新的范式。

Abstract: Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.
  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness.

</details>


### [6] [Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs](https://arxiv.org/abs/2512.20210)
*Yinan Ni,Xiao Yang,Yuqi Tang,Zhimin Qiu,Chen Wang,Tingzhou Yuan*

Main category: cs.DC

TL;DR: 该论文与 MLIR、编译器、HLS、DSL 和图处理中的 **无** 直接关联。它可能与 **编译器** 领域的优化技术有间接关联，因为它涉及系统级的性能优化和资源管理。主要涉及 **LLM 推理服务** 的性能优化和 **内存管理** 方面的工作。

该论文提出了 P-LoRA，一个主动且对碎片化敏感的无服务器 LLM 推理系统，用于服务基于 LoRA 的 LLM。为了解决冷启动延迟和 GPU 内存碎片化问题，P-LoRA 引入了基于 LSTM 的流量预测器来主动预取适配器（将冷启动延迟降低 68%），以及受操作系统虚拟内存启发的分页式适配器内存管理机制（保持 GPU 内存利用率在 87% 以上）。实验结果表明，P-LoRA 在高并发下比 S-LoRA 吞吐量提高了 1.52 倍，平均首个 Token 生成时间 (TTFT) 降低了 35%。


<details>
  <summary>Details</summary>
Motivation: 在无服务器环境中部署基于 LoRA 的 LLM 推理服务面临两个关键挑战：
1. **冷启动延迟：** 响应式地加载适配器会导致显著的冷启动延迟。
2. **GPU 内存碎片化：** 频繁的适配器交换会导致严重的 GPU 内存碎片化。

Method: P-LoRA 采用了两项关键创新技术：
1. **基于 LSTM 的轻量级流量预测器：** 用于预测适配器需求，并主动地将热门适配器从主机内存预取到 GPU，以减少冷启动延迟。
2. **受操作系统虚拟内存启发的分页式适配器内存管理机制：** 用于管理适配器内存，解决 GPU 内存碎片化问题，并在异构适配器秩下保持较高的 GPU 内存利用率。

Result: P-LoRA 取得了以下实验结果：
1. 冷启动延迟最多降低了 **68%**。
2. 即使在异构适配器秩下，GPU 内存利用率保持在 **87%** 以上。
3. 吞吐量比 S-LoRA 高 **1.52 倍**。
4. 在高并发场景下，平均首个 Token 生成时间（TTFT）降低了 **35%**。

Conclusion: P-LoRA 通过引入基于 LSTM 的流量预测器和受操作系统虚拟内存启发的分页式适配器内存管理机制，有效解决了在无服务器环境中部署 LoRA LLM 推理服务时面临的冷启动延迟和 GPU 内存碎片化问题。实验结果表明，P-LoRA 在高并发场景下显著提高了吞吐量并降低了平均首个 Token 生成时间 (TTFT)。

Abstract: The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [3D Stack In-Sensor-Computing (3DS-ISC): Accelerating Time-Surface Construction for Neuromorphic Event Cameras](https://arxiv.org/abs/2512.20073)
*Hongyang Shang,Shuai Dong,Ye Ke,Arindam Basu*

Main category: cs.AR

TL;DR: 该论文与**编译器（Compiler）**部分相关，因为它涉及到定制的硬件加速架构和低功耗电路设计，这通常是高级综合（HLS）和特定领域架构编译器优化的目标。

这项工作提出了一种 3D Stack In-Sensor-Computing（3DS-ISC）的事件视觉处理架构，通过引入基于指数衰减的实时归一化方法和利用 DRAM 泄漏特性的定制模拟电路设计，成功地将传感、存储和计算集成起来。3DS-ISC 大幅减少了功耗（最高 69 倍）和面积（1.9 倍），解决了传统的存储墙问题。在去噪、图像分类和图像重建等多个 CV 任务中，该架构均取得了与高精度数字实现相当或更优的性能，为实时、资源高效的事件处理提供了新的基础。


<details>
  <summary>Details</summary>
Motivation: 现有的事件视觉处理系统面临的主要挑战是严重的**存储墙问题**（memory wall problem），这导致了高功耗、高延迟和较大的芯片面积。传统的数字实现方案，特别是使用 16-bit SRAM 存储时间戳的方法，需要大量的能量和资源。因此，需要一种集成的、高效的架构，能够将传感、存储和计算结合起来，以实时、低功耗、小面积地处理事件数据。这项工作的动机正是为了解决这些限制，提出一种高度集成的感算一体化（In-Sensor-Computing）架构。

Method: 本文提出了一个名为 3D Stack In-Sensor-Computing（3DS-ISC）的架构，用于高效的事件视觉处理。核心方法包括：1. **实时归一化（Real-time Normalization）**：引入基于指数衰减函数的实时归一化方法来构建时间表面（time-surface），使用泄漏特性进行时间戳归一化，以减少硬件开销并保留时间信息。2. **定制电路设计**：利用 DRAM（动态随机存取存储器）的泄漏特性进行时间戳归一化。使用定制的交错金属-氧化物-金属电容器（MOMCAP）来存储电荷，并使用低泄漏开关（LL switch）延长有效电荷存储时间。3. **集成与计算**：将传感、存储和计算功能集成，以解决存储墙问题。4. **应用与性能评估**：使用空间-时间相关滤波器（STCF）进行去噪，并将其时间表面应用于 GoogleNet 进行图像分类任务（N-MNIST, N-Caltech101, CIFAR10-DVS, DVS128 Gesture），以及应用于图像重建（DAVIS240C 数据集）来验证其在真实计算机视觉任务中的性能。

Result: 3D Stack In-Sensor-Computing（3DS-ISC）架构与 2D 对应物相比，将功耗、延迟和面积分别减少了 **69 倍、2.2 倍和 1.9 倍**。与使用 16-bit SRAM 存储时间戳的工作相比，ISC 模拟阵列可以将功耗降低 **三个数量级**。在实际计算机视觉任务中：1. **去噪**：使用空间-时间相关滤波器（STCF）进行去噪时，3D-ISC 达到了与高精度时间戳数字实现**几乎相当的精度**。2. **图像分类**：时间表面作为 GoogleNet 的输入，在 N-MNIST 上达到了 99%，N-Caltech101 上达到 85%，CIFAR10-DVS 上达到 78%，DVS128 Gesture 上达到 97%，**与最先进的结果相当**。3. **图像重建**：在 DAVIS240C 数据集上实现了最高的平均 SSIM（0.62），**优于三种比较方法**。

Conclusion: 这项工作提出了一个高效且资源受限的事件处理新框架。3DS-ISC 架构，结合了新颖的实时时间表面归一化和模拟电路设计，成功克服了传统数字实现中的存储墙问题和高功耗，同时在多个计算机视觉任务中保持了与高精度数字实现相当甚至超越的性能。这项工作为实时、资源高效的事件处理奠定了基础，并预示着未来计算电路的集成将带来更广泛的应用。

Abstract: This work proposes a 3D Stack In-Sensor-Computing (3DS-ISC) architecture for efficient event-based vision processing. A real-time normalization method using an exponential decay function is introduced to construct the time-surface, reducing hardware usage while preserving temporal information. The circuit design utilizes the leakage characterization of Dynamic Random Access Memory(DRAM) for timestamp normalization. Custom interdigitated metal-oxide-metal capacitor (MOMCAP) is used to store the charge and low leakage switch (LL switch) is used to extend the effective charge storage time. The 3DS-ISC architecture integrates sensing, memory, and computation to overcome the memory wall problem, reducing power, latency, and reducing area by 69x, 2.2x and 1.9x, respectively, compared with its 2D counterpart. Moreover, compared to works using a 16-bit SRAM to store timestamps, the ISC analog array can reduce power consumption by three orders of magnitude. In real computer vision (CV) tasks, we applied the spatial-temporal correlation filter (STCF) for denoise, and 3D-ISC achieved almost equivalent accuracy compared to the digital implementation using high precision timestamps. As for the image classification, time-surface constructed by 3D-ISC is used as the input of GoogleNet, achieving 99% on N-MNIST, 85% on N-Caltech101, 78% on CIFAR10-DVS, and 97% on DVS128 Gesture, comparable with state-of-the-art results on each dataset. Additionally, the 3D-ISC method is also applied to image reconstruction using the DAVIS240C dataset, achieving the highest average SSIM (0.62) among three methods. This work establishes a foundation for real-time, resource-efficient event-based processing and points to future integration of advanced computational circuits for broader applications.

</details>


### [8] [Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling](https://arxiv.org/abs/2512.20198)
*Huizheng Wang,Taiquan Wei,Hongbin Wang,Zichuan Wang,Xinru Tang,Zhiheng Yue,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: 该论文与编译器和HLS相关（算法-硬件协同设计，加速器架构，数据流优化）。这是一个专注于提高大语言模型（LLMs）推理效率和吞吐量的算法-硬件协同设计加速器研究。STAR通过引入跨阶段协同、基于前导零的稀疏性预测以及优化的FlashAttention机制，专门针对大规模令牌并行（LTPP）场景进行了优化。实验结果表明，STAR在速度、能效和面积效率上均显著优于现有GPU和SOTA加速器，并在超长序列处理上表现出色。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的自注意力机制依赖于高吞吐量推理和大规模令牌并行（LTPP）。现有的动态稀疏性加速器在LTPP场景下效率低下，原因是它们采用了孤立的阶段优化（stage-isolated optimizations）。作者认为，通过对端到端稀疏性加速流程的重新审视，发现了一个被忽视的机会：跨阶段的协同（cross-stage coordination）可以显著减少冗余计算和内存访问，从而提高效率。

Method: STAR的核心方法是跨阶段协同（cross-stage coordination），旨在减少冗余计算和内存访问。具体实施包括：1. 提出了一種基于前导零（leading-zero-based）的稀疏性预测方法，使用对数域加法操作来最小化预测开销。2. 采用了分布式排序和一种排序更新的FlashAttention机制。3. 引入了协调分块策略（coordinated tiling strategy），以实现细粒度的阶段交互，提高内存效率和延迟。4. 设计了专门的STAR加速器架构（dedicated STAR accelerator architecture）来支持这些优化。5. 将STAR部署到多核空间架构（multi-core spatial architecture）上，优化数据流和执行编排，以进行超长序列处理。

Result: STAR相较于A100 GPU实现了高达9.2倍的速度提升和71.2倍的能效提升。相较于最先进（SOTA）的加速器，STAR在能效上提高了16.1倍，在面积效率上提高了27.1倍。Spatial-STAR（部署在多核空间架构上的STAR）相较于基线设计，在处理超长序列时实现了20.1倍的吞吐量提升。

Conclusion: STAR是一种用于Transformer推理的跨阶段计算和内存高效的算法-硬件协同设计，特别针对 LTPP。它通过跨阶段协同、基于前导零的稀疏性预测、分布式排序和优化的FlashAttention机制，辅以专门的加速器架构，显著提高了LLM推理的吞吐量和能效，并在超长序列处理方面表现出色。

Abstract: Large language models (LLMs) rely on self-attention for contextual understanding, demanding high-throughput inference and large-scale token parallelism (LTPP). Existing dynamic sparsity accelerators falter under LTPP scenarios due to stage-isolated optimizations. Revisiting the end-to-end sparsity acceleration flow, we identify an overlooked opportunity: cross-stage coordination can substantially reduce redundant computation and memory access. We propose STAR, a cross-stage compute- and memory-efficient algorithm-hardware co-design tailored for Transformer inference under LTPP. STAR introduces a leading-zero-based sparsity prediction using log-domain add-only operations to minimize prediction overhead. It further employs distributed sorting and a sorted updating FlashAttention mechanism, guided by a coordinated tiling strategy that enables fine-grained stage interaction for improved memory efficiency and latency. These optimizations are supported by a dedicated STAR accelerator architecture, achieving up to 9.2$\times$ speedup and 71.2$\times$ energy efficiency over A100, and surpassing SOTA accelerators by up to 16.1$\times$ energy and 27.1$\times$ area efficiency gains. Further, we deploy STAR onto a multi-core spatial architecture, optimizing dataflow and execution orchestration for ultra-long sequence processing. Architectural evaluation shows that, compared to the baseline design, Spatial-STAR achieves a 20.1$\times$ throughput improvement.

</details>


### [9] [Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization](https://arxiv.org/abs/2512.20495)
*He Zhu,Zheng Liu,Xingyang Li,Anbang Wu,Jieru Zhao,Fangxin Liu,Yiming Gan,Jingwen Leng,Yu Feng*

Main category: cs.AR

TL;DR: 部分涉及MLIR或HLS或编译器或DSL或图处理或编译器的部分：无。
太长不看：本文提出了一个名为Nebula的框架，用于大规模3DGS的协同渲染加速，通过流式传输LoD搜索后的中间结果、引入时间感知LoD搜索及新颖的立体光栅化，显著减少了通信带宽，提升了运动到光子的速度。


<details>
  <summary>Details</summary>
Motivation: 当前建筑领域的3D高斯溅射（3DGS）设计忽略了其可扩展性，对于超大规模3DGS而言非常脆弱。此外，VR带宽要求使得无法从云端提供高保真和平滑的VR内容。本文旨在解决大规模3DGS协同渲染中的可伸缩性、带宽和运动到光子延迟问题。

Method: 本文提出了一个名为Nebula的协同加速框架，用于大规模3DGS协同渲染。该方法不流式传输视频，而是在经过LoD搜索后流式传输中间结果，从而将云端和客户端之间的数据通信量减少了1925%。为了增强运动到光子的体验，作者在云端引入了一个时间感知LoD搜索，通过利用帧间的时间相干性来解决不规则内存访问和减少冗余数据访问。在客户端，作者提出了一种新颖的立体光栅化，使得在立体渲染过程中两眼可以共享大部分计算，并实现位精确的质量。

Result: Nebula（具有最小的硬件增强）实现了2.7倍的运动到光子速度提升，并在有损视频流上将带宽开销减少了1925%。

Conclusion: Nebula通过引入时间感知LoD搜索和新颖的立体光栅化，显著提高了大规模3DGS协同渲染的运动到光子速度和带宽效率。作者认为通过最小的硬件补充，可以将运动到光子速度提高2.7倍，并将带宽开销减少1925%。

Abstract: 3D Gaussian splatting (3DGS) has drawn significant attention in the architectural community recently. However, current architectural designs often overlook the 3DGS scalability, making them fragile for extremely large-scale 3DGS. Meanwhile, the VR bandwidth requirement makes it impossible to deliver high-fidelity and smooth VR content from the cloud.
  We present Nebula, a coherent acceleration framework for large-scale 3DGS collaborative rendering. Instead of streaming videos, Nebula streams intermediate results after the LoD search, reducing 1925% data communication between the cloud and the client. To further enhance the motion-to-photon experience, we introduce a temporal-aware LoD search in the cloud that tames the irregular memory access and reduces redundant data access by exploiting temporal coherence across frames. On the client side, we propose a novel stereo rasterization that enables two eyes to share most computations during the stereo rendering with bit-accurate quality. With minimal hardware augmentations, Nebula achieves 2.7$\times$ motion-to-photon speedup and reduces 1925% bandwidth over lossy video streaming.

</details>


### [10] [Composing Mini Oscilloscope on Embedded Systems](https://arxiv.org/abs/2512.20571)
*Brennan Romero,D. G. Perera*

Main category: cs.AR

TL;DR: 不相关。
本文旨在利用 Nuvoton NUC-140 嵌入式系统开发平台重现常规示波器的基本功能，通过定制子板连接外设（如 BNC 探头、键盘、校准信号），并用开发板的 LCD 显示波形。实验证明该系统是一个合格的调试工具，实现了常规示波器 90% 的常用功能，例如各种触发模式、波形缩放和探头校准。


<details>
  <summary>Details</summary>
Motivation: 使用 Nuvoton NUC-140 嵌入式系统开发平台模仿常规示波器的基本功能，以创建一个有能力的调试工具。

Method: 使用 Nuvoton NUC-140 嵌入式系统开发平台作为示波器的前端和显示方法。设计了一个定制的子板，将 NUC-140 连接到各种外围设备，包括两个 BNC 示波器探头连接、一个外部九键键盘和一个校准信号。利用 NUC-140 开发板上的 LCD 作为波形显示。

Result: 所提出的系统具有很强的调试能力，实现了常规示波器中 90% 的常用功能，包括：自动、边沿触发和单次模式；使用垂直和水平缩放进行波形可视化；探头校准。

Conclusion: 本文成功地使用 Nuvoton NUC-140 嵌入式系统开发平台作为前端和显示方法，重现了常规示波器的基本功能。实验结果表明，该系统是一个非常有能力的调试工具，实现了常规示波器中通常使用的 90% 的功能。

Abstract: In this paper, our goal is to reproduce the basic functionalities of a regular oscilloscope, using the Nuvoton NUC-140 embedded systems development platform as the front-end and display method. A custom-built daughter board connects the NUC-140 to a variety of peripherals, including two BNC scope-probe connections, an external nine-button keypad, and a calibration signal. The LCD of the NUC-140 development board serves as the waveform display. From the experimental results, it is demonstrated that our proposed system became a very competent debugging tool. It implements 90% of the features we typically use on original oscilloscopes, including: automatic, edge-triggered, and single modes; waveform visualization using vertical and horizontal scaling; probe calibration.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [11] [Approximation and parameterized algorithms for covering disjointness-compliable set families](https://arxiv.org/abs/2512.20180)
*Zeev Nutov,Anael Vaknin*

Main category: cs.DS

TL;DR: 该论文与图处理（Graph Processing，因为涉及 $k$-MST、$G$-P2P、Steiner 等图上的覆盖问题和最小成本边集问题）和编译器（Compiler，涉及组合优化）相关。TLDR: 本文研究了覆盖一类新性质的集合族（不交性兼容但非对称）的最小成本边集问题，并证明了其近似比为 $O(\alpha \log \tau)$，其中 $\tau$ 是包含极小集合的数量，$\alpha$ 是 $\tau=1$ 时的最佳近似比。这一推广结果首次为广义点对点连接（G-P2P）问题提供了 $O(\log n)$ 的确定性近似算法，并改进了多根覆盖 Steiner 问题的近似比。同时，本文还分析了这些问题的参数化复杂性，证明了在对称情况下是固定参数可解的。


<details>
  <summary>Details</summary>
Motivation: Goemans 和 Williamson 的经典结果表明，覆盖一个“Proper”（即“不交性兼容”且“对称”）集合族的最小成本边集问题，可以通过经典的原始-对偶算法得到 $2$ 的近似比。然而，许多著名的算法问题（如 $k$-MST、$G$-P2P、Group Steiner、覆盖 Steiner 及其多根版本）所对应的集合族是“不交性兼容”的，但不是“对称”的。因此，需要研究针对这类“不交性兼容但非对称”集合族的覆盖问题，以推广 Goemans 和 Williamson 的经典结果，并为这些著名的组合优化问题提供近似保证。

Method: 本文的主要方法是引入了一种新的集合族性质——“不交性兼容”（disjointness-compliable）但“非对称”的集合族，并证明了覆盖这类集合族的最小成本边集问题的近似比为 $O(\alpha \log \tau)$。其中，$\tau$ 是集合族中包含极小集合的数量，$\alpha$ 是 $\tau=1$ 时的最佳近似比。接着，作者将这一理论结果应用于一系列著名的组合优化问题（如 G-P2P、多根覆盖 Steiner 问题）以得到具体的近似算法和近似比。此外，本文还从参数化复杂性（parameterized complexity）的角度，分析了以 $\tau$ 为参数时的覆盖问题的可解性。

Result: 1. **一般性近似比结果：** 证明了对于任何“不交性兼容”但“非对称”的集合族 ${\cal F}$，覆盖它的最小成本边集问题具有 $O(\alpha \log \tau)$ 的近似比，其中 $\tau$ 是 ${\cal F}$ 中包含极小集合的数量，$α$ 是 $\tau=1$ 时的最佳近似比。
2. **具体应用：**
    * 为 G-P2P 问题提供了第一个确定性多项式时间 $O(\log n)$-近似算法（其中 $\tau=1$ 的情况是 $k$-MST 问题）。
    * 为多根版本的覆盖 Steiner 问题（每个根有自己的组集合）提供了 $O(\log^4 n)$ 的近似比（其中 $\tau=1$ 的情况是覆盖 Steiner 问题）。
3. **参数化复杂性结果：**
    * 当以 $\tau$ 为参数时，如果集合族 ${\cal F}$ 是 Proper 的，则覆盖问题是 Fixed Parameter Tractable (FPT)，可在 $O^*(3^\tau)$ 时间内解决。
    * 对于非对称的情况，在 $O^*(3^\tau)$ 时间内可以实现 $\alpha$ 和 $\alpha+1$ 之间的近似比，这在本质上是最佳的。

Conclusion: 本文分析了一类新的集合族覆盖问题，即针对“不交性兼容但非对称”的集合族，并证明了其近似比为 $O(\alpha \log \tau)$，其中 $\tau$ 是包含极小集合的数量，$\alpha$ 是 $\tau=1$ 时的最佳近似比。此外，本文还分析了这类问题在参数化复杂性方面的表现，并在对称和非对称情况下分别得到了固定的参数可解性和近似比结果。这项工作推广了 Goemans 和 Williamson 的经典结果，为一系列著名的组合优化问题（如 G-P2P、多根覆盖 Steiner 问题）提供了新的近似算法或改进了现有算法的近似比。

Abstract: A set-family ${\cal F}$ is disjointness-compliable if $A' \subseteq A \in {\cal F}$ implies $A' \in {\cal F}$ or $A \setminus A' \in {\cal F}$; if ${\cal F}$ is also symmetric then ${\cal F}$ is proper. A classic result of Goemans and Williamson [SODA 92:307-316] states that the problem of covering a proper set-family by a min-cost edge set admits approximation ratio $2$, by a classic primal-dual algorithm. However, there are several famous algorithmic problems whose set-family ${\cal F}$ is disjointness-compliable but not symmetric -- among them $k$-Minimum Spanning Tree ($k$-MST), Generalized Point-to-Point Connection (G-P2P), Group Steiner, Covering Steiner, multiroot versions of these problems, and others. We will show that any such problem admits approximation ratio $O(α\log τ)$, where $τ$ is the number of inclusion-minimal sets in the family ${\cal F}$ that models the problem and $α$ is the best known approximation ratio for the case when $τ=1$. This immediately implies several results, among them the following two. (i) The first deterministic polynomial time $O(\log n)$-approximation algorithm for the G-P2P problem. Here the $τ=1$ case is the $k$-MST problem. (ii) Approximation ratio $O(\log^4 n)$ for the multiroot version of the Covering Steiner problem, where each root has its own set of groups. Here the $τ=1$ case is the Covering Steiner problem.
  We also discuss the parameterized complexity of covering a disjointness-compliable family ${\cal F}$, when parametrized by $τ$. We will show that if ${\cal F}$ is proper then the problem is fixed parameter tractable and can be solved in time $O^*(3^τ)$. For the non-symmetric case we will show that the problem admits approximation ratio between $α$ and $α+1$ in time $O^*(3^τ)$, which is essentially the best possible.

</details>


### [12] [On the near-tightness of $χ\leq 2r$: a general $σ$-ary construction and a binary case via LFSRs](https://arxiv.org/abs/2512.20598)
*Vinicius T. V. Date,Leandro M. Zatesko*

Main category: cs.DS

TL;DR: 该论文与图处理和编译器或 HLS 不相关，但与 DSL 相关（压缩字符串索引中的重复性度量）和 MLIR 相关（该术语不适用）。文章围绕压缩字符串索引中的 $ \chi $ 和 $ r $ 两个重复性度量之间的关系展开, 理论证明 $ \chi \leq 2r $，但经验上该界限较松。本文的目的是通过构造特殊案例来证明 $ \chi \leq 2r $ 渐进紧密性，最终提供了两个证明该上界渐进紧密性的案例：一个针对任意字母表 $ \sigma $ 的通用构造，以及一个针对二元字母表的基于 LFSR 的 de Bruijn 序列构造。此外，研究还表明对于 $ \sigma \geq 3 $ de Bruijn 序列不能缩小 $ \chi $ 和 $ r $ 之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的研究表明，在压缩字符串索引领域，重复性度量 $ \chi $（基于后缀集合）和 $ r $（Burrows--Wheeler 变换 BWT 中的 runs 数量）之间存在理论界 $ \chi \leq 2r $，但经验结果（特别是 $ \sigma=4 $ 时 $ \chi $ 约为 $ 1.13r $ 到 $ 1.33r $）显示这个界限是松散的。本文的动机在于填补理论与经验之间的差距，通过识别渐进紧密的案例来更好地理解 $ \chi \leq 2r $ 这一上界。

Method: 通过构造特殊字符串案例来分析和证明 $ \chi \leq 2r $ 上界的渐进紧密性。具体方法包括：1. 针对任意字母表大小 $ \sigma $ 的一般性构造；2. 针对二元字母表（$ \sigma=2 $）的基于线性反馈移位寄存器（LFSR）的 de Bruijn 序列构造。此外，还对 de Bruijn 序列在循环 BWT 中的运行最小模式进行了新的表征，并分析了 $ \sigma \geq 3 $ 时 de Bruijn 序列的表现。

Result: 本文成功地提供了两个证明 $ \chi \leq 2r $ 上界具有渐进紧密性的案例：1. 针对任意 $ \sigma $ 值的通用构造；2. 针对二元字母表的 de Bruijn 序列构造，该序列是通过 $ \mathbb{F}_2 $ 上的本原多项式的线性反馈移位寄存器（LFSR）生成的。第二个案例还提供了一种新的表征，说明了哪些 de Bruijn 序列在循环 BWT 中实现了文献中的运行最小模式。此外，研究结果表明，对于 $ \sigma \geq 3 $，de Bruijn 序列不能缩小 $ \chi $ 和 $ r $ 之间的差距。

Conclusion: 本文分析了压缩字符串索引中，$ \chi $ 和 $ r $ 两个重复性度量之间的关系。尽管已有理论证明 $ \chi \leq 2r $，但经验结果表明该上界较松。本文通过构造具有渐进紧密性的两个案例：一个针对任意字母表大小 $ \sigma $ 的一般性构造，以及一个针对二元字母表的基于线性反馈移位寄存器（LFSR）的 de Bruijn 序列构造，来证明 $ \chi \leq 2r $ 的渐进紧密性。此外，研究还发现 de Bruijn 序列在 $ \sigma \geq 3 $ 时不能缩小 $ \chi $ 和 $ r $ 之间的差距。这些结果有助于更好地理解和评估基于 $ \chi $ 度量的压缩字符串索引的性能。

Abstract: In the field of compressed string indexes, recent work has introduced suffixient sets and their corresponding repetitiveness measure $χ$. In particular, researchers have explored its relationship to other repetitiveness measures, notably $r$, the number of runs in the Burrows--Wheeler Transform (BWT) of a string. Navarro et al. (2025) proved that $χ\leq 2r$, although empirical results by Cenzato et al. (2024) suggest that this bound is loose, with real data bounding $χ$ by around $1.13r$ to $1.33r$ when the size of the alphabet is $σ= 4$. To better understand this gap, we present two cases for the asymptotic tightness of the $χ\leq 2r$ bound: a general construction for arbitrary $σ$ values, and a binary alphabet case, consisting of de Bruijn sequences constructed by linear-feedback shift registers (LFSRs) from primitive polynomials over $\mathbb{F}_2$. The second is a novel characterization of which de Bruijn sequences achieve the literature run-minimal pattern for the cyclic BWT. Moreover, we show that de Bruijn sequences fail to close the gap for $σ\geq 3$.

</details>
