<div id=toc></div>

# Table of Contents

- [cs.DS](#cs.DS) [Total: 3]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [1] [Weighted Fourier Factorizations: Optimal Gaussian Noise for Differentially Private Marginal and Product Queries](https://arxiv.org/abs/2512.21499)
*Christian Janos Lebeda,Aleksandar Nikolov,Haohua Tang*

Main category: cs.DS

TL;DR: This paper is related to **DSL** (marginal queries can be considered as a domain-specific language for statistical queries) and **Compiler** (mechanisms for query answering can be seen as a form of compilation/transformation of queries). The paper revisits differentially private release of marginal queries with Gaussian noise. It proposes a factorization mechanism based on the Fourier basis which is exactly optimal among all factorization mechanisms for weighted marginal and product queries, runs in polynomial time, and achieves near-optimal performance for extended marginal queries.


<details>
  <summary>Details</summary>
Motivation: 本文的动机是重新审视在差分隐私下发布带有加性（相关）高斯噪声的边际查询问题，旨在找到一种更通用、更简单、运行更快且达到最优性能的机制来处理任意工作负载的加权边际查询，并将其推广到更一般的查询类型（如乘积查询和扩展边际查询）。作者旨在为现有工作（如Xiao et al. [Neurips 2023]）提供一个更优的替代方案。

Method: 本文提出了一种基于傅里叶变换的算法来解决在差分隐私下发布带有加性（相关）高斯噪声的边际查询问题。具体方法是：在傅里叶基中，使用具有经过仔细校准的方差的独立噪声发布查询，然后使用逆傅里叶变换重建边际查询的答案。该方法进一步推广到乘积查询和扩展边际查询。

Result: 作者提出了一种基于傅里叶基的分解机制，该机制被证明在所有分解机制中是完全最优的，无论是在最小化加权噪声方差之和还是最小化最大噪声方差方面。该机制的运行时间是数据集大小和输出大小的多项式时间，优于依赖半定规划的现有算法。该方法恢复了Xiao et al. [Neurips 2023]的结果，但使用了更简单的算法和更优的运行时间。此外，该算法对于推广的乘积查询仍然是完全最优的。对于扩展边际查询，该机制达到了近乎最优的性能，仅在低阶项上有所不同。

Conclusion: 本文重新探讨了在差分隐私下使用加性（相关）高斯噪声发布边际查询的问题，并提出了一种基于傅里叶变换的通用算法。该算法被证明在加权边际查询和乘积查询上，均是所有分解机制中最优的，并且运行时间是多项式级别的。对于扩展边际查询，该算法也达到了近乎最优的性能。这项工作通过更简单、更高效的方法，改进了现有结果，并提供了更广泛的适用性。

Abstract: We revisit the task of releasing marginal queries under differential privacy with additive (correlated) Gaussian noise. We first give a construction for answering arbitrary workloads of weighted marginal queries, over arbitrary domains. Our technique is based on releasing queries in the Fourier basis with independent noise with carefully calibrated variances, and reconstructing the marginal query answers using the inverse Fourier transform. We show that our algorithm, which is a factorization mechanism, is exactly optimal among all factorization mechanisms, both for minimizing the sum of weighted noise variances, and for minimizing the maximum noise variance. Unlike algorithms based on optimizing over all factorization mechanisms via semidefinite programming, our mechanism runs in time polynomial in the dataset and the output size. This construction recovers results of Xiao et al. [Neurips 2023] with a simpler algorithm and optimality proof, and a better running time.
  We then extend our approach to a generalization of marginals which we refer to as product queries. We show that our algorithm is still exactly optimal for this more general class of queries. Finally, we show how to embed extended marginal queries, which allow using a threshold predicate on numerical attributes, into product queries. We show that our mechanism is almost optimal among all factorization mechanisms for extended marginals, in the sense that it achieves the optimal (maximum or average) noise variance up to lower order terms.

</details>


### [2] [Fully Dynamic Spectral Sparsification for Directed Hypergraphs](https://arxiv.org/abs/2512.21671)
*Sebastian Forster,Gramoz Goranci,Ali Momeni*

Main category: cs.DS

TL;DR: 该论文与图处理相关，因为它处理了超图（图的推广）的谱稀疏化问题。
这篇论文提出了一种简单、近乎最优的全动态算法，用于维护有向超图的谱稀疏化器，其稀疏化器大小为 $O(n^2 / \varepsilon ^2 \log ^7 m)$，摊销更新时间为 $O(r^2 \log ^3 m)$。此外，该方法被扩展到并行批处理动态设置，首次实现了基于谱稀疏化的算法，能以 $O(kr^2 \log ^3 m)$ 的摊销工作量和 $O(\log ^2 m)$ 深度处理一批 $k$ 个超边更新。


<details>
  <summary>Details</summary>
Motivation: 现有的研究对谱超图稀疏化，即图谱稀疏化的自然推广，表现出浓厚的兴趣。然而，对于有向超图的谱稀疏化以及动态维护稀疏化器的问题仍需要新的解决方案。

Method: 本文提出了一种针对有向超图的全动态算法，用于维护谱稀疏化器。该算法采用了一种简单的方法，并且将其扩展到了并行批处理动态（batch-dynamic）设置，这是首次在谱稀疏化领域采用此设置。

Result: 本文提出的全动态算法实现了接近最优的稀疏化器大小 $O(n^2 / \varepsilon ^2 \log ^7 m)$ 和 $O(r^2 \log ^3 m)$ 的摊销更新时间。在并行批处理动态设置中，该算法首次实现了基于谱的稀疏化算法，能够以 $O(kr^2 \log ^3 m)$ 的摊销工作量和 $O(\log ^2 m)$ 的深度处理任意 $k$ 个超边插入或删除批处理操作。

Conclusion: 本文提出了一种针对有向超图谱稀疏化的简单全动态算法，并将其扩展到并行批处理动态设置，为谱稀疏化问题提供了新的有效解决方案，尤其在处理大规模动态超图数据时具有显著优势。

Abstract: There has been a surge of interest in spectral hypergraph sparsification, a natural generalization of spectral sparsification for graphs. In this paper, we present a simple fully dynamic algorithm for maintaining spectral hypergraph sparsifiers of \textit{directed} hypergraphs. Our algorithm achieves a near-optimal size of $O(n^2 / \varepsilon ^2 \log ^7 m)$ and amortized update time of $O(r^2 \log ^3 m)$, where $n$ is the number of vertices, and $m$ and $r$ respectively upper bound the number of hyperedges and the rank of the hypergraph at any time.
  We also extend our approach to the parallel batch-dynamic setting, where a batch of any $k$ hyperedge insertions or deletions can be processed with $O(kr^2 \log ^3 m)$ amortized work and $O(\log ^2 m)$ depth. This constitutes the first spectral-based sparsification algorithm in this setting.

</details>


### [3] [A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication](https://arxiv.org/abs/2512.21980)
*A. I. Perminov*

Main category: cs.DS

TL;DR: 与 DSL、图处理、MLIR、编译器或 HLS **无关**。
该论文提出了一种新的计算 $3 \times 3$ 非交换环上矩阵乘法的最先进算法，通过结合三元限制的翻转图探索和贪婪交集约减的自动化搜索，在保持 23 秩的前提下，将加法复杂度从 60 次降低到 58 次，将总标量操作数从 83 次降低到 81 次。新方案仅使用 $\{-1, 0, 1\}$ 系数，确保了高效性和可移植性。


<details>
  <summary>Details</summary>
Motivation: 优化的目标是寻找计算精确 $3 \times 3$ 非交换环上矩阵乘法的更快速算法，具体来说，是在保持 23 秩（即 23 次乘法）的前提下，**降低加法复杂度**和**总标量操作数**，从而提高计算效率和可移植性。原先最佳方案的加法复杂度为 60 次，本文旨在超越该记录。

Method: 研究通过自动化搜索发现新的 $3 \times 3$ 矩阵乘法方案。该搜索方法结合了**三元限制的翻转图探索 (ternary-restricted flip-graph exploration)** 和用于公共子表达式消除的**贪婪交集约减 (greedy intersection reduction)**。这种方法能够系统地探索并优化乘法方案中的加法操作。最终的方案使用了仅包含 $\{-1, 0, 1\}$ 的系数。

Result: 本文提出了一种新的精确 $3 \times 3$ 非交换环上矩阵乘法方案：

1.  **秩 (Rank):** 保持了 23 秩（即 23 次乘法）。
2.  **加法复杂度 (Additive Complexity):** 达到 58 次标量加法，优于此前最佳的 60 次。
3.  **总标量操作数 (Total Scalar Operation Count):** 从 83 次降低到 81 次。
4.  **系数特性:** 方案中仅使用 $\{-1, 0, 1\}$ 中的系数，极大提高了跨任意域的**效率**和**可移植性**。

Conclusion: 本文提出了一种新的 $3 \times 3$ 矩阵乘法快速算法，该算法在非交换环上实现了 23 秩的方案，并将加法复杂度降低到 58 次，总标量操作数降低到 81 次。该方案使用仅包含 $\{-1, 0, 1\}$ 系数的乘法因子，保证了跨任意域的效率和可移植性。这一改进通过结合三元限制的翻转图探索和用于公共子表达式消除的贪婪交集约减的自动化搜索方法发现。

Abstract: This paper presents a new state-of-the-art algorithm for exact $3\times3$ matrix multiplication over general non-commutative rings, achieving a rank-23 scheme with only 58 scalar additions. This improves the previous best additive complexity of 60 additions without a change of basis. The result was discovered through an automated search combining ternary-restricted flip-graph exploration with greedy intersection reduction for common subexpression elimination. The resulting scheme uses only coefficients from $\{-1, 0, 1\}$, ensuring both efficiency and portability across arbitrary fields. The total scalar operation count is reduced from 83 to 81.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum](https://arxiv.org/abs/2512.21340)
*Dimitrios Amaxilatis,Themistoklis Sarantakos,Nikolaos Tsironis,Souvik Sengupta,Kostas Ramantas,Jhofre Ojeda*

Main category: cs.DC

TL;DR: 本文与DSL、图处理、MLIR、编译器、HLS等技术领域无关。

智慧城市正采纳数据中心架构，以增强城市服务的效率、可持续性和恢复力。


<details>
  <summary>Details</summary>
Motivation: 智慧城市寻求提升城市服务的效率、可持续性和恢复力，以此促进进步和发展。因此，采用数据中心架构成为驱动这些提升的关键动机。

Method: 抽象层面上，本文提出通过采用数据中心架构来支持智慧城市的发展。这种架构的实施将是提升城市服务效率的关键方法。

Result: 通过在智慧城市中采用数据中心架构，实现了城市服务的效率、可持续性和恢复力的增强。具体的增强程度和影响范围未在摘要中详细说明。

Conclusion: 考虑到摘要简短且缺乏具体细节，对于城市服务管理效率、可持续性和恢复力的提升，需要进一步研究数据中心架构的具体实现方式及其对智慧城市各个领域的影响。未来的工作应着重于探索如何最优化地利用这些架构，以应对智慧城市发展中所面临的挑战。

Abstract: Smart cities are increasingly adopting data-centric architectures to enhance the efficiency, sustainability, and resilience of urban services.

</details>


### [5] [Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism](https://arxiv.org/abs/2512.21487)
*Xinglin Pan,Shaohuai Shi,Wenxiang Lin,Yuxin Wang,Zhenheng Tang,Wei Wang,Xiaowen Chu*

Main category: cs.DC

TL;DR: 该论文与编译器相关，因为它涉及高效的任务调度和并行优化，特别是在分布式硬件上实现高性能推理。

该论文提出 FinDEP，一个针对分布式专家并行（DEP）的细粒度任务调度算法，旨在解决 MoE 模型推理时任务重叠度低和性能受限的问题。FinDEP 通过将计算/通信划分为更小的任务、构建灵活的调度优化模型和开发高效的求解器，最大化任务重叠，从而在 DeepSeek-V2 和 Qwen3-MoE 等模型上，实现了高达 1.61 倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有的 MoE 架构虽然能以亚线性计算增长扩展模型规模，但由于键值（KV）缓存和稀疏专家激活，导致推理时的内存需求大。最近的分布式专家并行（DEP）将注意力机制和专家分配给专用的 GPU 组，但它缺乏对共享专家和高效任务调度的支持，从而限制了性能。因此，需要一种更精细的调度方法来提高 MoE 推理的吞吐量。

Method: 作者提出了 FinDEP，一个针对 DEP 的细粒度任务调度算法，旨在通过最大化任务重叠来提高 MoE 推理的吞吐量。具体方法包括：1）将计算和通信划分为更小的任务，实现细粒度流水线；2）建立一个支持可变粒度和排序的调度优化模型；3）开发一个高效的求解器来解决大型搜索空间问题。

Result: 在四个 GPU 系统上使用 DeepSeek-V2 和 Qwen3-MoE 模型的实验中，FinDEP 相较于现有方法，将吞吐量提高了高达 1.61 倍，并在一个 32-GPU 系统上实现了高达 1.24 倍的加速。

Conclusion: FinDEP通过细粒度的任务调度和流水线化，有效地解决了分布式 MoE 推理中任务重叠度不足的问题，显著提高了吞吐量。其核心创新在于对计算和通信任务的细粒度划分、灵活的调度优化模型以及高效的求解器。FinDEP 的提出对于大规模 MoE 模型的部署和高效推理具有重要意义。

Abstract: The mixture-of-experts (MoE) architecture scales model size with sublinear computational increase but suffers from memory-intensive inference due to KV caches and sparse expert activation. Recent disaggregated expert parallelism (DEP) distributes attention and experts to dedicated GPU groups but lacks support for shared experts and efficient task scheduling, limiting performance.
  We propose FinDEP, a fine-grained task scheduling algorithm for DEP that maximizes task overlap to improve MoE inference throughput. FinDEP introduces three innovations: 1) partitioning computation/communication into smaller tasks for fine-grained pipelining, 2) formulating a scheduling optimization supporting variable granularity and ordering, and 3) developing an efficient solver for this large search space.
  Experiments on four GPU systems with DeepSeek-V2 and Qwen3-MoE show FinDEP improves throughput by up to 1.61x over prior methods, achieving up to 1.24x speedup on a 32-GPU system.

</details>


### [6] [nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures](https://arxiv.org/abs/2512.21571)
*Hui Guo,Qihang Zheng,Chenghai Huo,Dongliang Guo,Haoqi Yang,Yang Zhang*

Main category: cs.DC

TL;DR: This paper is related to compiler and DSL. More specifically, it introduces an end-to-end compilation framework named nncase for efficient deployment of LLMs on diverse hardware. The core of nncase is an e-graph-based term rewriting engine and three key optimization modules (Auto Vectorize, Auto Distribution, Auto Schedule). nncase outperforms MLC LLM and Intel IPEX and achieves performance comparable to the hand-optimized llama.cpp, validating the viability of automated compilation for high-performance LLM deployment.


<details>
  <summary>Details</summary>
Motivation: 高效部署大型语言模型（LLMs）受到内存架构异构性的阻碍。传统的编译器工作流程零碎，适应成本高，难以有效应对这种异构性。因此，需要一个端到端、统一的编译框架来优化跨多样化目标（异构硬件）的 LLM 部署。

Method: nncase 框架包含一个基于 e-graph 的项重写引擎，用于全局探索计算和数据移动策略，以减轻相位排序问题。它集成了三个关键优化模块：1. **自动向量化 (Auto Vectorize)**：适应异构计算单元。2. **自动分布 (Auto Distribution)**：搜索并行策略，并进行成本感知的通信优化。3. **自动调度 (Auto Schedule)**：最大化片上缓存局部性。此外，一个缓冲区感知的代码生成 (Codegen) 阶段确保了高效的内核实例化。

Result: 在 Qwen3 系列模型上的评估显示，nncase 的性能超越了主流框架，如 MLC LLM 和 Intel IPEX。在 CPU 上，nncase 的性能与经过手动优化的 llama.cpp 相当。这证明了 nncase 在实现高性能 LLM 自动化部署方面的有效性。

Conclusion: nncase 是一个端到端、开源的编译框架，通过集成的优化模块（自动向量化、自动分布、自动调度）和一个基于 e-graph 的项重写引擎，解决了 LLM 部署中内存架构异构性和传统编译器的局限性。nncase 在性能上超越了主流框架（如 MLC LLM 和 Intel IPEX），达到了与手写优化 llama.cpp 相当的水平，证明了自动化编译在高性能 LLM 部署中的可行性。

Abstract: The efficient deployment of large language models (LLMs) is hindered by memory architecture heterogeneity, where traditional compilers suffer from fragmented workflows and high adaptation costs. We present nncase, an open-source, end-to-end compilation framework designed to unify optimization across diverse targets. Central to nncase is an e-graph-based term rewriting engine that mitigates the phase ordering problem, enabling global exploration of computation and data movement strategies. The framework integrates three key modules: Auto Vectorize for adapting to heterogeneous computing units, Auto Distribution for searching parallel strategies with cost-aware communication optimization, and Auto Schedule for maximizing on-chip cache locality. Furthermore, a buffer-aware Codegen phase ensures efficient kernel instantiation. Evaluations show that nncase outperforms mainstream frameworks like MLC LLM and Intel IPEX on Qwen3 series models and achieves performance comparable to the hand-optimized llama.cpp on CPUs, demonstrating the viability of automated compilation for high-performance LLM deployment. The source code is available at https://github.com/kendryte/nncase.

</details>


### [7] [BLEST: Blazingly Efficient BFS using Tensor Cores](https://arxiv.org/abs/2512.21967)
*Deniz Elbek,Kamer Kaya*

Main category: cs.DC

TL;DR: 该论文与图处理（广度优先搜索作为核心图核）和编译器/硬件加速（利用GPU Tensor Cores进行加速和性能优化）相关。
广度优先搜索（BFS）是图计算的基础内核，但将它有效地映射到现代GPU的高吞吐量Tensor Cores（TC）上面临负载均衡、冗余和同步的挑战。BLEST是一个TC加速的框架，它将拉式BFS流水线围绕位图结构和精心设计的执行布局进行重构。它引入了Binarised Virtual Slice Sets (BVSS)以在warp级别实现负载均衡，并使用两种图重排序策略来优化内存效率和局部性。在计算层面，BLEST开发了一种批处理的稀疏矩阵-稀疏向量乘法模式，该模式利用位操作TC瓦片来减少MMA调用。通过内核融合和延迟顶点更新，BLEST减少了同步开销、原子操作开销并提高了缓存局部性。实验证明，BLEST在真实图中比现有的高性能框架平均快3.58倍到4.9倍。


<details>
  <summary>Details</summary>
Motivation: 现代GPU，如NVIDIA，提供了专用的大吞吐量矩阵乘积累加（MMA）单元（如Tensor Cores, TC），这些单元主要针对密集操作。然而，图计算中的核心算法，如广度优先搜索（BFS），本质上是不规则且非结构化的稀疏操作。如何有效地将BFS等不规则图计算映射到TC等密集硬件单元上，同时避免冗余、负载不均衡和同步开销，是提高图处理性能的关键挑战。该研究的动机在于利用TC的卓越吞吐量来加速图计算，克服稀疏性带来的挑战。

Method: BLEST是一个TC加速的框架，它围绕位图（bitmap-oriented）结构和精心设计的执行布局重新构建了拉式（pull-based）BFS流水线。关键方法包括：
1. **Binarised Virtual Slice Sets (BVSS)**：引入该机制以在warp级别强制进行负载均衡，并消除对前沿（frontier）不敏感的工作分配。
2. **图重排序策略**：应用两种互补的重排序策略以提高内存效率和更新局部性：面向压缩的重排序（适用于社交图）和减少带宽的重排序（适用于非社交图）。
3. **批处理 SpMSpV 乘法模式**：开发了一种使用位操作TC瓦片（bitwise TC tiles）的批处理稀疏矩阵-稀疏向量乘法模式，以在不浪费输出条目的情况下处理点积，从而减少所需的MMA调用次数。
4. **内核融合与延迟顶点更新**：结合内核融合和延迟顶点更新方案，以减少主机端同步、减轻原子操作开销并提高缓存局部性。

Result: 实验结果表明，BLEST在广泛的真实世界图数据集上，相对于现有先进的图处理框架，取得了显著的性能提升。具体而言，与 BerryBees 相比，平均加速比达到 3.58 倍；与 Gunrock 相比，平均加速比达到 4.64 倍；与 GSWITCH 相比，平均加速比达到 4.9 倍。

Conclusion: BLEST通过重新设计以位图为导向的拉式BFS流水线，并精心设计执行布局，将图计算的高效性与GPU张量核心（TC）的高吞吐量相结合。它在各种实际图数据集上取得了显著的性能提升，平均加速比分别达到 BerryBees 的 3.58 倍、Gunrock 的 4.64 倍和 GSWITCH 的 4.9 倍，证明了其在利用现代GPU硬件进行图算法加速方面的有效性。

Abstract: Breadth-First Search (BFS) is a fundamental graph kernel that underpins a wide range of applications. While modern GPUs provide specialised Matrix-Multiply-Accumulate (MMA) units, e.g., Tensor Cores (TC), with extremely high throughput, they target dense operations, making it non-trivial to exploit them for irregular, unstructured graph computations. In particular, fully utilising them for a BFS requires an efficient mapping of the edge operations onto TCs while avoiding redundancy, load imbalance, and synchronisation. We present BLEST, a TC-accelerated framework that reformulates the pull-based BFS pipeline around a bitmap-oriented structure and a carefully engineered execution layout. BLEST introduces Binarised Virtual Slice Sets (BVSS) to enforce warp-level load balancing and to eliminate frontier-oblivious work assignment. To improve both memory efficiency and update locality across diverse graphs, we apply two complementary graph reordering strategies: a compression-oriented ordering for social-like graphs and a bandwidth-reducing ordering for non-social graphs. At the compute level, we develop a batched SpMSpV multiplication pattern that uses the bitwise TC tiles to handle dot products without wasting output entries, thereby reducing the number of required MMA calls. Finally, BLEST combines kernel fusion with a lazy vertex update scheme to reduce host-side synchronisation, mitigate atomic overheads, and improve cache locality. Experiments show that BLEST delivers, on average, $3.58\times$, $4.64\times$ and $4.9\times$ speedup over BerryBees, Gunrock, and GSWITCH, respectively, across a broad set of real-world graphs.

</details>


### [8] [Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models](https://arxiv.org/abs/2512.21884)
*Tingyang Sun,Ting He,Bo Ji,Parimal Parag*

Main category: cs.DC

TL;DR: 该论文与编译器、HLS、DSL、或 MLIR 无关。它与图处理（Graph Processing）无直接关联，但涉及分布式系统、模型块（图结构数据的一种表示）的放置和通信优化，如果将LLM模型结构理解为一种超大规模的计算图，部分模型块到GPU的映射和通信可以被泛化的视为图处理和优化问题的一部分，因此在广义上与图处理有一丁点关联，不过它主要的核心是**分布式系统（Distributed System）**和**资源调度/管理（Resource Scheduling/Management）**。

总结：大语言模型（LLM）推理成本高昂，PETALS等分布式系统通过将模型块分散到低端GPU上部署，但性能严重依赖资源分配。本文首次系统研究了分布式LLM推理中的块放置和请求路由资源分配问题，提出了经实验验证的性能模型、NP-难性证明，以及一种具有性能保证的多项式复杂度离线和在线优化算法。实验和仿真结果表明，该方案能显著缩短推理时间。此外，还开发了一个轻量级的纯CPU模拟器用于性能预测。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的推理成本高昂，即使在训练后也需要高端 GPU。现有的 PETALS 等分布式系统通过将模型块分配到互联网上分布的低端 GPU 上以降低部署门槛，但这带来了严峻的资源分配挑战：如何最优地进行模型块放置和请求路由，以避免系统性能受其关键依赖性的限制。本文旨在系统研究分布式 LLM 推理中的资源分配问题，以优化性能。

Method: 本文首先提出了经实验验证的性能模型，用于预测给定块放置和请求路由决策下的推理性能。基于此模型，将离线优化问题表述为混合整数线性规划（MILP）问题，并证明了其 NP-难性，然后提出了一个具有性能保证的多项式复杂度算法。最后，将离线算法调整用于在线设置，在有界负载下具有相同的性能保证。此外，本文还开发了一个轻量级的纯 CPU 模拟器，用于预测分布式 LLM 推理的性能。

Result: 主要成果包括：1) 实验验证的性能模型，能够预测给定块放置和请求路由决策下的推理性能；2) 首次将块放置和请求路由的离线优化问题公式化为混合整数线性规划问题，并证明了其 NP-难性，同时提出了一个具有保证性能的多项式复杂度算法；3) 将离线算法应用于在线设置，在有界负载下具有相同的性能保证。通过实验和仿真验证，所提出的解决方案相比现有最佳方案能够显著缩短在地域分布式服务器上的推理时间。此外，副产品是一个轻量级、纯 CPU 的模拟器，可预测分布式 LLM 推理的性能。

Conclusion: 本文首次系统研究了分布式 LLM 推理中的资源分配问题，提出了经实验验证的性能模型、离线和在线优化算法，并在实验和仿真中证明了所提方案相比现有最佳方案能显著缩短推理时间。这表明了通过优化资源分配，可以克服分布式 LLM 推理系统的性能瓶颈，降低部署成本，同时研究所开发的轻量级模拟器可以有效支持未来的研究和部署评估。

Abstract: Large language models have demonstrated extraordinary performance in many AI tasks but are expensive to use, even after training, due to their requirement of high-end GPUs. Recently, a distributed system called PETALS was developed to lower the barrier for deploying LLMs by splitting the model blocks across multiple servers with low-end GPUs distributed over the Internet, which was much faster than swapping the model parameters between the GPU memory and other cheaper but slower local storage media. However, the performance of such a distributed system critically depends on the resource allocation, and how to do so optimally remains unknown. In this work, we present the first systematic study of the resource allocation problem in distributed LLM inference, with focus on two important decisions: block placement and request routing. Our main results include: experimentally validated performance models that can predict the inference performance under given block placement and request routing decisions, a formulation of the offline optimization of block placement and request routing as a mixed integer linear programming problem together with the NP-hardness proof and a polynomial-complexity algorithm with guaranteed performance, and an adaptation of the offline algorithm for the online setting with the same performance guarantee under bounded load. Through both experiments and experimentally-validated simulations, we have verified that the proposed solution can substantially reduce the inference time compared to the state-of-the-art solution in diverse settings with geographically-distributed servers. As a byproduct, we have also developed a light-weighted CPU-only simulator capable of predicting the performance of distributed LLM inference on GPU servers, which can evaluate large deployments and facilitate future research for researchers with limited GPU access.

</details>


### [9] [FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion](https://arxiv.org/abs/2512.22036)
*Zhuoran Zhu,Chunyang Zhu,Hao Lin,Xu Fu,Yiming Zhou,Quanlu Zhang,Zhenhua Li,Feng Qian,Chao Yu,Boxun Li,Guohao Dai,Yu Wang*

Main category: cs.DC

TL;DR: 涉及领域：编译器/通信库、图处理（数据路由）、DSL（无）。MoE 模型依赖专家并行，需要高效的数据混洗。但现有通信库处理这种混洗效率低，开销大。FUSCO 是一个 MoE 友好的通信库，通过融合数据转换和通信，解决了 MoE 专家主导布局与通信库设备主导布局的冲突。它使用流水线通信引擎、轻量级规划和负载平衡机制，有效执行混洗。FUSCO 在基准测试中比 NCCL 和 DeepEP 快 3.84 倍和 2.01 倍，在端到端 MoE 任务中显著降低了训练和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 大规模混合专家（MoE）模型为了高效训练和推理，依赖于“专家并行”，需要分布式数据混洗将每个令牌路由到其指定的专家。然而，现有通信库处理这种混洗操作效率低下，其开销可以占到整体运行时间的一半以上。因此，迫切需要一种专为 MoE 设计的、能够高效处理数据混洗的通信库，以解决现有库在处理 MoE 专家主导数据布局冲突时的性能瓶颈。

Method: 作者提出了 FUSCO，这是一个 MoE 友好的通信库。FUSCO 的核心方法是基于 MoE 模型的“专家主导”数据布局与现有通信操作所需的“设备主导”布局之间的冲突观察，通过融合数据转换和通信来实现高效轻量级的数据混洗。具体实施上，FUSCO 捕获细粒度数据布局，然后由流水线通信引擎进行解释，在通信路径上有效地执行所需的混洗。此外，作者还设计了轻量级规划和负载平衡机制来消除冗余通信并分散流量。

Result: FUSCO 在代表性基准测试中取得了显著的性能提升。与 NCCL 相比，FUSCO 实现了高达 3.84 倍的加速；与现有最先进的 MoE 通信库 DeepEP 相比，FUSCO 实现了高达 2.01 倍的加速。在端到端 MoE 任务中，与 NCCL 和 DeepEP 相比，FUSCO 分别将训练延迟降低了 1.17-1.39 倍和 1.10-1.19 倍，并将推理中的首个令牌生成延迟降低了 1.09-1.25 倍和 1.06-1.16 倍。

Conclusion: FUSCO 通过融合数据转换和通信的方法，提供了一种 MoE 友好的、高效且轻量级的数据混洗机制。它解决了 MoE 固有的专家主导数据布局与现有通信库期望的设备主导布局之间的冲突问题。FUSCO 显著减少了大规模 MoE 模型训练和推理中的通信开销，特别是在 MoE 模型的专家并行方面表现出色。

Abstract: Large-scale Mixture-of-Experts (MoE) models rely on \emph{expert parallelism} for efficient training and inference, which splits experts across devices and necessitates distributed data shuffling to route each token to its assigned experts. However, existing communication libraries handle this shuffling poorly; its overhead can account for over half of end-to-end runtime. We present FUSCO, an MoE-friendly communication library that achieves efficient and lightweight data shuffling through fused data transformation and communication, based on the key observation that MoE's expert-major data layout conflicts with the device-major layout expected by communication operations. FUSCO captures the fine-grained data layout, which is then interpreted by a pipelined communication engine that performs the required shuffling efficiently along the communication path. Lightweight planning and load-balancing mechanisms complement the engine by eliminating redundant communication and dispersing traffic. Evaluations on representative benchmarks illustrate that FUSCO achieves up to 3.84$\times$ and 2.01$\times$ speedups over NCCL and DeepEP (the state-of-the-art MoE communication library), respectively. In end-to-end MoE tasks, compared to NCCL and DeepEP, FUSCO reduces the training latency by 1.17-1.39$\times$ and 1.10-1.19$\times$, and lowers the first-token generation latency in inference by 1.09-1.25$\times$ and 1.06-1.16$\times$.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [10] [Quantitative Verification of Omega-regular Properties in Probabilistic Programming](https://arxiv.org/abs/2512.21596)
*Peixin Wang,Jianhao Bai,Min Zhang,C. -H. Luke Ong*

Main category: cs.PL

TL;DR: 该论文与 **DSL (Probabilistic Programming)**、**Graph Processing (Temporal evolution/traces)**、**Compiler (Executable programs)**、**HLS (N/A)**、**MLIR (N/A)** 相关。论文介绍了一种名为“时间后验推理”（TPI）的新框架，它将概率编程与时间逻辑相结合，用于计算满足 $\omega$-regular 规范的程序执行轨迹的后验分布，解决了现有方法无法捕捉概率行为时间演化的问题。为提供严格的定量保证，TPI 开发了一种新的方法，通过分解 Rabin 接受条件并构建随机障碍证书来计算 $\omega$-regular 属性满足概率的上界和下界。该方法在原型工具 TPInfer 中得到实现，并被证明在概率模型的时间属性推理中是有效且高效的。


<details>
  <summary>Details</summary>
Motivation: 现有的概率编程推理技术通常只计算固定时间点（通常是程序终止时）程序状态的后验分布，因此无法捕捉概率行为的时间演化。这种局限性促使作者开发一种能够对程序执行的**时间属性**进行推理的新框架。

Method: 本文提出了时间后验推理（TPI）框架，它将概率编程与时间逻辑相结合，用于计算在可能的时间观测条件下满足 $\omega$-regular 规范的执行轨迹的后验分布。为了获得严格的定量保证，作者开发了一种计算 $\omega$-regular 属性满足概率上界和下界的新方法。该方法将 Rabin 接受条件分解为持续性（persistence）和循环（recurrence）组件，并构建随机障碍证书来可靠地约束每个组件。

Result: 作者在原型工具 TPInfer 中实现了所提出的方法，并在基准测试套件上进行了评估。结果表明，该方法能够对概率模型中的丰富时间属性进行有效且高效的推理。计算 $\omega$-regular 属性满足概率上界和下界的新方法能够提供严格的定量保证。

Conclusion: 本文提出了时间后验推理（TPI）框架，它通过计算满足 $\omega$-regular 规范并在可能的时间观测条件下编程执行轨迹的后验分布，解决了现有概率编程推理技术无法捕捉概率行为时间演化的问题。TPI 为概率程序提供了捕捉丰富时间属性的有效且高效的推理机制，并通过新的有界方法提供了严格的定量保证。

Abstract: Probabilistic programming provides a high-level framework for specifying statistical models as executable programs with built-in randomness and conditioning. Existing inference techniques, however, typically compute posterior distributions over program states at fixed time points, most often at termination, thereby failing to capture the temporal evolution of probabilistic behaviors. We introduce temporal posterior inference (TPI), a new framework that unifies probabilistic programming with temporal logic by computing posterior distributions over execution traces that satisfy omega-regular specifications, conditioned on possibly temporal observations. To obtain rigorous quantitative guarantees, we develop a new method for computing upper and lower bounds on the satisfaction probabilities of omega-regular properties. Our approach decomposes Rabin acceptance conditions into persistence and recurrence components and constructs stochastic barrier certificates that soundly bound each component. We implement our approach in a prototype tool, TPInfer, and evaluate it on a suite of benchmarks, demonstrating effective and efficient inference over rich temporal properties in probabilistic models.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling](https://arxiv.org/abs/2512.22066)
*Hannah Atmer,Yuan Yao,Thiemo Voigt,Stefanos Kaxiras*

Main category: cs.AR

TL;DR: 是，该论文与以下领域相关：编译器（加速器设计、能耗优化）、DSL/Graph Processing/MLIR（该论文关注的是LLM推理加速器的硬件设计与优化，虽然不是直接关于编译器前端、DSL或MLIR，但其目标是优化硬件执行效率，这与高性能计算和编译器后端优化紧密相关）。
太长不看摘要：本文研究了SRAM大小和工作频率对LLM推理（预填充和解码阶段）能效和性能的影响。发现SRAM大小主导总能耗，较大SRAM增加静态能耗。高频率在内存瓶颈前能降低延迟和能耗（通过减少静态能耗）。最佳配置是高频率（1200-1400MHz）和小缓冲区（32KB-64KB），实现了最佳能耗延迟积（EDP）。研究强调内存带宽是性能上限，为LLM加速器设计提供了具体的架构见解。


<details>
  <summary>Details</summary>
Motivation: 部署大型语言模型（LLM）的能耗是其成本和环境影响的关键因素。本文的动机是系统地研究片上SRAM大小和工作频率这两个关键硬件参数对LLM推理的能效和性能的影响，特别是分别探讨计算密集型（compute-bound）的预填充（prefill）阶段和内存密集型（memory-bound）的解码（decode）阶段的不同行为。目的是提供具体的架构洞察，以设计节能的LLM加速器。

Method: 本文采用了结合多种模拟工具的仿真方法：使用OpenRAM进行能耗建模；使用LLMCompass进行延迟仿真；使用ScaleSIM进行脉动阵列（systolic array）的运行强度分析。通过这种多工具结合的仿真方法，详细分析了不同硬件配置下的能耗和性能表现。

Result: 研究结果表明：
1. **SRAM大小对能耗的主导作用：** 总能耗主要由SRAM大小决定，无论是在预填充还是解码阶段。较大的SRAM缓冲区会显著增加由于泄漏产生的静态能耗，且不能通过相应的延迟改善来抵消。
2. **高频率的微妙影响：**
    * 提高工作频率能减少预填充阶段的延迟。
    * 提高频率对内存密集型解码阶段延迟的积极影响受到外部内存带宽的限制。
    * **反直觉的发现：** 高计算频率可以通过减少执行时间，从而减少静态能耗的消耗，其减少量大于动态功耗的增加量，反而降低总能耗。
3. **最佳配置：** 最佳的硬件配置是高工作频率（1200MHz-1400MHz）和小的本地缓冲区（32KB-64KB）。
4. **内存带宽瓶颈：** 内存带宽是性能的上限，提高计算频率只有在工作负载仍是计算密集型时才产生性能增益；一旦达到内存瓶颈，性能将不再提高。

Conclusion: 本文的结论是，对于所模拟的LLM推理工作负载，存在一个最佳的硬件配置，即高工作频率（1200MHz-1400MHz）和小的本地缓冲区大小（32KB-64KB）。这种配置实现了最佳的能耗延迟积（EDP），平衡了低延迟和高能效。研究还发现，内存带宽是性能的上限，超出内存瓶颈后，提高计算频率不会带来性能提升。这些具体的架构见解对于设计数据中心中节能的LLM加速器具有重要指导意义。

Abstract: Energy consumption dictates the cost and environmental impact of deploying Large Language Models. This paper investigates the impact of on-chip SRAM size and operating frequency on the energy efficiency and performance of LLM inference, focusing on the distinct behaviors of the compute-bound prefill and memory-bound decode phases. Our simulation methodology combines OpenRAM for energy modeling, LLMCompass for latency simulation, and ScaleSIM for systolic array operational intensity. Our findings show that total energy use is predominantly determined by SRAM size in both phases, with larger buffers significantly increasing static energy due to leakage, which is not offset by corresponding latency benefits. We quantitatively explore the memory-bandwidth bottleneck, demonstrating that while high operating frequencies reduce prefill latency, their positive impact on memory-bound decode latency is capped by the external memory bandwidth. Counter-intuitively, high compute frequency can reduce total energy by reducing execution time and consequently decreasing static energy consumption more than the resulting dynamic power increase. We identify an optimal hardware configuration for the simulated workload: high operating frequencies (1200MHz-1400MHz) and a small local buffer size of 32KB to 64KB. This combination achieves the best energy-delay product, balancing low latency with high energy efficiency. Furthermore, we demonstrate how memory bandwidth acts as a performance ceiling, and that increasing compute frequency only yields performance gains up to the point where the workload becomes memory-bound. This analysis provides concrete architectural insights for designing energy-efficient LLM accelerators, especially for datacenters aiming to minimize their energy overhead.

</details>
