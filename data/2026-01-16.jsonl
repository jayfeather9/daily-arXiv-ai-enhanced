{"id": "2601.09808", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.09808", "abs": "https://arxiv.org/abs/2601.09808", "authors": ["Chen Ling", "Yachen Wang"], "title": "From Dynamic to Lexical: A Comparative Exploration of Scoping Rules in SAS and R", "comment": "This paper was originally published in the SESUG 2025 Conference Proceedings. Cary, NC", "summary": "Variable scoping dictates how and where variables are accessible within programming languages, playing a crucial role in code efficiency and organization. This paper examines the distinct scoping rules in SAS and R, focusing on SAS's dynamic scoping and R's lexical scoping. In SAS, dynamic scoping utilizes symbol tables, resolving variables at runtime by dynamically searching through active macro layers. R, in contrast, employs lexical scoping, using environments to resolve variables based on the structure in which functions are defined. Illustrative examples highlight the differences between these scoping strategies, showcasing their impact on code behavior. Additionally, the paper outlines methods for inspecting variables in SAS's symbol tables and R's environments, offering practical insights for debugging and optimization. Strategies for controlling variable scope in both languages are discussed, enhancing code precision and reliability. This exploration equips programmers with critical understanding to optimize variable management, improving their programming practices in SAS and R."}
{"id": "2601.09839", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.09839", "abs": "https://arxiv.org/abs/2601.09839", "authors": ["Chen Ling", "Yachen Wang"], "title": "Lazy Evaluation: A Comparative Analysis of SAS MACROs and R Functions", "comment": "This paper was originally published in SESUG 2025 Conference Proceedings. Cary, NC: SouthEast SAS Users Group", "summary": "Lazy evaluation is a powerful technique that can optimize code execution by deferring evaluations until their results are required, thus enhancing efficiency. In most modern programming languages, like R, lazy evaluation is commonly applied to function arguments. However, the application of lazy evaluation in SAS has not been extensively explored. This paper focuses on the mechanisms of lazy evaluation in SAS MACROs and R functions, offering a comparative analysis of the underlying principles that drive these processes.\n  R's lazy evaluation is driven by a data structure called Promise, which postpones evaluation and does not occupy memory until the value is needed, utilizing a call-by-need strategy. SAS, on the other hand, achieves lazy evaluation through its symbol tables, employing memory to store parameters, and operates on a call-by-name basis. These discrepancies in lazy evaluation strategies can notably impact the results of R functions and SAS MACROs. By examining these distinct approaches, the paper illuminates the impact of lazy evaluation on programming efficiency, supported by illustrative examples. As the shift from SAS to R becomes increasingly prevalent in the pharmaceutical industry, understanding these techniques enables programmers to optimize their code for greater efficacy. This exploration serves as a guide to enhance programming capabilities and performance in both languages."}
{"id": "2601.09986", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.09986", "abs": "https://arxiv.org/abs/2601.09986", "authors": ["Cheng Zhang", "Qiancheng Fu", "Hang Ji", "Ines Santacruz Del Valle", "Alexandra Silva", "Marco Gaboardi"], "title": "Outrunning Big KATs: Efficient Decision Procedures for Variants of GKAT", "comment": "Conditionally Accepted at ESOP 2026", "summary": "This paper presents several efficient decision procedures for trace equivalence of GKAT automata, which make use of on-the-fly symbolic techniques via SAT solvers. To demonstrate applicability of our algorithms, we designed symbolic derivatives for CF-GKAT, a practical system based on GKAT designed to validate control-flow transformations. We implemented the algorithms in Rust and evaluated them on both randomly generated benchmarks and real-world control-flow transformations. Indeed, we observed order-of-magnitude performance improvements against existing implementations for both KAT and CF-GKAT. Notably, our experiments also revealed a bug in Ghidra, an industry-standard decompiler, highlighting the practical viability of these systems."}
{"id": "2601.09773", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09773", "abs": "https://arxiv.org/abs/2601.09773", "authors": ["Binglei Lou", "Ruilin Wu", "Philip Leong"], "title": "Enhancing LUT-based Deep Neural Networks Inference through Architecture and Connectivity Optimization", "comment": "arXiv admin note: substantial text overlap with arXiv:2503.12829, arXiv:2406.04910", "summary": "Deploying deep neural networks (DNNs) on resource-constrained edge devices such as FPGAs requires a careful balance among latency, power, and hardware resource usage, while maintaining high accuracy. Existing Lookup Table (LUT)-based DNNs -- such as LogicNets, PolyLUT, and NeuraLUT -- face two critical challenges: the exponential growth of LUT size and inefficient random sparse connectivity. This paper presents SparseLUT, a comprehensive framework that addresses these challenges through two orthogonal optimizations. First, we propose an architectural enhancement that aggregates multiple PolyLUT sub-neurons via an adder, significantly reducing LUT consumption by 2.0x-13.9x and lowering inference latency by 1.2x-1.6x, all while maintaining comparable accuracy. Building upon this foundation, we further introduce a non-greedy training algorithm that optimizes neuron connectivity by selectively pruning less significant inputs and strategically regrowing more effective ones. This training optimization, which incurs no additional area and latency overhead, delivers consistent accuracy improvements across benchmarks -- achieving up to a 2.13% gain on MNIST and 0.94% on Jet Substructure Classification compared to existing LUT-DNN approaches."}
{"id": "2601.09860", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.09860", "abs": "https://arxiv.org/abs/2601.09860", "authors": ["Sepideh Mahabadi", "Sherry Sarkar", "Jakub Tarnawski"], "title": "Improved Algorithms for Fair Matroid Submodular Maximization", "comment": null, "summary": "Submodular maximization subject to matroid constraints is a central problem with many applications in machine learning. As algorithms are increasingly used in decision-making over datapoints with sensitive attributes such as gender or race, it is becoming crucial to enforce fairness to avoid bias and discrimination. Recent work has addressed the challenge of developing efficient approximation algorithms for fair matroid submodular maximization. However, the best algorithms known so far are only guaranteed to satisfy a relaxed version of the fairness constraints that loses a factor 2, i.e., the problem may ask for $\\ell$ elements with a given attribute, but the algorithm is only guaranteed to find $\\lfloor \\ell/2 \\rfloor$. In particular, there is no provable guarantee when $\\ell=1$, which corresponds to a key special case of perfect matching constraints.\n  In this work, we achieve a new trade-off via an algorithm that gets arbitrarily close to full fairness. Namely, for any constant $\\varepsilon>0$, we give a constant-factor approximation to fair monotone matroid submodular maximization that in expectation loses only a factor $(1-\\varepsilon)$ in the lower-bound fairness constraint. Our empirical evaluation on a standard suite of real-world datasets -- including clustering, recommendation, and coverage tasks -- demonstrates the practical effectiveness of our methods."}
{"id": "2601.09978", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.09978", "abs": "https://arxiv.org/abs/2601.09978", "authors": ["Jer Shyuan Ng", "Wathsara Daluwatta", "Shehan Edirimannage", "Charitha Elvitigala", "Asitha Kottahachchi Kankanamge Don", "Ibrahim Khalil", "Heng Zhang", "Dusit Niyato"], "title": "Federated Unlearning in Edge Networks: A Survey of Fundamentals, Challenges, Practical Applications and Future Directions", "comment": null, "summary": "The proliferation of connected devices and privacy-sensitive applications has accelerated the adoption of Federated Learning (FL), a decentralized paradigm that enables collaborative model training without sharing raw data. While FL addresses data locality and privacy concerns, it does not inherently support data deletion requests that are increasingly mandated by regulations such as the Right to be Forgotten (RTBF). In centralized learning, this challenge has been studied under the concept of Machine Unlearning (MU), that focuses on efficiently removing the influence of specific data samples or clients from trained models. Extending this notion to federated settings has given rise to Federated Unlearning (FUL), a new research area concerned with eliminating the contributions of individual clients or data subsets from the global FL model in a distributed and heterogeneous environment. In this survey, we first introduce the fundamentals of FUL. Then, we review the FUL frameworks that are proposed to address the three main implementation challenges, i.e., communication cost, resource allocation as well as security and privacy. Furthermore, we discuss applications of FUL in the modern distributed computer networks. We also highlight the open challenges and future research opportunities. By consolidating existing knowledge and mapping open problems, this survey aims to serve as a foundational reference for researchers and practitioners seeking to advance FL to build trustworthy, regulation-compliant and user-centric federated systems."}
{"id": "2601.10463", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.10463", "abs": "https://arxiv.org/abs/2601.10463", "authors": ["Xinyu Shi", "Simei Yang", "Francky Catthoor"], "title": "Architectural Classification of XR Workloads: Cross-Layer Archetypes and Implications", "comment": null, "summary": "Edge and mobile platforms for augmented and virtual reality, collectively referred to as extended reality (XR) must deliver deterministic ultra-low-latency performance under stringent power and area constraints. However, the diversity of XR workloads is rapidly increasing, characterized by heterogeneous operator types and complex dataflow structures. This trend poses significant challenges to conventional accelerator architectures centered around convolutional neural networks (CNNs), resulting in diminishing returns for traditional compute-centric optimization strategies. Despite the importance of this problem, a systematic architectural understanding of the full XR pipeline remains lacking. In this paper, we present an architectural classification of XR workloads using a cross-layer methodology that integrates model-based high-level design space exploration (DSE) with empirical profiling on commercial GPU and CPU hardware. By analyzing a representative set of workloads spanning 12 distinct XR kernels, we distill their complex architectural characteristics into a small set of cross-layer workload archetypes (e.g., capacity-limited and overhead-sensitive). Building on these archetypes, we further extract key architectural insights and provide actionable design guidelines for next-generation XR SoCs. Our study highlights that XR architecture design must shift from generic resource scaling toward phase-aware scheduling and elastic resource allocation in order to achieve greater energy efficiency and high performance in future XR systems."}
{"id": "2601.10511", "categories": ["cs.DS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10511", "abs": "https://arxiv.org/abs/2601.10511", "authors": ["Paul Burkhardt", "David G. Harris", "Kevin T Schmitt"], "title": "Scalable Algorithms for Approximate DNF Model Counting", "comment": null, "summary": "Model counting of Disjunctive Normal Form (DNF) formulas is a critical problem in applications such as probabilistic inference and network reliability. For example, it is often used for query evaluation in probabilistic databases. Due to the computational intractability of exact DNF counting, there has been a line of research into a variety of approximation algorithms. These include Monte Carlo approaches such as the classical algorithms of Karp, Luby, and Madras (1989), as well as methods based on hashing (Soos et al. 2023), and heuristic approximations based on Neural Nets (Abboud, Ceylan, and Lukasiewicz 2020).\n  We develop a new Monte Carlo approach with an adaptive stopping rule and short-circuit formula evaluation. We prove it achieves Probably Approximately Correct (PAC) learning bounds and is asymptotically more efficient than the previous methods. We also show experimentally that it out-performs prior algorithms by orders of magnitude, and can scale to much larger problems with millions of variables."}
{"id": "2601.10177", "categories": ["cs.DC", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.10177", "abs": "https://arxiv.org/abs/2601.10177", "authors": ["Ziting Zhang", "Kai Wan", "Minquan Cheng", "Shuo Shao", "Giuseppe Caire"], "title": "Distributed Linearly Separable Computation with Arbitrary Heterogeneous Data Assignment", "comment": null, "summary": "Distributed linearly separable computation is a fundamental problem in large-scale distributed systems, requiring the computation of linearly separable functions over different datasets across distributed workers. This paper studies a heterogeneous distributed linearly separable computation problem, including one master and N distributed workers. The linearly separable task function involves Kc linear combinations of K messages, where each message is a function of one dataset. Distinguished from the existing homogeneous settings that assume each worker holds the same number of datasets, where the data assignment is carefully designed and controlled by the data center (e.g., the cyclic assignment), we consider a more general setting with arbitrary heterogeneous data assignment across workers, where `arbitrary' means that the data assignment is given in advance and `heterogeneous' means that the workers may hold different numbers of datasets. Our objective is to characterize the fundamental tradeoff between the computable dimension of the task function and the communication cost under arbitrary heterogeneous data assignment. Under the constraint of integer communication costs, for arbitrary heterogeneous data assignment, we propose a universal computing scheme and a universal converse bound by characterizing the structure of data assignment, where they coincide under some parameter regimes. We then extend the proposed computing scheme and converse bound to the case of fractional communication costs."}
{"id": "2601.10706", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.10706", "abs": "https://arxiv.org/abs/2601.10706", "authors": ["Quinten De Man", "Atharva Sharma", "Kishen N Gowda", "Laxman Dhulipala"], "title": "UFO Trees: Practical and Provably-Efficient Parallel Batch-Dynamic Trees", "comment": "To appear in PPoPP 2026", "summary": "The dynamic trees problem is to maintain a tree under edge updates while supporting queries like connectivity queries or path queries. Despite the first data structure for this fundamental problem -- the link-cut tree -- being invented 40 years ago, our experiments reveal that they are still the fastest sequential data structure for the problem. However, link-cut trees cannot support parallel batch-dynamic updates and have limitations on the kinds of queries they support.\n  In this paper, we design a new parallel batch-dynamic trees data structure called UFO trees that simultaneously supports a wide range of query functionality, supports work-efficient parallel batch-dynamic updates, and is competitive with link-cut trees when run sequentially. We prove that a key reason for the strong practical performance of both link-cut trees and UFO trees is that they can perform updates and queries in sub-logarithmic time for low-diameter trees. We perform an experimental study of our optimized C++ implementations of UFO trees with ten other dynamic tree implementations, several of which are new, in a broad benchmark of both synthetic and real-world trees of varying diameter and size. Our results show that, in both sequential and parallel settings, UFO trees are the fastest dynamic tree data structure that supports a wide range of queries. Our new implementation of UFO trees has low space usage and easily scales to billion-size inputs, making it a promising building block for implementing more complex dynamic graph algorithms in practice."}
{"id": "2601.10277", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.10277", "abs": "https://arxiv.org/abs/2601.10277", "authors": ["Evangelos Kolyvas", "Alexandros Antonov", "Spyros Voulgaris"], "title": "SCRamble: Adaptive Decentralized Overlay Construction for Blockchain Networks", "comment": "5 pages, 3 figures, The 27th ACM International Conference on Distributed Computing and Networking, ACM ICDCN 2026", "summary": "Despite being under development for over 15 years, transaction throughput remains one of the key challenges confronting blockchains, which typically has a cap of a limited number of transactions per second. A fundamental factor limiting this metric is the network latency associated with the block propagation throughout of the underlying peer-to-peer network, typically formed through random connections. Accelerating the dissemination of blocks not only improves transaction rates, but also enhances system security by reducing the probability of forks. This paper introduces SCRamble: a decentralized protocol that significantly reduces block dissemination time in blockchain networks. SCRamble's effectiveness is attributed to its innovative link selection strategy, which integrates two heuristics: a scoring mechanism that assesses block arrival times from neighboring peers, and a second heuristic that takes network latency into account."}
{"id": "2601.10582", "categories": ["cs.DC", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.10582", "abs": "https://arxiv.org/abs/2601.10582", "authors": ["Mridankan Mandal", "Smit Sanjay Shende"], "title": "Mitigating GIL Bottlenecks in Edge AI Systems", "comment": null, "summary": "Deploying Python based AI agents on resource-constrained edge devices presents a runtime optimization challenge: high thread counts are needed to mask I/O latency, yet Python's Global Interpreter Lock (GIL) serializes execution. We demonstrate that naive thread-pool scaling causes a \"saturation cliff\": >= 20% throughput degradation at overprovisioned thread counts (N >= 512) on edge-representative configurations. We present a lightweight profiling tool and adaptive runtime system using a Blocking Ratio metric (beta) that distinguishes genuine I/O wait from GIL contention. Our library-based solution achieves 96.5% of optimal performance without manual tuning, outperforming multiprocessing (limited by ~8x memory overhead on devices with 512 MB-2 GB RAM) and asyncio (blocked by CPU-bound phases). Evaluation across seven edge AI workload profiles, including real ML inference with ONNX Runtime MobileNetV2, demonstrates 93.9% average efficiency. Comparative experiments with Python 3.13t (free threading) show that while GIL elimination enables ~4x throughput on multi-core edge devices, the saturation cliff persists on single-core devices, validating our beta metric for both GIL and no-GIL environments. This provides practical optimization for edge AI systems."}
