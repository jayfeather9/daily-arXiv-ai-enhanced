<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [OOPredictor: Predicting Object-Oriented Accesses using Static Analysis](https://arxiv.org/abs/2512.03972)
*Hassan Arafat,David Bremner,Kenneth B. Kent,Julian Wang*

Main category: cs.PL

TL;DR: 该论文与编译器相关，它在 OpenJ9 JVM 的 OMR 优化器基础设施内实现了静态分析。TLDR: 面向对象编程中的指针追逐降低了缓存性能，目前的解决方案（如运行时分析）开销大。本文提出了一种编译时静态分析方法来预测程序访问模式，并将预测结果表示为马尔可夫链。在 OpenJ9 JVM 上实现的实验表明，该预测器具有良好准确性，并可用于指导如复制型垃圾收集器等，以优化数据局部性和缓解负载停顿。


<details>
  <summary>Details</summary>
Motivation: 对象导向编程（OOP）虽然通过关注点分离和设计适应性降低了开发和维护成本，但其固有的间接性导致了过度的“指针追逐”（pointer chasing）。这会破坏数据局部性，严重影响缓存结构的性能。此外，现代硬件预取器（主要是跨距预取器）不擅长处理指针追逐产生的不可预测的访问模式。现有的软件解决方案大多依赖于运行时分析或先前运行的数据，带来了显著的运行时开销。为了解决这个问题，作者提出了一种在编译时进行静态分析的方法来预测最常见的访问模式。

Method: 本文提出了一种基于编译时静态分析的方法来预测程序在运行时最常见的访问模式。具体来说，它在 OpenJ9 JVM 的 OMR 优化器基础设施内实现了原型，用于分析 Java 程序。该方法将预测结果表示为马尔可夫链，以模拟程序的预期行为。通过将模型与使用检测解释器测量的实际运行时行为进行比较来评估预测器的有效性。

Result: 所提出的预测器在 Java 程序中表现出良好的准确性。它的输出是马尔可夫链，能够有效地建模程序预期行为。这种预测能力可以用于指导最小侵入性的负载停顿（load stall）缓解策略，例如通知复制型垃圾收集器（GCs）采用对局部性更友好的复制顺序，从而潜在地提高性能。

Conclusion: 本文提出了一种编译时静态分析方法，用于预测 Java 程序在运行时最常见的对象访问模式，解决了对象导向编程中由于指针追逐导致的性能下降问题。实验结果表明，该预测器具有良好的准确性，可以用于指导最小侵入性的负载停顿缓解策略，例如为复制垃圾收集器提供更优化的复制顺序建议，从而提高程序性能和数据局部性。

Abstract: Object-oriented Programming has become one of the most dominant design paradigms as the separation of concerns and adaptability of design reduce development and maintenance costs. However, the convenience is not without cost. The added indirection inherent in such designs causes excessive pointer chasing, negatively affecting locality, which in turn degrades the performance of cache structures. Furthermore, modern hardware prefetchers are mostly stride prefetchers that are ill-equipped to handle the unpredictability of access patterns generated by pointer chasing. Most software approaches that seek to address this problem resort to profiling the program as it runs, which comes with a significant run-time overhead or requires data from previous runs. In this paper, we propose the use of compile-time static analysis to predict the most common access patterns displayed by a program during run time. Since Java is one of the most popular object-oriented languages, we implement our prototype within the OpenJ9 JVM, inside the OMR optimizer infrastructure. The outputs of our proposed predictor are Markov chains that model the expected behavior of the program. The effectiveness of the proposed predictor is evaluated by comparing the model with the actual run-time behavior of the program measured using an instrumented interpreter. Our experiments show that the proposed predictor exhibits good accuracy and can be used to inform minimally intrusive load stall mitigation strategies, e.g. informing copying GCs on more locality-friendly copying orders

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [2] [On the Complexity of the Ordered Covering Problem in Distance Geometry](https://arxiv.org/abs/2512.03124)
*Michael Souza,Júlio Araújo,John Kesley Costa,Carlile Lavor*

Main category: cs.DS

TL;DR: 关联：无。总结：有序覆盖问题（OCP）出现在离散化分子距离几何问题（DMDGP）中，其剪枝边的排序影响 SBBU 算法的性能。Souza 等人（2023）将 OCP 形式化为具有有序、指数成本的超图覆盖问题，并提出了一个性能优异的贪心启发式算法，但最优解的计算复杂度仍然是未解决的问题。本文通过从强 NP-complete 的 3-Partition 问题进行多项式时间归约，证明了 OCP 是 NP-complete 的。这一结果确立了寻找最优边排序的计算障碍，并为实践中使用的启发式方法提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: OCP 在离散化分子距离几何问题 (DMDGP) 中出现，其中剪枝边的排序显著影响用于蛋白质结构确定的 SBBU 算法的性能。虽然已有的贪心启发式算法性能优于原始 SBBU 排序很多，但寻找 OCP 最优解的计算复杂度仍然是一个未解决的问题。

Method: 通过从强 NP-complete 的 3-Partition 问题到 OCP 的多项式时间归约。归约构造了一个紧密的预算，使得最优解恰好对应于有效的 3-Partition。

Result: 证明了 OCP 是 NP-complete 的。

Conclusion: OCP 是 NP-complete 的证明为寻找最优排序带来了计算障碍，并为目前实践中使用的启发式方法提供了理论依据。

Abstract: The Ordered Covering Problem (OCP) arises in the context of the Discretizable Molecular Distance Geometry Problem (DMDGP), where the ordering of pruning edges significantly impacts the performance of the SBBU algorithm for protein structure determination. In recent work, Souza et al. (2023) formalized OCP as a hypergraph covering problem with ordered, exponential costs, and proposed a greedy heuristic that outperforms the original SBBU ordering by orders of magnitude. However, the computational complexity of finding optimal solutions remained open. In this paper, we prove that OCP is NP-complete through a polynomial-time reduction from the strongly NP-complete 3-Partition problem. Our reduction constructs a tight budget that forces optimal solutions to correspond exactly to valid 3-partitions. This result establishes a computational barrier for optimal edge ordering and provides theoretical justification for the heuristic approaches currently used in practice.

</details>


### [3] [Complexity of Local Search for CSPs Parameterized by Constraint Difference](https://arxiv.org/abs/2512.03275)
*Aditya Anand,Vincent Cohen-Addad,Tommaso d'Orsi,Anupam Gupta,Euiwoong Lee,Debmalya Panigrahi,Sijin Peng*

Main category: cs.DS

TL;DR: 该论文与图处理（**作为经典参数化优化问题的一个泛化**）相关。
该论文研究了局部搜索的参数化复杂性，其中参数 $k$ 定义为当前解 $P$ 与最优可行解 $S^*$ 之间的距离 $|P\Delta S^*|$。作者将此模型应用于约束满足问题（CSPs），即在给定一个当前解和寻找一个与最优解一样好的可行解，并在参数化的时间内找到了所有布尔字母对称 CSPs 参数化复杂性的完整刻画。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究局部搜索的参数化复杂性，特别是针对优化问题的局部搜索，该优化问题是找到宇宙 $U$ 的最大可行子集 $S$。新的输入包括一个当前解 $P$ 和一个通常的输入。在存在一个最优可行解 $S^*$ 的情况下，目标是在参数化的时间内找到一个与 $S^*$ 一样好的可行解，其中参数 $k$ 是当前解 $P$ 与最优可行解 $S^*$ 之间的距离 $|P\Delta S^*|$。该模型推广了许多经典的参数化优化问题，例如最小删除元素使得问题可行的参数化问题，对应于 $P=U$ 的情况。将该参数化模型应用于约束满足问题（CSPs），目标是为所有布尔字母对称 CSP 提供一个完整的参数化复杂性刻画。

Method: 本文提出了一个局部搜索的参数化复杂性模型，其中参数 $k$ 是当前解 $P$ 与最优可行解 $S^*$ 之间的距离 $|P\Delta S^*|$。然后，将该模型应用于布尔字母对称约束满足问题（CSPs），其中 $U$ 是约束集，如果存在满足 $U'$ 中所有约束的赋值，则 $U'$ 约束子集是可行的。本文通过这个模型对所有布尔字母对称 CSP 的参数化复杂性进行了完整的刻画。

Result: 本文给出了所有布尔字母对称约束满足问题（CSPs）的参数化复杂性（基于局部搜索的参数化模型）的完整刻画。

Conclusion: 本文将局部搜索的参数化复杂性应用于布尔字母对称约束满足问题（CSPs），并给出了其参数化复杂性的完整刻画。对于给定的当前解和最优可行解，局部搜索的目标是在参数（**当前解与最优可行解之间的距离 $|P\Delta S^*|$**）的指数时间内找到一个与最优解一样好的可行解。

Abstract: In this paper, we study the parameterized complexity of local search, whose goal is to find a good nearby solution from the given current solution. Formally, given an optimization problem where the goal is to find the largest feasible subset $S$ of a universe $U$, the new input consists of a current solution $P$ (not necessarily feasible) as well as an ordinary input for the problem.
  Given the existence of a feasible solution $S^*$, the goal is to find a feasible solution as good as $S^*$ in parameterized time $f(k) \cdot n^{O(1)}$, where $k$ denotes the distance $|PΔS^*|$. This model generalizes numerous classical parameterized optimization problems whose parameter $k$ is the minimum number of elements removed from $U$ to make it feasible, which corresponds to the case $P = U$.
  We apply this model to widely studied Constraint Satisfaction Problems (CSPs), where $U$ is the set of constraints, and a subset $U'$ of constraints is feasible if there is an assignment to the variables satisfying all constraints in $U'$. We give a complete characterization of the parameterized complexity of all boolean-alphabet symmetric CSPs, where the predicate's acceptance depends on the number of true literals.

</details>


### [4] [Fast approximate $\ell$-center clustering in high dimensional spaces](https://arxiv.org/abs/2512.03304)
*Mirosław Kowaluk,Andrzej Lingas,Mia Persson*

Main category: cs.DS

TL;DR: 该论文不涉及 DSL、图处理、MLIR、编译器或 HLS。该论文的重点是为高维空间中的 $\ell$-中心聚类和最小直径 $\ell$-聚类问题设计高效的近似算法，主要通过随机降维技术来减少这些算法对维度大小的依赖性，从而实现显著的加速，特别是在 $\ell$ 和维度都非常大的情况下。


<details>
  <summary>Details</summary>
Motivation: 在高维欧几里得和汉明空间中，$\ell$-中心聚类和最小直径 $\ell$-聚类是重要的优化问题。现有的近似算法，特别是当 $\ell$ 和维度都非常大时，其运行时间对维度大小的依赖性很高，因此，研究目标是设计出更高效的近似算法，以显著减少运行时间，同时保持较低的近似比。

Method: 本文的核心方法是随机降维，并提出了一个通用方法来减少假设的 $\ell$-中心问题算法在高维欧几里得空间中运行时间对维度大小的依赖性。具体地，利用该方法，作者为 $\ell$-中心聚类和最小直径 $\ell$-聚类问题提供了 $(2+\epsilon)$-近似算法，它们在 $\ell$ 和维度都超对数时比已知的 $2$-近似算法快得多。此外，作者将这种通用方法应用于高维欧几里得空间中 $\ell$-中心聚类问题的现有快速近似算法，以及包含 $z$ 个异常点的 $\ell$-中心聚类问题的 $O(1)$-近似方法，从而实现了加速。

Result: 本文提供了在高维欧几里得和汉明空间中 $\ell$-中心聚类和最小直径 $\ell$-聚类问题的 $(2+\epsilon)$-近似算法，在 $\ell$ 和维度都超对数时，这些算法比已知的 $2$-近似算法显著加快。其次，作者成功将通用方法应用于高维欧几里得空间中 $\ell$-中心聚类问题的现有快速近似算法，进一步提高了速度。最后，作者还实现了对包含 $z$ 个异常点的 $\ell$-中心聚类问题的已知 $O(1)$-近似方法的加速。这些结果是通过基于随机降维的一般方法实现的，该方法有效地减少了算法对维度的依赖性。

Conclusion: 本文提出了一种利用随机降维技术来加速高维欧几里得和汉明空间中 $\ell$-中心聚类和最小直径 $\ell$-聚类问题的高效近似算法。通过改进现有算法的运行时间对维度大小的依赖性，并在高维 $\ell$-中心聚类及其包含异常点的泛化问题上实现了显著的加速和较低的近似比。

Abstract: We study the design of efficient approximation algorithms for the
  $\ell$-center clustering and minimum-diameter $\ell$-clustering
  problems in high dimensional Euclidean and Hamming spaces. Our main
  tool is randomized dimension reduction. First, we present a general
  method of reducing the dependency of the running time of a
  hypothetical algorithm for the $\ell$-center problem in a high
  dimensional Euclidean space on the dimension size. Utilizing in
  part this method, we provide $(2+ε)$- approximation
  algorithms for the $\ell$-center clustering and minimum-diameter
  $\ell$-clustering problems in Euclidean and Hamming spaces that are
  substantially faster than the known $2$-approximation ones when both
  $\ell$ and the dimension are super-logarithmic. Next, we apply the
  general method to the recent fast approximation algorithms with
  higher approximation guarantees for the $\ell$-center clustering
  problem in a high dimensional Euclidean space. Finally, we provide a
  speed-up of the known $O(1)$-approximation method for the
  generalization of the $\ell$-center clustering problem to include
  $z$ outliers (i.e., $z$ input points can be ignored while computing
  the maximum distance of an input point to a center) in high
  dimensional Euclidean and Hamming spaces.

</details>


### [5] [Matrix Editing Meets Fair Clustering: Parameterized Algorithms and Complexity](https://arxiv.org/abs/2512.03718)
*Robert Ganian,Hung P. Hoang,Simon Wietheger*

Main category: cs.DS

TL;DR: 与DSL、图处理、MLIR、编译器或HLS无关。

本文研究计算公平均值聚类离散向量的计算问题，该问题等价于通过最多改变$k$个值将彩色矩阵编辑成一个具有少量不同色平衡行的矩阵。我们证明了即使对于高度受限的公平均值聚类问题实例，也不存在类似于非公平设置中的固定参数算法（FPT）。随后，我们建立了该问题的完整复杂性图景，并通过对问题实例施加额外约束、固定参数近似，或使用针对树状矩阵的替代参数化，获得了可处理性结果，从而规避了所建立的下限。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于研究计算公平均值聚类离散向量的计算问题。这个问题在公平和非公平设置下都是 NP 困难的。尽管在非公平设置中存在已知的固定参数算法，但在公平设置中是否存在类似的算法以及其计算复杂性仍然是一个待解决的问题。此外，公平性在聚类问题中越来越重要，因此有必要理解如何在计算上有效地实现公平聚类。

Method: 本文首先通过排除适用于高度受限的公平均值聚类实例的固定参数算法，建立了其计算复杂性的下限。然后，通过探索三种方法来规避此下限并获得可处理性结果：1. 对问题实例施加额外的约束；2. 采用固定参数近似；3. 使用针对树状矩阵的替代参数化。

Result: 本文的主要结果包括：1. 证明了即使对于高度受限的公平均值聚类实例，也不存在类似于普通设置中的固定参数算法，从而建立了其计算复杂性的下限；2. 建立了一个完整的复杂性图景；3. 获得了可处理性结果，这些结果通过对问题实例施加额外约束、固定参数近似或使用针对树状矩阵的替代参数化成功避开了所获得的下限。

Conclusion: 本文研究了公平均值聚类问题，该问题等价于编辑一个彩色矩阵，使其具有少量不同的色平衡行。我们证明了即使是高度受限的公平均值聚类实例，也不存在类似于普通设置中的固定参数算法。我们通过对问题实例施加额外的约束、固定参数近似或使用针对类树矩阵的替代参数化，得到了可避免此下限的可处理性结果。这为理解公平均值聚类问题的计算复杂性及其可处理性边界奠定了基础。

Abstract: We study the computational problem of computing a fair means clustering of discrete vectors, which admits an equivalent formulation as editing a colored matrix into one with few distinct color-balanced rows by changing at most $k$ values. While NP-hard in both the fairness-oblivious and the fair settings, the problem is well-known to admit a fixed-parameter algorithm in the former ``vanilla'' setting. As our first contribution, we exclude an analogous algorithm even for highly restricted fair means clustering instances. We then proceed to obtain a full complexity landscape of the problem, and establish tractability results which capture three means of circumventing our obtained lower bound: placing additional constraints on the problem instances, fixed-parameter approximation, or using an alternative parameterization targeting tree-like matrices.

</details>


### [6] [Robust Algorithms for Path and Cycle Problems in Geometric Intersection Graphs](https://arxiv.org/abs/2512.03843)
*Malory Marin,Jean-Florent Raymond,Rémi Watrigant*

Main category: cs.DS

TL;DR: 本论文与**图处理**相关，因为它研究了在几何交图（Geometric Intersection Graphs）上的经典连接性问题，如哈密顿路径/环和长路径。
**TLDR:** 本文提出了一个新工具“$\lambda$-连接划分”和一个低树宽定理，实现了在$\mathbb{R}^d$中相似大小胖物体交图上，针对**哈密顿路径/环**问题的首个**鲁棒**且**ETH紧**的亚指数时间算法$2^{O(n^{1-1/d})}$，解决了该领域的一个开放问题。同时，本文还首次设计了针对**长路径**问题的**鲁棒**亚指数级参数化算法$2^{O(k^{1-1/d}\log^2 k)} \cdot n^{O(1)}$。


<details>
  <summary>Details</summary>
Motivation: 现有的在几何交图上求解经典连接性问题（如哈密顿路径/环、长路径）的算法，即使是亚指数级的，往往需要图的几何表示（即“非鲁棒”），而且这些问题的精确指数时间算法缺乏ETH紧界。特别是，解决de Berg et al. [STOC 2018]提出的ETH紧的精确算法的开放问题，以及为长路径问题在固定维度上设计鲁棒的亚指数参数化算法是主要的研究动机。作者希望开发出一种**鲁棒**的、**ETH紧**的算法框架，它不依赖于图的几何信息。

Method: 论文设计并应用了一个新工具，即“$\lambda$-linked partition”（$\lambda$连接划分），这是一个将顶点集划分为高度连接组的划分，可在多项式时间内计算，且不依赖于图的几何表示。此外，论文还建立了一个几何交图的低树宽模式覆盖定理，该定理是对Marx-Pilipczuk [ESA 2017]结果的改进。基于这些工具，论文设计了以下算法：
1. **哈密顿路径/环**：通过结合$\lambda$-linked partition，设计了鲁棒的ETH紧的亚指数时间算法$2^{O(n^{1-1/d})}$。
2. **长路径**：通过结合$\lambda$-linked partition和低树宽模式覆盖定理，设计了鲁棒的随机参数化算法$2^{O(k^{1-1/d}\log^2 k)}\, n^{O(1)}$。

Result: 1. **精确算法**：获得了首个针对$\mathbb{R}^d$中相似大小胖物体交图上**哈密顿路径**和**哈密顿环**的**鲁棒**且**ETH紧**的算法，时间复杂度为$2^{O(n^{1-1/d})}$。这解决了de Berg et al. [STOC 2018]的一个开放问题。
2. **参数化算法**：设计了第一个针对任意固定维度$d$的**长路径**问题的**鲁棒**亚指数级参数化算法，时间复杂度为$2^{O(k^{1-1/d}\log^2 k)}\, n^{O(1)}$，其中$k$是自然参数。
3. **理论工具**：引入了“$\lambda$-linked partition”这一新工具，并建立了几何交图的低树宽模式覆盖定理。

Conclusion: 这篇论文通过引入“$\lambda$-linked partition”和建立几何交图的低树宽模式覆盖理论，提出并实现了首个针对$\mathbb{R}^d$中相似大小胖物体交图上**哈密顿路径/环**问题的**鲁棒**指数时间算法，时间复杂度为$2^{O(n^{1-1/d})}$，是ETH紧的。同时，论文还为**长路径**问题设计了第一个鲁棒亚指数级参数化算法，时间复杂度为$2^{O(k^{1-1/d}\log^2 k)} \cdot n^{O(1)}$。这些成果解决了该领域的一个开放问题，并推动了几何图上连接性问题的精确和参数化算法研究。

Abstract: We study the design of robust subexponential algorithms for classical connectivity problems on intersection graphs of similarly sized fat objects in $\mathbb{R}^d$. In this setting, each vertex corresponds to a geometric object, and two vertices are adjacent if and only if their objects intersect. We introduce a new tool for designing such algorithms, which we call a $λ$-linked partition. This is a partition of the vertex set into groups of highly connected vertices. Crucially, such a partition can be computed in polynomial time and does not require access to the geometric representation of the graph. We apply this framework to problems related to paths and cycles in graphs. First, we obtain the first robust ETH-tight algorithms for Hamiltonian Path and Hamiltonian Cycle, running in time $2^{O(n^{1-1/d})}$ on intersection graphs of similarly sized fat objects in $\mathbb{R}^d$. This resolves an open problem of de Berg et al. [STOC 2018] and completes the study of these problems on geometric intersection graphs from the viewpoint of ETH-tight exact algorithms. We further extend our approach to the parameterized setting and design the first robust subexponential parameterized algorithm for Long Path in any fixed dimension $d$. More precisely, we obtain a randomized robust algorithm running in time $2^{O(k^{1-1/d}\log^2 k)}\, n^{O(1)}$ on intersection graphs of similarly sized fat objects in $\mathbb{R}^d$, where $k$ is the natural parameter. Besides $λ$-linked partitions, our algorithm also relies on a low-treewidth pattern covering theorem that we establish for geometric intersection graphs, which may be viewed as a refinement of a result of Marx-Pilipczuk [ESA 2017]. This structural result may be of independent interest.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing](https://arxiv.org/abs/2512.03608)
*Lishuo Deng,Shaojie Xu,Jinwu Chen,Changwei Yan,Jiajie Wang,Zhe Jiang,Weiwei Shan*

Main category: cs.AR

TL;DR: 本论文与**编译器**、**HLS**、**图处理**、**MLIR**无关，而与**DSL**无关，但与**LLMs**的**硬件部署/架构**密切相关，特别是**in-flash computing (IFC)**在**边缘计算**上的应用，属于**系统架构/硬件加速**领域。
**太长不看（TLDR）**：本文提出了KVNAND，首个DRAM-free、基于计算型闪存（IFC）的LLM部署架构，它将模型权重和庞大的KV缓存都存储在3D NAND闪存中，解决了边缘设备上的权重加载和长上下文DRAM容量瓶颈。通过利用IFC、头组并行和页级映射等技术，KVNAND将闪存转变为实用的长上下文KV存储介质，在不同上下文长度下比配备DRAM的IFC设计获得近2倍的速度提升，并能支持超长上下文（100K）。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署大型语言模型（LLMs）面临两大挑战：1. **权重加载和带宽压力**：LLMs参数量巨大（数百亿），单批次自回归推理算术强度极低，导致资源受限平台上严重的权重加载和带宽瓶颈。2. **KV缓存容量和成本**：现有的in-flash computing (IFC) 解决方案将权重计算集成在闪存中，但仍依赖DRAM存储Key-Value (KV) 缓存。随着上下文长度的增长，KV缓存大小可能超过模型权重，造成高昂的DRAM成本和容量限制。将KV缓存卸载到闪存中又会导致严重的性能损失。本文的动机是解决这些瓶颈，实现完全在闪存中存储权重和KV缓存的DRAM-free架构。

Method: KVNAND通过以下方法实现了突破：1. **DRAM-free架构**：将模型权重和KV缓存完全存储在计算型3D NAND闪存（IFC-based）中。2. **克服闪存性能挑战**：利用IFC（in-flash computing）加速所有内存密集型操作，减少数据传输开销。3. **并行机制**：引入“头组并行”（head-group parallelism）来提高吞吐量。4. **数据映射策略**：采用“页级KV缓存映射”（page-level KV cache mapping）以将令牌访问模式与闪存组织结构对齐。5. **设计空间探索**：提出了一个设计空间探索框架来评估离散紧凑的KVNAND变体，以平衡权重和KV放置，自动识别最佳设计权衡。

Result: KVNAND在MHA 7B和GQA 70B LLMs上的评估显示：1. **性能提升**：在128/1K/10K上下文长度下，KVNAND相对于配备DRAM的IFC设计实现了1.98倍/1.94倍/2.05倍的几何平均加速。2. **上下文支持能力**：解决了在100K上下文长度下的内存不足（out-of-memory）故障。这些结果验证了KVNAND作为长上下文KV存储实用介质的有效性。

Conclusion: KVNAND是首个基于IFC的无DRAM架构，通过将模型权重和KV缓存完整存储在计算型3D NAND闪存中，解决了边缘设备部署大型语言模型（LLMs）时面临的权重加载和KV缓存容量/能耗瓶颈。通过IFC的内存密集型操作加速、分组并行机制和页级KV缓存映射策略，KVNAND成功地将闪存转变为一种实用的、适用于长上下文的KV存储介质。其评估结果证明了其在不同上下文长度下相对于配备DRAM的IFC设计的显著性能提升和更大的上下文支持能力。

Abstract: Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.
  We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\(\times\)/1.94\(\times\)/2.05\(\times\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.

</details>


### [8] [Lightweight Unified Sha-3/Shake Architecture with a Fault-Resilient State](https://arxiv.org/abs/2512.03616)
*Christian Ewert,Amrit Sharma Poudel,Mouadh Ayache,Andrija Neskovic,Rainer Buchty,Mladen Berekovic,Sebastian Berndt,Saleh Mulhem*

Main category: cs.AR

TL;DR: 相关性：该论文与密码学硬件实现（Cryptography Hardware Implementation）相关，特别关注轻量级实现和抗故障设计，属于计算机体系结构（Computer Architecture）领域。
TLDR: 后量子密码方案中的 Sha-3 和 Shake 哈希函数需要轻量级和高可靠性的实现。本文提出了一种统一的 Sha-3/Shake 哈希引擎，它使用字节级就地划分的 Keccak 状态机制，并集成了基于二维奇偶校验的多维交叉奇偶校验机制来保护 Keccak 状态。该设计不仅涵盖了所有标准哈希配置，而且在保持高故障检测率（三个状态故障 100% 检测）的同时，显著降低了面积开销（整体设计缩小 4.5 倍，ASIC/FPGA 验证），使其非常适用于资源受限的 PQC 应用。


<details>
  <summary>Details</summary>
Motivation: 密码学中的哈希函数，尤其是 Sha-3 和 Shake，是后量子密码（PQC）方案的关键组成部分，需要轻量级的实现。为了确保整个 PQC 系统的可靠性，抗故障设计是不可或缺的。因此，研究的动机是提出一种轻量级、且具有高可靠性的抗故障统一哈希引擎设计，以满足资源受限的 PQC 应用的需求。

Method: 该文提出了一种支持 Sha-3 和 Shake 的统一哈希引擎设计，采用一种字节级就地划分（byte-wise in-place partitioning）的 Keccak 状态机制。同时，为了实现抗故障，该设计利用 Keccak 状态的立方体结构，引入了二维奇偶校验（two-dimensional parity checks）的多维交叉奇偶校验机制来保护 Keccak 状态，实现了对密钥状态的高故障检测和低面积开销。

Result: 所提出的统一哈希引擎涵盖了所有标准哈希配置，并集成了抗故障机制。与现有技术相比，该设计在寄存器级故障检测方面具有竞争力，实现了对三个 Keccak 状态故障的 100% 检测和对更高数量故障的近乎 100% 检测。同时，多维交叉奇偶校验机制使面积开销提高了 3.7 倍，整体抗故障引擎设计缩小了 4.5 倍（通过 ASIC 和 FPGA 实现证明）。集成到 RISC-V 环境中时，带有集成抗故障机制的统一哈希引擎引入的面积开销不到 8%。这表明该方法为资源受限的 PQC 应用提供了一种鲁棒且轻量级的抗故障保护方案。

Conclusion: 该论文提出了一种新的抗故障统一哈希引擎设计，集成了多维交叉奇偶校验机制。实验结果表明，该设计在保证高故障检测率的前提下，显著减小了面积开销，尤其适用于资源受限的后量子密码应用。

Abstract: Hash functions have become a key part of standard Post-quantum cryptography (PQC) schemes, especially Sha-3 and Shake, calling arXiv:submit/7045552 [cs.AR] 3 Dec 2025 for lightweight implementation. A fault-resilient design is always desirable to make the whole PQC system reliable. We, therefore, propose a) a unified hash engine supporting Sha-3 and Shake that follows a byte-wise in-place partitioning mechanism of the so-called Keccak state, and b) an according fault detection for Keccak state protection exploiting its cube structure by deploying two-dimensional parity checks. It outperforms the state-of-the-art (SoA) regarding area requirements at competitive register-level fault detection by achieving 100% detection of three and still near 100% of higher numbers of Keccak state faults. Unlike SoA solutions, the proposed unified hash engine covers all standard hash configurations. Moreover, the introduced multidimensional cross-parity check mechanism achieves a 3.7x improvement in area overhead, with an overall 4.5x smaller fault-resilient engine design as demonstrated in ASIC and FPGA implementations. Integrated into a RISC-V environment, the unified hash engine with the integrated fault-resilient mechanism introduced less than 8% area overhead. Our approach thus provides a robust and lightweight fault-detection solution for protecting hash functions deployed in resource-constrained PQC applications.

</details>


### [9] [The BrainScaleS-2 multi-chip system: Interconnecting continuous-time neuromorphic compute substrates](https://arxiv.org/abs/2512.03781)
*Joscha Ilmberger,Johannes Schemmel*

Main category: cs.AR

TL;DR: 该论文涉及HLS和FPGA。本文描述了BrainScaleS-2系统通过FPGA互连和额外的Aggregator单元实现计算基板的规模扩展。系统最终集成在一个标准的19英寸机架中，实现了芯片间低至1.3微秒的延迟性能。


<details>
  <summary>Details</summary>
Motivation: 集成BrainScaleS-2模拟神经元和突触电路芯片所需的数字外围电路，并通过扩展互连基板，将单个ASIC系统扩展成更大的计算平台，以实现复杂的神经形态实验。

Method: 通过额外的Aggregator单元和FPGA互连方案，实现了计算基板的规模扩展。Aggregator提供多达12个收发器链路连接到Node-FPGA背板，并提供4个用于进一步扩展的收发器通道。最终系统将两个互连的背板集成到一个标准的19英寸机架箱中。

Result: 在所有脉冲速率下，每个背板内都实现了芯片到芯片的延迟低于1.3微秒（其中包含跨越三个FPGA的四跳延迟）。系统被集成在一个标准的19英寸4U机架箱中。

Conclusion: 本文介绍了BrainScaleS-2系统通过额外的Aggregator单元和FPGA互连，实现了计算基板的规模扩展。最终系统集成在一个标准的19英寸4U机架箱中，包含了两个互连的背板以及以太网交换机、系统控制器和电源。系统在所有脉冲速率下，实现了芯片到芯片的延迟低于1.3微秒的性能。

Abstract: The BrainScaleS-2 SoC integrates analog neuron and synapse circuits with digital periphery, including two CPUs with SIMD extensions. Each ASIC is connected to a Node-FPGA, providing experiment control and Ethernet connectivity. This work details the scaling of the compute substrate through FPGA-based interconnection via an additional Aggregator unit. The Aggregator provides up to 12 transceiver links to a backplane of Node-FPGAs, as well as 4 transceiver lanes for further extension. Two such interconnected backplanes are integrated into a standard 19in rack case with 4U height together with an Ethernet switch, system controller and power supplies. For all spike rates, chip-to-chip latencies -- consisting of four hops across three FPGAs -- below 1.3$μ$s are achieved within each backplane.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [10] [Tuning of Vectorization Parameters for Molecular Dynamics Simulations in AutoPas](https://arxiv.org/abs/2512.03565)
*Luis Gall,Samuel James Newcome,Fabio Alexander Gratl,Markus Mühlhäußer,Manish Kumar Mishra,Hans-Joachim Bungartz*

Main category: cs.DC

TL;DR: 该论文与编译器、DSL、图处理、MLIR、HLS等相关，因为它关注的是通过SIMD向量化技术来优化计算性能，这通常是编译器优化的一部分，也与高性能计算和并行计算相关联。/本文研究了在分子动力学（MD）模拟库AutoPas中，利用SIMD向量化技术优化分子间对偶力计算。重点在于探索不同的粒子值加载顺序，并结合模拟特定的参数（如粒子密度、邻居识别算法）来区分最优的向量化顺序。通过将这种优化整合到AutoPas的动态调优机制中，实验结果显示，与AutoPas旧方法相比，运行时考虑不同的粒子交互顺序可显著提高力计算的性能。


<details>
  <summary>Details</summary>
Motivation: 分子动力学（MD）模拟中的对偶力计算是计算密集型的，作者旨在通过应用SIMD向量化技术来优化AutoPas粒子模拟库中的这一计算过程，尤其关注粒子值加载到向量寄存器的顺序对性能和能耗的影响。此外，由于最优的MD算法可能在运行时（run-time）变化，因此需要研究模拟特有的参数（如粒子密度和邻居识别算法）的影响，并实现动态调优。

Method: 本文通过研究不同的SIMD向量化加载顺序，并结合模拟特有的参数（如粒子密度、邻居识别算法）来区分最佳的交互次序，并将其集成到AutoPas的动态调优机制中，以便在运行时选择最优的向量化顺序。

Result: 基准测试表明，与AutoPas先前的方法相比，在运行时考虑不同的粒子交互顺序，可以显著提高（considerable performance improvement）力计算的性能。

Conclusion: 本文通过扩展AutoPas的动态调优机制，使其能够在运行时选择最优的SIMD向量化顺序，从而根据模拟的具体参数（如粒子密度、邻居识别算法）来优化分子动力学（MD）模拟中的对偶力计算性能，从而实现显著的性能提升。

Abstract: Molecular Dynamics simulations can help scientists to gather valuable insights for physical processes on an atomic scale. This work explores various techniques for SIMD vectorization to improve the pairwise force calculation between molecules in the scope of the particle simulation library AutoPas. The focus lies on the order in which particle values are loaded into vector registers to achieve the most optimal performance regarding execution time or energy consumption.
  As previous work indicates that the optimal MD algorithm can change during runtime, this paper investigates simulation-specific parameters like particle density and the impact of the neighbor identification algorithms, which distinguishes this work from related projects. Furthermore, AutoPas' dynamic tuning mechanism is extended to choose the optimal vectorization order during runtime.
  The benchmarks show that considering different particle interaction orders during runtime can lead to a considerable performance improvement for the force calculation compared to AutoPas' previous approach.

</details>


### [11] [On the Challenges of Energy-Efficiency Analysis in HPC Systems: Evaluating Synthetic Benchmarks and Gromacs](https://arxiv.org/abs/2512.03697)
*Rafael Ravedutti Lucio Machado,Jan Eitzinger,Georg Hager,Gerhard Wellein*

Main category: cs.DC

TL;DR: 涉及领域：本文不直接涉及 DSL、图处理、MLIR、编译器或 HLS。它主要关注高性能计算 (HPC) 和能效分析。
太长不看：本文分析了在 Fritz 和 Alex HPC 集群上，针对 Intel Ice Lake/Sapphire Rapids CPU 和 Nvidia A40/A100 GPU 运行合成基准测试和 Gromacs 包时，利用 MPI 进行能效分析的挑战和陷阱。文章展示了使用 Likwid 和 Nvidia 分析工具获得的指标和结果，并提出了未来能效分析研究的最佳实践。


<details>
  <summary>Details</summary>
Motivation: 对高性能计算 (HPC) 集群上运行的合成基准测试和实际应用（如 Gromacs）进行能效分析，并揭示在异构硬件和并行计算环境下进行准确能效测量和分析的挑战和陷阱，最终提出未来的最佳实践。

Method: 在 Fritz 和 Alex HPC 集群上，使用 MPI 并行，在 Intel Ice Lake、Sapphire Rapids CPU 以及 Nvidia A40 和 A100 GPU 的全插槽上进行实验。使用 Likwid 和 Nvidia 性能分析工具收集能效指标和测量值。通过实验揭示并讨论了在测量和分析过程中遇到的挑战和陷阱。

Result: 在不同硬件（Intel Ice Lake/Sapphire Rapids CPU，Nvidia A40/A100 GPU）上运行合成基准测试和 Gromacs，并获得了使用 Likwid 和 Nvidia 分析工具测量的能效指标和结果。实验结果揭示了在异构 HPC 环境中进行能效分析时存在的具体挑战和陷阱。提出了一系列最佳实践。

Conclusion: 本文分析了在 Fritz 和 Alex HPC 集群上对不同硬件（Intel Ice Lake 和 Sapphire Rapids CPU，Nvidia A40 和 A100 GPU）运行合成基准测试和 Gromacs 包时，进行能效分析所遇到的挑战和陷阱，并提出了未来能效分析研究的最佳实践。主要的挑战在于准确的度量和分析在混合硬件和软件并行（MPI）环境下的能耗和性能数据。

Abstract: This paper discusses the challenges encountered when analyzing the energy efficiency of synthetic benchmarks and the Gromacs package on the Fritz and Alex HPC clusters. Experiments were conducted using MPI parallelism on full sockets of Intel Ice Lake and Sapphire Rapids CPUs, as well as Nvidia A40 and A100 GPUs. The metrics and measurements obtained with the Likwid and Nvidia profiling tools are presented, along with the results. The challenges and pitfalls encountered during experimentation and analysis are revealed and discussed. Best practices for future energy efficiency analysis studies are suggested.

</details>


### [12] [OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference](https://arxiv.org/abs/2512.03927)
*Liujianfu Wang,Yuyang Du,Yuchen Pan,Soung Chang Liew,Jiacheng Liu,Kexin Chen*

Main category: cs.DC

TL;DR: 该论文与编译器、DSL、图处理和MLIR等方面没有直接关联，它涉及**LLM**及其在**边缘计算**设备上的部署和优化，属于**系统优化**和**分布式推理框架**的范畴。太长不看：MoE模型难以部署在内存受限的边缘设备上。本文提出了OD-MoE，一个分布式MoE推理框架，通过两个主要机制（并行化加载/计算和超精确的超前专家预测器）实现了完全按需专家加载，从而消除了专家缓存的需求。实验证明，OD-MoE在大幅减少GPU内存占用的同时（仅需1/3内存），能保持接近完全缓存部署（75%）的解码速度，并且首次支持在GPU内存小于1GB的边缘设备上进行MoE推理。


<details>
  <summary>Details</summary>
Motivation: MoE架构在LLM中具有优势，但在部署到内存受限的低成本边缘设备时面临挑战。现有的专家卸载方法（将专家参数存储在CPU内存中并缓存热门专家到GPU内存）虽然有所改善，但用于专家缓存的GPU内存利用率低于密集型LLM，且仍占用宝贵的GPU内存。消除专家缓存的需求，以在内存限制更严格的边缘设备上实现高效的MoE推理是该研究的动机。

Method: OD-MoE是一个分布式MoE推理框架，通过完全按需加载专家来消除对专家缓存的需求。它建立在两个关键机制之上：1) 在分布式边缘节点上并行化专家加载和专家计算；2) 一个超精确的模拟预测器，可以在专家计算进行时提前预测多层的专家激活。通过这些机制，OD-MoE能够在激活之前及时地将目标专家动态加载到其中一个分布式节点，并在之后迅速将其驱逐，释放GPU内存供后续专家使用。

Result: 实验结果表明：1) OD-MoE实现了99.94%的专家激活预测准确率，远超所有现有方法；2) OD-MoE在仅使用1/3 GPU内存的情况下，提供了与完全GPU缓存的MoE部署约75%的解码速度；3) OD-MoE通过消除专家缓存的需求，使得在GPU内存不足1GB的边缘节点上进行MoE推理成为可能。

Conclusion: OD-MoE通过消除专家缓存的需求，首次在GPU内存不足1GB的边缘设备上实现了MoE推理，从而为在LLM时代的低成本物联网设备上部署MoE模型提供了可行的途径。

Abstract: Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era.

</details>
