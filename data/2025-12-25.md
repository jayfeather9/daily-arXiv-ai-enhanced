<div id=toc></div>

# Table of Contents

- [cs.DS](#cs.DS) [Total: 4]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [1] [In-Place BWT and Lyndon Array Construction in Constant Space](https://arxiv.org/abs/2512.20869)
*Felipe A. Louza,Arnaud Lefebvre*

Main category: cs.DS

TL;DR: 该论文与以下主题相关：无。本文没有涉及DSL、图处理、MLIR、编译器或HLS等主题。

太长不读：本文提出了一个基于Crochemore等人原位BWT算法的扩展，实现了使用$O(1)$额外空间构建Lyndon数组的方法。它通过在从右向左的BWT构建中增量维护后缀排名，并使用简单的“下一个较小值”过程导出Lyndon数组。虽然时间复杂度为$O(n^2)$且不适合实际应用，但它在概念上简洁，并适用于无界字母表。


<details>
  <summary>Details</summary>
Motivation: 旨在提出一种能够在$O(1)$额外空间内构建Lyndon数组的方法，扩展现有的原位BWT算法（如Crochemore等人的算法），以在空间受限的环境下实现Lyndon数组的简洁构建。

Method: 扩展了Crochemore等人的原位BWT算法，在从右向左构建BWT的过程中增量维护后缀的字典序排名，并通过一个简单的“下一个较小值”（next-smaller-value）过程来导出Lyndon数组。

Result: 实现了$O(1)$额外空间复杂度的Lyndon数组构建。该方法具有概念上的简洁性，并可用于无界字母表，但代价是时间复杂度为二次方（$O(n^2)$），因此不适用于实际的高效应用。

Conclusion: 本文提出了一种将后缀的字典序排名增量维护并在从右向左构建BWT的过程中直接导出Lyndon数组的方法。这种方法实现了$O(1)$额外空间复杂度的Lyndon数组构建，但时间复杂度为二次方，主要意义在于概念的简洁性，不适合实际应用，但适用于无界字母表。

Abstract: We present an extension of the in-place BWT algorithm of Crochemore et al. [8] that enables the construction of the Lyndon array using O(1) extra space. Our approach incrementally maintains the lexicographic ranks of the suffixes during the right-to-left BWT construction and then derives the Lyndon array through a simple next-smaller-value procedure. Although not intended for practical use due to its quadratic running time, the method is conceptually simple and works for unbounded alphabets.

</details>


### [2] [Fairness in the k-Server Problem](https://arxiv.org/abs/2512.20960)
*Mohammadreza Daneshvaramoli,Helia Karisani,Mohammad Hajiesmaili,Shahin Kamali,Cameron Musco*

Main category: cs.DS

TL;DR: 该论文与图处理（$k$-server 问题，涉及度量空间上的服务器移动）相关。/
本文正式研究了 $k$-server 问题的公平性，提出了 $(\alpha, \beta)$-公平性概念，旨在公平分配服务器的移动成本。作者证明了在离线和针对无知对手的在线设置中，可以在不牺牲竞争性的情况下实现公平性。离线算法能够将最优解转化为公平解，成本增加很小。对于在线设置下的完全自适应对手，经典双覆盖算法（DCA）在直线度量和 $k=2$ 的树形度量上是公平的，但在一般树形度量上则不然。研究表明，在完全自适应对手下的公平性问题仍然是一个重要的开放挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的 $k$-server 问题研究主要集中在最小化总移动成本，而忽略了如何公平 распределить 服务器之间的成本。本文的动机是正式研究 $k$-server 问题的公平性，目标是不仅最小化总成本，而且公平地分配服务器之间的成本，以解决成本分配不均的问题。

Method: 本文首先定义了 $k$-server 问题的 $(\alpha, \beta)$-公平性概念，其中 $\alpha \ge 1$ 和 $\beta \ge 0$。然后，作者分两个方面进行研究：1. 离线设置下，开发了一种确定性算法，可以将任何最优解转换为 $(\alpha, \beta)$-公平的解，且成本增加很小。2. 在线设置下，证明了任何竞争性算法都可以转换为一个随机在线算法，该算法对无知对手是公平的并且仍然具有竞争性。最后，对经典双覆盖算法（DCA）在不同度量空间下的公平性进行了分析，包括在线段度量和树形度量 ($k=2$) 上的公平性，以及在一般树形度量上的不公平性。

Result: 1. **公平性定义及实现：** 提出了 $(\alpha, \beta)$-公平性的通用概念，即任何服务器的成本不超过总成本的 $\alpha/k$ 部分加上一个加性项 $\beta$。2. **离线设置：** 提出了一个确定性算法，对于任意 $\varepsilon > 0$，可以将最优解转换为 $(\alpha, \beta)$-公平解，其中 $\alpha= 1 + \varepsilon$ 和 $\beta= O(\mathrm{diam} \cdot \log k / \varepsilon)$，总成本仅增加 $O(\mathrm{diam} \cdot k \log k / \varepsilon)$ 的加性项。3. **在线设置（无知对手）：** 任何竞争性算法都可以转换为一个随机在线算法，该算法对无知对手（oblivious adversary）具有高概率公平性，并且仅有很小的竞争性损失。4. **在线设置（完全自适应对手）：** 经典双覆盖算法（DCA）在直线度量（line metrics）和 $k=2$ 时的树形度量（tree metrics）上是公平的。然而，DCA 在一般树形度量上对于任何非空参数都不是公平的。5. **开放问题：** 在线设置下，面对完全自适应对手，是否存在确定性或随机的公平算法仍是一个重大的开放问题。

Conclusion: 本文启动了对 $k$-server 问题的公平性的正式研究，提出了一种新的 $(\alpha, \beta)$-公平性概念，并证明了在离线和在线设置中，公平性可以在不损失竞争性的情况下实现。特别地，在线设置中，面对完全自适应的对手，公平性问题仍有待解决，但作者对特定算法在特定图结构上的公平性进行了初步探索，并指出了其局限性。

Abstract: We initiate a formal study of fairness for the $k$-server problem, where the objective is not only to minimize the total movement cost, but also to distribute the cost equitably among servers. We first define a general notion of $(α,β)$-fairness, where, for parameters $α\ge 1$ and $β\ge 0$, no server incurs more than an $α/k$-fraction of the total cost plus an additive term $β$. We then show that fairness can be achieved without a loss in competitiveness in both the offline and online settings. In the offline setting, we give a deterministic algorithm that, for any $\varepsilon > 0$, transforms any optimal solution into an $(α,β)$-fair solution for $α= 1 + \varepsilon$ and $β= O(\mathrm{diam} \cdot \log k / \varepsilon)$, while increasing the cost of the solution by just an additive $O(\mathrm{diam} \cdot k \log k / \varepsilon)$ term. Here $\mathrm{diam}$ is the diameter of the underlying metric space. We give a similar result in the online setting, showing that any competitive algorithm can be transformed into a randomized online algorithm that is fair with high probability against an oblivious adversary and still competitive up to a small loss.
  The above results leave open a significant question: can fairness be achieved in the online setting, either with a deterministic algorithm or a randomized algorithm, against a fully adaptive adversary? We make progress towards answering this question, showing that the classic deterministic Double Coverage Algorithm (DCA) is fair on line metrics and on tree metrics when $k = 2$. However, we also show a negative result: DCA fails to be fair for any non-vacuous parameters on general tree metrics.

</details>


### [3] [Time-Bucketed Balance Records: Bounded-Storage Ephemeral Tokens for Resource-Constrained Systems](https://arxiv.org/abs/2512.20962)
*Shaun Scovil,Bhargav Chickmagalur Nanjundappa*

Main category: cs.DS

TL;DR: This paper is related to graph processing or compiler or HLS: None of the above.
本文旨在解决具有“生存时间”（TTL）语义的同质化代币在区块链上实现时，因存储不受限制增长和拒绝服务攻击而面临的挑战。通过引入“时间分桶余额记录”数据结构，将时间离散化为 $k$ 个桶并合并存款，该方法能够在保持代币到期时间不早于配置 TTL 的同时，将每个账户的存储记录数量限制在 $O(k)$，并在 Solidity 参考实现中展示了良好的实用效率和 gas 成本效益。


<details>
  <summary>Details</summary>
Motivation: 实现具有“生存时间”（TTL）语义的同质化代币（Fungible tokens）需要为每个存入的代币单元跟踪单独的到期时间。这种朴素的实现方式会导致：1. 为每次存款创建一个新的余额记录，导致存储（Storage）无限制增长。2. 系统容易受到拒绝服务（DoS）攻击。为了解决这些问题，特别是维持对存储的限制并确保正确的到期语义，需要一种新的高效数据结构。

Method: 文章提出“时间分桶余额记录”（time-bucketed balance records）数据结构。具体方法是将时间离散化为 $k$ 个桶，将同一时间桶内的存款合并，以限制唯一到期时间戳的数量。通过这种方式，每个账户的存储记录被限制在 $O(k)$ 的范围内。作者在形式上证明了该方法的三个关键特性：存储受 $k+1$ 条记录限制、实际到期时间不早于配置的 TTL、以及攻击者无法将受害者的操作成本增加到超过 $O(k)$（摊销）的最坏情况。文章还提供了一个 Solidity 参考实现，并测量了其 gas 成本以证明实际效率。

Result: 1. **存储限制：** 证明了无论存款频率如何，存储记录都被限制在 $k+1$ 条。2. **TTL 保障：** 证明了实际的到期时间总是至少等于配置的 TTL（生存时间）。3. **操作成本保障：** 证明了攻击者无法将受害者的操作成本增加到超过 $O(k)$（摊销）的最坏情况。4. **实用性：** 提供了一个基于 Solidity 的参考实现，并通过测量的 gas 成本证明了其在实际应用中的效率。

Conclusion: 本文提出了一种称为“时间分桶余额记录”（time-bucketed balance records）的数据结构，可以有效地解决在实现具有 TTL 语义的同质化代币时，由于需要跟踪每个代币单元的到期时间而导致的存储不受限制增长和潜在的拒绝服务攻击问题。该方法通过将时间离散化为 $k$ 个桶，将同一时间桶内的存款合并，从而将每个账户的存储记录数量限制在 $O(k)$，并且保证代币的实际到期时间总是大于或等于配置的 TTL。论文提供了一个实用且在 gas 成本上高效的 Solidity 参考实现。

Abstract: Fungible tokens with time-to-live (TTL) semantics require tracking individual expiration times for each deposited unit. A naive implementation creates a new balance record per deposit, leading to unbounded storage growth and vulnerability to denial-of-service attacks. We present time-bucketed balance records, a data structure that bounds storage to O(k) records per account while guaranteeing that tokens never expire before their configured TTL. Our approach discretizes time into k buckets, coalescing deposits within the same bucket to limit unique expiration timestamps. We prove three key properties: (1) storage is bounded by k+1 records regardless of deposit frequency, (2) actual expiration time is always at least the configured TTL, and (3) adversaries cannot increase a victim's operation cost beyond O(k)[amortized] worst case. We provide a reference implementation in Solidity with measured gas costs demonstrating practical efficiency.

</details>


### [4] [An O($nlogn$) approximate knapsack algorithm](https://arxiv.org/abs/2512.21195)
*Nick Dawes*

Main category: cs.DS

TL;DR: 该论文与 DSL、图处理、MLIR、编译器或 HLS 无关。

太长不看：本文提出了一种改进的动态规划算法，用于快速准确地解决大型 0/1 背包问题。该算法具有 $O(n \log n)$ 的计算复杂度和空间复杂度，并且误差可预测。实验结果表明，该算法具有高精度和高效率，能够以极快的速度求解大规模问题。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在解决大型 0/1 背包问题时，可能在速度和精度方面存在不足。这促使作者开发一种新型的动态规划算法，以实现快速、准确的求解。

Method: 该方法是一种改进的动态规划算法，用于解决 0/1 背包问题。它具有 $O(n \log n)$ 的计算复杂度和 $O(n \log n)$ 的空间复杂度，并能预测最大误差。实验结果显示其精度随解的大小 $k$ 呈超线性增长。

Result: 该算法对于 $k=10^3$ 的问题，平均最大小数误差为 $10^{-4}$；对于 $k=10^5$ 的问题，平均最大小数误差为 $10^{-7}$，精度随 $k$ 超线性增长。在给定 $n$ 的情况下，运行时间为常量时间。在普通台式机上，处理 $n=10^3$ 的问题需 $10^{-3}$ 秒，处理 $n=10^6$ 的问题需 2 秒。

Conclusion: 本文提出了一种改进的动态规划算法，能够快速准确地解决大型 0/1 背包问题。该算法具有可预测的误差上界，并且在实际应用中表现出高精度和高效率。

Abstract: A modified dynamic programming algorithm rapidly and accurately solves large 0/1 knapsack problems. It has computational O($nlogn$), space O($nlogn$) and predictable maximum error. Experimentally it's accuracy increases faster than linearly with the solution size $k$. Problems with $k=10^3$ are solved with an average maximum fractional error of $10^{-4}$ and problems with $k=10^5$ with an average maximum fractional error of $10^{-7}$. The algorithm runs in constant time for all problems with a given $n$. On a common desktop computer the algorithm processes $n=10^3$ problems in $10^{-3}$ seconds and $n=10^6$ problems in 2 seconds.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [5] [NotSoTiny: A Large, Living Benchmark for RTL Code Generation](https://arxiv.org/abs/2512.20823)
*Razine Moundir Ghorab,Emanuele Parisi,Cristian Gutierrez,Miquel Alberti-Binimelis,Miquel Moreto,Dario Garcia-Gasulla,Gokcen Kestor*

Main category: cs.AR

TL;DR: 本文涉及编译器和硬件描述语言（RTL即寄存器传输级语言）。该论文介绍了NotSoTiny，一个新的基准，用于评估LLMs生成RTL代码的能力。该基准从Tiny Tapeout社区的真实硬件设计中构建，具有去重、验证和定期更新机制，以解决现有基准规模有限、设计简单、验证不足和数据污染等问题。评估结果显示NotSoTiny比现有基准更具挑战性，有助于推动LLMs在硬件设计中的应用和改进。


<details>
  <summary>Details</summary>
Motivation: 现有的RTL基准在规模上有限、偏向于简单的设计、验证严谨性不足，并且容易受到数据污染的影响。这些限制使得难以在真实的设置中评估大型语言模型（LLMs）生成RTL代码的能力。因此，本文的动机是引入一个更具挑战性、结构更丰富、更具情境感知的RTL生成基准，以克服这些限制并推动该领域的发展。

Method: 本文的主要方法是构建NotSoTiny基准。具体来说，NotSoTiny是从数百个由Tiny Tapeout社区产生的实际硬件设计中构建的，通过自动化流程去除了重复项，并验证了设计的正确性。为了减轻数据污染的问题，该基准与Tiny Tapeout的发布计划同步，定期纳入新的设计。文章还通过评估结果，证明了NotSoTiny的任务比现有基准更具挑战性。

Result: 本文引入了NotSoTiny基准，该基准用于评估大型语言模型在生成结构丰富和具有情境感知能力的RTL代码方面的能力。NotSoTiny是从数百个实际的Tiny Tapeout硬件设计中构建的，具有去重、正确性验证和定期更新以减轻数据污染的自动化流程。评估结果显示，NotSoTiny中的任务比先前的基准更具挑战性，证明了它在克服当前LLMs应用于硬件设计中的局限性方面的有效性。

Conclusion: 本文介绍了NotSoTiny基准的引入及其在评估LLM生成RTL代码能力方面的价值。作者认为NotSoTiny提供了比现有基准更具挑战性和真实性的评估任务，能够有效克服当前LLM在硬件设计应用中存在的局限性，并指导这项有前景的技术的改进。未来的工作将涉及利用NotSoTiny持续评估和改进LLM的RTL生成能力。

Abstract: LLMs have shown early promise in generating RTL code, yet evaluating their capabilities in realistic setups remains a challenge. So far, RTL benchmarks have been limited in scale, skewed toward trivial designs, offering minimal verification rigor, and remaining vulnerable to data contamination. To overcome these limitations and to push the field forward, this paper introduces NotSoTiny, a benchmark that assesses LLM on the generation of structurally rich and context-aware RTL. Built from hundreds of actual hardware designs produced by the Tiny Tapeout community, our automated pipeline removes duplicates, verifies correctness and periodically incorporates new designs to mitigate contamination, matching Tiny Tapeout release schedule. Evaluation results show that NotSoTiny tasks are more challenging than prior benchmarks, emphasizing its effectiveness in overcoming current limitations of LLMs applied to hardware design, and in guiding the improvement of such promising technology.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications](https://arxiv.org/abs/2512.20953)
*Yuxiao Wang,Yuedong Xu,Qingyang Duan,Yuxuan Liu,Lei Jiao,Yinghao Yu,Jun Wu*

Main category: cs.DC

TL;DR: 该论文与编译器、DSL、图处理、MLIR、HLS等领域不直接相关。
大型语言模型（LLMs）和新款GPU的增长推动了在异构GPU环境下进行分布式训练的需求。本文提出了AutoHet系统，旨在解决异构环境中实现3D并行化的挑战，如不对称的张量并行化和高效的梯度同步。AutoHet通过将设备分组和负载平衡建模为优化问题，自动确定最优的并行化方案，实现不对称3D并行结构和细粒度工作负载分配，以最小化每次迭代的训练时间。此外，AutoHet还提供了一种高效的故障恢复策略，以应对抢占式实例。实验结果表明，AutoHet在训练吞吐量上比现有系统（如Megatron-LM和Whale）快了高达1.79倍，在恢复速度上比抢占式实例基线快了4.38倍。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的快速发展，以及新款GPU产品的不断发布，对在异构GPU环境下进行分布式训练的需求显著增加。然而，在异构集群上实现高效的3D并行化训练面临对称张量并行化、高效的梯度同步、以及在不同性能GPU间平衡计算能力和内存利用率等关键挑战。作者的动机是开发一个系统（AutoHet）来克服这些挑战，自动优化异构GPU环境下的分布式训练性能。

Method: 本文首先全面分析了实现3D并行化在异构GPU环境中面临的挑战，包括对对称张量并行化、不对称流水线并行化中的梯度同步以及内存利用率与计算效率权衡的需求。在此基础上，提出了AutoHet系统，它通过将设备分组和负载平衡建模为优化问题来最小化每次迭代的训练时间，从而自动确定最优的并行化方案。AutoHet支持不对称的3D并行结构和细粒度的工作负载分配。此外，AutoHet还为弹性训练和实例抢占引入了高效的恢复策略，优先从本地节点获取训练状态，只有缺失的检查点才从云存储下载。

Result: AutoHet在三个大规模模型和三种不同GPU类型的组合上的广泛评估表明，它优于现有的DNN训练系统。与Megatron-LM和Whale相比，AutoHet在训练吞吐量上实现了高达1.79倍的加速。与抢占式实例基线相比，恢复速度实现了4.38倍的加速。

Conclusion: 本文分析了在异构GPU环境中实现分布式训练时3D并行化的挑战，并提出了AutoHet系统以自动化和优化这一过程。AutoHet通过不对称的3D并行结构和细粒度的工作负载分配，有效地平衡了不同性能GPU的计算能力和内存使用，显著提高了训练吞吐量和故障恢复速度。结论是AutoHet是解决异构集群下LLM分布式训练效率问题的有效工具。

Abstract: The rapid growth of large language models (LLMs) and the continuous release of new GPU products have significantly increased the demand for distributed training across heterogeneous GPU environments. In this paper, we present a comprehensive analysis of the challenges involved in implementing 3D parallelism in such environments, addressing critical issues such as the need for symmetric tensor parallelism, efficient gradient synchronization in asymmetric pipeline parallelism, and the trade-offs between memory utilization and computational efficiency. Building upon these insights, we introduce AutoHet, a novel system that automatically identifies the optimal parallelism plan for distributed training on heterogeneous GPUs. AutoHet supports asymmetric 3D parallelism structures and facilitates fine-grained workload distribution. We propose a theoretical model that frames the device grouping and load balancing as an optimization problem to minimize per-iteration training time, thus effectively balancing computing power and memory usage across GPUs with diverse capabilities. To enable elastic training upon spot instance preemption, AutoHet presents an efficient recovery strategy that prioritizes to retrieve training states from local nodes, and only downloads the missing checkpoints from the cloud storage. Our extensive evaluation, conducted on three large-scale models and utilizing combinations of three different GPU types, demonstrates that AutoHet outperforms existing DNN training systems, achieving up to a 1.79$\times$ speedup in training throughput compared with Megatron-LM and Whale, and a 4.38$\times$ speedup of recovery speed compared to a spot instance baseline.

</details>


### [7] [Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions](https://arxiv.org/abs/2512.20967)
*Linggao Kong,Yuedong Xu,Lei Jiao,Chuan Xu*

Main category: cs.DC

TL;DR: 该论文与编译器、HLS、MLIR、DSL或图处理不直接相关，但它涉及到成本优化和调度问题，这在涉及大规模机器学习模型（例如模型微调）时是重要的操作方面。太长不看：随着基础模型的规模增大，微调成本也随之增加。本文提出了一种在线调度框架，通过混合使用易失的现货（Spot）实例和稳定的按需（On-demand）实例来解决成本和截止期限的挑战。该框架利用现货市场价格和可用性的可预测性，提出了一个结合预测的整数规划模型和一套在线分配及策略选择算法。实验证明，该方法能自适应选择最佳策略，持续优于基线，并将效用提升高达54.8%。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的规模不断增长，微调成本越来越高。虽然GPU现货实例提供了低成本选择，但其不稳定的价格和可用性使得有截止期限的调度挑战性大。该研究旨在通过混合使用现货和按需实例，利用现货市场价格和可用性的可预测性，实现成本效益高的调度，同时管理预测误差的敏感性。

Method: 作者首先提出了一个整数规划问题，用于在考虑价格和可用性动态的情况下，对混合使用现货实例和按需实例进行建模。接着，提出了一种基于承诺视界控制方法的在线分配算法，该算法依赖于预测并引入了“承诺水平”来约束部分决策序列。针对预测不准确的情况，提出了一种不依赖预测的互补在线算法。最后，开发了一种在线策略选择算法，该算法通过学习池中最佳策略来不断选择合适的算法，并证明了其具有$\mathcal{O}(\sqrt{T})$的后悔界。

Result: 实验结果表明，所提出的在线框架可以根据不同的现货市场动态和预测质量自适应地选择最佳策略。该框架持续优于基线方法，并将效用提高了高达54.8%。理论分析证明了基于预测的算法在预测误差减小时能实现更紧密的性能界限，而策略选择算法具有$\mathcal{O}(\sqrt{T})$的后悔界。

Conclusion: 本文提出了一种在线框架，能够根据不断变化的市场动态和预测质量自适应地选择最佳调度策略，持续优于基线方法，并带来了高达54.8%的效用提升。这表明混合使用实例、利用市场波动预测和在线策略选择可以有效降低大模型微调的成本。

Abstract: As foundation models grow in size, fine-tuning them becomes increasingly expensive. While GPU spot instances offer a low-cost alternative to on-demand resources, their volatile prices and availability make deadline-aware scheduling particularly challenging. We tackle this difficulty by using a mix of spot and on-demand instances. Distinctively, we show the predictability of prices and availability in a spot instance market, the power of prediction in enabling cost-efficient scheduling and its sensitivity to estimation errors. An integer programming problem is formulated to capture the use of mixed instances under both the price and availability dynamics. We propose an online allocation algorithm with prediction based on the committed horizon control approach that leverages a \emph{commitment level} to enforce the partial sequence of decisions. When this prediction becomes inaccurate, we further present a complementary online algorithm without predictions. An online policy selection algorithm is developed that learns the best policy from a pool constructed by varying the parameters of both algorithms. We prove that the prediction-based algorithm achieves tighter performance bounds as prediction error decreases, while the policy selection algorithm possesses a regret bound of $\mathcal{O}(\sqrt{T})$. Experimental results demonstrate that our online framework can adaptively select the best policy under varying spot market dynamics and prediction quality, consistently outperforming baselines and improving utility by up to 54.8\%.

</details>


### [8] [Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality](https://arxiv.org/abs/2512.20968)
*Sirui Chen,Jingji Chen,Siqi Zhu,Ziheng Jiang,Yanghua Peng,Xuehai Qian*

Main category: cs.DC

TL;DR: 是的，这篇论文与**编译器**、**图处理**（可能间接关联，因为涉及数据排布和并行计算，但主要不是图处理算法）和**MLIR**（没有直接提及，但分布式并行计算优化与编译器基础设施相关）和**DSL**（没有直接提及）和****HLS（没有直接提及）相关，因为它关注于**大型语言模型（LLMs）**中的**分布式注意力算法**的优化，特别是**通信效率**和**可扩展性**的改进，这属于高性能计算和编译优化的范畴。
**TLDR:** 针对 LLM 扩展上下文窗口时 Ring-Attention 通信效率低的问题，本文提出了 **Mesh-Attention** 分布式注意力算法。该方法使用基于矩阵的新模型，并为每个 GPU 分配一个**二维 Tile** 而非一维块，以降低通信计算比。Mesh-Attention 的理论分析显示其具有更低的通信复杂性和更好的可扩展性。实验证明，在 256 个 GPU 上，Mesh-Attention 比现有算法平均快 **2.9 倍**，通信量减少了约 **79.0%**，在大规模部署中展示出优越的性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式注意力算法，如 Ring-Attention，在扩展大型语言模型（LLMs）的上下文窗口时，由于通信流量过大而存在可扩展性限制。目标是提出一种更高效的分布式注意力算法，以解决这一瓶颈。

Method: 通过重新思考分布式注意力的设计空间，并引入一个新的基于矩阵的模型，提出了 Mesh-Attention 算法。该方法将二维计算块分配给每个 GPU，以降低通信计算比（CommCom）。还包含一个贪心算法来有效搜索 Tile 内部的调度空间，并确保 GPU 间高效通信。

Result: Mesh-Attention 算法在 256 个 GPU 上实现了高达 3.4 倍（平均 2.9 倍）的速度提升，并将通信量减少了高达 85.4%（平均 79.0%）。理论分析显示其具有低得多的通信复杂性和更好的可扩展性。实验结果证明了其在大规模部署中能保持优越性能，显著降低开销。

Conclusion: Mesh-Attention显著优于现有的分布式注意力算法（如 Ring-Attention），在性能、通信效率和大规模部署的可扩展性方面都展现出优势，为扩展大型语言模型的上下文窗口提供了一个更高效的解决方案。

Abstract: Distributed attention is a fundamental problem for scaling context window for Large Language Models (LLMs). The state-of-the-art method, Ring-Attention, suffers from scalability limitations due to its excessive communication traffic. This paper proposes a new distributed attention algorithm, Mesh-Attention, by rethinking the design space of distributed attention with a new matrix-based model. Our method assigns a two-dimensional tile -- rather than one-dimensional row or column -- of computation blocks to each GPU to achieve higher efficiency through lower communication-computation (CommCom) ratio. The general approach covers Ring-Attention as a special case, and allows the tuning of CommCom ratio with different tile shapes. Importantly, we propose a greedy algorithm that can efficiently search the scheduling space within the tile with restrictions that ensure efficient communication among GPUs. The theoretical analysis shows that Mesh-Attention leads to a much lower communication complexity and exhibits good scalability comparing to other current algorithms.
  Our extensive experiment results show that Mesh-Attention can achieve up to 3.4x speedup (2.9x on average) and reduce the communication volume by up to 85.4% (79.0% on average) on 256 GPUs. Our scalability results further demonstrate that Mesh-Attention sustains superior performance as the system scales, substantially reducing overhead in large-scale deployments. The results convincingly confirm the advantage of Mesh-Attention.

</details>
