{"id": "2512.03594", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.03594", "abs": "https://arxiv.org/abs/2512.03594", "authors": ["Afsara Khan", "Austin Rovinski"], "title": "Accelerating Detailed Routing Convergence through Offline Reinforcement Learning", "comment": "To be published in the Design, Automation and Test in Europe (DATE) 2026 Conference", "summary": "Detailed routing remains one of the most complex and time-consuming steps in modern physical design due to the challenges posed by shrinking feature sizes and stricter design rules. Prior detailed routers achieve state-of-the-art results by leveraging iterative pathfinding algorithms to route each net. However, runtimes are a major issue in detailed routers, as converging to a solution with zero design rule violations (DRVs) can be prohibitively expensive.\n  In this paper, we propose leveraging reinforcement learning (RL) to enable rapid convergence in detailed routing by learning from previous designs. We make the key observation that prior detailed routers statically schedule the cost weights used in their routing algorithms, meaning they do not change in response to the design or technology. By training a conservative Q-learning (CQL) model to dynamically select the routing cost weights which minimize the number of algorithm iterations, we find that our work completes the ISPD19 benchmarks with 1.56x average and up to 3.01x faster runtime than the baseline router while maintaining or improving the DRV count in all cases. We also find that this learning shows signs of generalization across technologies, meaning that learning designs in one technology can translate to improved outcomes in other technologies."}
{"id": "2512.03608", "categories": ["cs.AR", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.03608", "abs": "https://arxiv.org/abs/2512.03608", "authors": ["Lishuo Deng", "Shaojie Xu", "Jinwu Chen", "Changwei Yan", "Jiajie Wang", "Zhe Jiang", "Weiwei Shan"], "title": "KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing", "comment": null, "summary": "Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.\n  We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\\(\\times\\)/1.94\\(\\times\\)/2.05\\(\\times\\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length."}
{"id": "2512.03616", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.03616", "abs": "https://arxiv.org/abs/2512.03616", "authors": ["Christian Ewert", "Amrit Sharma Poudel", "Mouadh Ayache", "Andrija Neskovic", "Rainer Buchty", "Mladen Berekovic", "Sebastian Berndt", "Saleh Mulhem"], "title": "Lightweight Unified Sha-3/Shake Architecture with a Fault-Resilient State", "comment": "-", "summary": "Hash functions have become a key part of standard Post-quantum cryptography (PQC) schemes, especially Sha-3 and Shake, calling arXiv:submit/7045552 [cs.AR] 3 Dec 2025 for lightweight implementation. A fault-resilient design is always desirable to make the whole PQC system reliable. We, therefore, propose a) a unified hash engine supporting Sha-3 and Shake that follows a byte-wise in-place partitioning mechanism of the so-called Keccak state, and b) an according fault detection for Keccak state protection exploiting its cube structure by deploying two-dimensional parity checks. It outperforms the state-of-the-art (SoA) regarding area requirements at competitive register-level fault detection by achieving 100% detection of three and still near 100% of higher numbers of Keccak state faults. Unlike SoA solutions, the proposed unified hash engine covers all standard hash configurations. Moreover, the introduced multidimensional cross-parity check mechanism achieves a 3.7x improvement in area overhead, with an overall 4.5x smaller fault-resilient engine design as demonstrated in ASIC and FPGA implementations. Integrated into a RISC-V environment, the unified hash engine with the integrated fault-resilient mechanism introduced less than 8% area overhead. Our approach thus provides a robust and lightweight fault-detection solution for protecting hash functions deployed in resource-constrained PQC applications."}
{"id": "2512.03781", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.03781", "abs": "https://arxiv.org/abs/2512.03781", "authors": ["Joscha Ilmberger", "Johannes Schemmel"], "title": "The BrainScaleS-2 multi-chip system: Interconnecting continuous-time neuromorphic compute substrates", "comment": null, "summary": "The BrainScaleS-2 SoC integrates analog neuron and synapse circuits with digital periphery, including two CPUs with SIMD extensions. Each ASIC is connected to a Node-FPGA, providing experiment control and Ethernet connectivity. This work details the scaling of the compute substrate through FPGA-based interconnection via an additional Aggregator unit. The Aggregator provides up to 12 transceiver links to a backplane of Node-FPGAs, as well as 4 transceiver lanes for further extension. Two such interconnected backplanes are integrated into a standard 19in rack case with 4U height together with an Ethernet switch, system controller and power supplies. For all spike rates, chip-to-chip latencies -- consisting of four hops across three FPGAs -- below 1.3$μ$s are achieved within each backplane."}
{"id": "2512.03124", "categories": ["cs.DS", "cs.CG"], "pdf": "https://arxiv.org/pdf/2512.03124", "abs": "https://arxiv.org/abs/2512.03124", "authors": ["Michael Souza", "Júlio Araújo", "John Kesley Costa", "Carlile Lavor"], "title": "On the Complexity of the Ordered Covering Problem in Distance Geometry", "comment": null, "summary": "The Ordered Covering Problem (OCP) arises in the context of the Discretizable Molecular Distance Geometry Problem (DMDGP), where the ordering of pruning edges significantly impacts the performance of the SBBU algorithm for protein structure determination. In recent work, Souza et al. (2023) formalized OCP as a hypergraph covering problem with ordered, exponential costs, and proposed a greedy heuristic that outperforms the original SBBU ordering by orders of magnitude. However, the computational complexity of finding optimal solutions remained open. In this paper, we prove that OCP is NP-complete through a polynomial-time reduction from the strongly NP-complete 3-Partition problem. Our reduction constructs a tight budget that forces optimal solutions to correspond exactly to valid 3-partitions. This result establishes a computational barrier for optimal edge ordering and provides theoretical justification for the heuristic approaches currently used in practice."}
{"id": "2512.03416", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03416", "abs": "https://arxiv.org/abs/2512.03416", "authors": ["Ruiqi Lai", "Hongrui Liu", "Chengzhi Lu", "Zonghao Liu", "Siyu Cao", "Siyang Shao", "Yixin Zhang", "Luo Mai", "Dmitrii Ustiugov"], "title": "TokenScale: Timely and Accurate Autoscaling for Disaggregated LLM Serving with Token Velocity", "comment": null, "summary": "The architectural shift to prefill/decode (PD) disaggregation in LLM serving improves resource utilization but struggles with the bursty nature of modern workloads. Existing autoscaling policies, often retrofitted from monolithic systems like those in AIBrix and DistServe, rely on lagging indicators such as GPU utilization or coarse-grained request counts. This results in slow reactions to load spikes, leading to significant Time-to First-Token (TTFT) and Time-Per-Output-Token (TPOT) SLO violations and costly over-provisioning. We introduce TokenScale, an autoscaling framework that resolves this performance mismatch through two innovations. First, we propose Token Velocity, a novel metric that unifies the prefill, network, and decode stages by quantifying their rate of work. As a leading indicator of system backpressure, it enables proactive scaling. Second, Convertible Decoders allow decoder GPUs to dynamically execute prefill tasks during traffic spikes, creating a rapid-response buffer that absorbs bursts and eliminates the initialization latency of new prefillers. Our evaluation on a GPU cluster with production traces shows TokenScale improves SLO attainment from 50-88% to 80-96% and reduces costs by 4-14% over state-of-the-art systems, including DistServe, BlitzScale, and AIBrix. By uniting a predictive metric with a flexible system design, TokenScale significantly boosts the performance and efficiency of disaggregated LLM serving infrastructure."}
{"id": "2512.03275", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.03275", "abs": "https://arxiv.org/abs/2512.03275", "authors": ["Aditya Anand", "Vincent Cohen-Addad", "Tommaso d'Orsi", "Anupam Gupta", "Euiwoong Lee", "Debmalya Panigrahi", "Sijin Peng"], "title": "Complexity of Local Search for CSPs Parameterized by Constraint Difference", "comment": "IPEC 2025", "summary": "In this paper, we study the parameterized complexity of local search, whose goal is to find a good nearby solution from the given current solution. Formally, given an optimization problem where the goal is to find the largest feasible subset $S$ of a universe $U$, the new input consists of a current solution $P$ (not necessarily feasible) as well as an ordinary input for the problem.\n  Given the existence of a feasible solution $S^*$, the goal is to find a feasible solution as good as $S^*$ in parameterized time $f(k) \\cdot n^{O(1)}$, where $k$ denotes the distance $|PΔS^*|$. This model generalizes numerous classical parameterized optimization problems whose parameter $k$ is the minimum number of elements removed from $U$ to make it feasible, which corresponds to the case $P = U$.\n  We apply this model to widely studied Constraint Satisfaction Problems (CSPs), where $U$ is the set of constraints, and a subset $U'$ of constraints is feasible if there is an assignment to the variables satisfying all constraints in $U'$. We give a complete characterization of the parameterized complexity of all boolean-alphabet symmetric CSPs, where the predicate's acceptance depends on the number of true literals."}
{"id": "2512.03487", "categories": ["cs.DC", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.03487", "abs": "https://arxiv.org/abs/2512.03487", "authors": ["Zhen Wang", "Bin Lin", "Qiang", "Ye"], "title": "Double-Edge-Assisted Computation Offloading and Resource Allocation for Space-Air-Marine Integrated Networks", "comment": null, "summary": "In this paper, we propose a double-edge-assisted computation offloading and resource allocation scheme tailored for space-air-marine integrated networks (SAMINs). Specifically, we consider a scenario where both unmanned aerial vehicles (UAVs) and a low earth orbit (LEO) satellite are equipped with edge servers, providing computing services for maritime autonomous surface ships (MASSs). Partial computation workloads of MASSs can be offloaded to both UAVs and the LEO satellite, concurrently, for processing via a multi-access approach. To minimize the energy consumption of SAMINs under latency constraints, we formulate an optimization problem and propose energy efficient algorithms to jointly optimize offloading mode, offloading volume, and computing resource allocation of the LEO satellite and the UAVs, respectively. We further exploit an alternating optimization (AO) method and a layered approach to decompose the original problem to attain the optimal solutions. Finally, we conduct simulations to validate the effectiveness and efficiency of the proposed scheme in comparison with benchmark algorithms."}
{"id": "2512.03304", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.03304", "abs": "https://arxiv.org/abs/2512.03304", "authors": ["Mirosław Kowaluk", "Andrzej Lingas", "Mia Persson"], "title": "Fast approximate $\\ell$-center clustering in high dimensional spaces", "comment": "18 pages", "summary": "We study the design of efficient approximation algorithms for the\n  $\\ell$-center clustering and minimum-diameter $\\ell$-clustering\n  problems in high dimensional Euclidean and Hamming spaces. Our main\n  tool is randomized dimension reduction. First, we present a general\n  method of reducing the dependency of the running time of a\n  hypothetical algorithm for the $\\ell$-center problem in a high\n  dimensional Euclidean space on the dimension size. Utilizing in\n  part this method, we provide $(2+ε)$- approximation\n  algorithms for the $\\ell$-center clustering and minimum-diameter\n  $\\ell$-clustering problems in Euclidean and Hamming spaces that are\n  substantially faster than the known $2$-approximation ones when both\n  $\\ell$ and the dimension are super-logarithmic. Next, we apply the\n  general method to the recent fast approximation algorithms with\n  higher approximation guarantees for the $\\ell$-center clustering\n  problem in a high dimensional Euclidean space. Finally, we provide a\n  speed-up of the known $O(1)$-approximation method for the\n  generalization of the $\\ell$-center clustering problem to include\n  $z$ outliers (i.e., $z$ input points can be ignored while computing\n  the maximum distance of an input point to a center) in high\n  dimensional Euclidean and Hamming spaces."}
{"id": "2512.03565", "categories": ["cs.DC", "cs.CE", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.03565", "abs": "https://arxiv.org/abs/2512.03565", "authors": ["Luis Gall", "Samuel James Newcome", "Fabio Alexander Gratl", "Markus Mühlhäußer", "Manish Kumar Mishra", "Hans-Joachim Bungartz"], "title": "Tuning of Vectorization Parameters for Molecular Dynamics Simulations in AutoPas", "comment": "20 pages, 8 figures. Submitted to the 5th International Conference on Computational Engineering (ICCE 2024). No changes were made after the peer review process", "summary": "Molecular Dynamics simulations can help scientists to gather valuable insights for physical processes on an atomic scale. This work explores various techniques for SIMD vectorization to improve the pairwise force calculation between molecules in the scope of the particle simulation library AutoPas. The focus lies on the order in which particle values are loaded into vector registers to achieve the most optimal performance regarding execution time or energy consumption.\n  As previous work indicates that the optimal MD algorithm can change during runtime, this paper investigates simulation-specific parameters like particle density and the impact of the neighbor identification algorithms, which distinguishes this work from related projects. Furthermore, AutoPas' dynamic tuning mechanism is extended to choose the optimal vectorization order during runtime.\n  The benchmarks show that considering different particle interaction orders during runtime can lead to a considerable performance improvement for the force calculation compared to AutoPas' previous approach."}
{"id": "2512.03311", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.03311", "abs": "https://arxiv.org/abs/2512.03311", "authors": ["Sandy Irani", "Michael Luby"], "title": "Singing a MIS", "comment": "34 pages, 4 figures", "summary": "We introduce a broadcast model called the singing model, where agents are oblivious of the size and structure of the communication network, even their immediate neighborhood. Agents can sing multiple notes which are heard by their neighbors. The model is a generalization of the beeping model, where agents can only emit sound at a single frequency. We give a simple and natural protocol where agents compete with their neighbors and their strength is reflected in the number of notes they sing. It converges in $O(log(n))$ time with high probability, where $n$ is the number of agents in the network. The protocol works in an asynchronous model where rounds vary in length and have different start times. It works with completely dynamic networks where agents can be faulty. The protocol is the first to converge to an MIS in logarithmic time for dynamic networks in a network oblivious model."}
{"id": "2512.03644", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03644", "abs": "https://arxiv.org/abs/2512.03644", "authors": ["Bohan Zhao", "Yuanhong Wang", "Chenglin Liu", "Jiagi Pan", "Guang Yang", "Ruitao Liu", "Tingrui Zhang", "Kai Luo", "Wei Xu"], "title": "FFTrainer: Fast Failover in Large-Language Model Training with Almost-Free State Management", "comment": null, "summary": "Recent developments in large language models (LLMs) have introduced new requirements for efficient and robust training. As LLM clusters scale, node failures, lengthy recoveries, and bulky checkpoints erode efficiency. Infrequent asynchronous checkpoints trigger costly rollbacks, yet higher frequencies add prohibitive overhead. To address these challenges, we propose FFTrainer, a system designed for robust LLM training. FFTrainer leverages surplus network capacity to quickly save and load states, thereby preventing rollbacks and accelerating recovery. Compared with prior checkpointing approaches, FFTrainer reduces recovery time by up to 98% and mitigates GPU utilization loss by up to 68% without hindering normal training."}
{"id": "2512.03972", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2512.03972", "abs": "https://arxiv.org/abs/2512.03972", "authors": ["Hassan Arafat", "David Bremner", "Kenneth B. Kent", "Julian Wang"], "title": "OOPredictor: Predicting Object-Oriented Accesses using Static Analysis", "comment": null, "summary": "Object-oriented Programming has become one of the most dominant design paradigms as the separation of concerns and adaptability of design reduce development and maintenance costs. However, the convenience is not without cost. The added indirection inherent in such designs causes excessive pointer chasing, negatively affecting locality, which in turn degrades the performance of cache structures. Furthermore, modern hardware prefetchers are mostly stride prefetchers that are ill-equipped to handle the unpredictability of access patterns generated by pointer chasing. Most software approaches that seek to address this problem resort to profiling the program as it runs, which comes with a significant run-time overhead or requires data from previous runs. In this paper, we propose the use of compile-time static analysis to predict the most common access patterns displayed by a program during run time. Since Java is one of the most popular object-oriented languages, we implement our prototype within the OpenJ9 JVM, inside the OMR optimizer infrastructure. The outputs of our proposed predictor are Markov chains that model the expected behavior of the program. The effectiveness of the proposed predictor is evaluated by comparing the model with the actual run-time behavior of the program measured using an instrumented interpreter. Our experiments show that the proposed predictor exhibits good accuracy and can be used to inform minimally intrusive load stall mitigation strategies, e.g. informing copying GCs on more locality-friendly copying orders"}
{"id": "2512.03419", "categories": ["cs.DS", "cs.DM", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03419", "abs": "https://arxiv.org/abs/2512.03419", "authors": ["Bharat Sharman", "Elkafi Hassini"], "title": "Comparative algorithm performance evaluation and prediction for the maximum clique problem using instance space analysis", "comment": null, "summary": "The maximum clique problem, a well-known graph-based combinatorial optimization problem, has been addressed through various algorithmic approaches, though systematic analyses of the problem instances remain sparse. This study employs the instance space analysis (ISA) methodology to systematically analyze the instance space of this problem and assess & predict the performance of state-of-the-art (SOTA) algorithms, including exact, heuristic, and graph neural network (GNN)-based methods. A dataset was compiled using graph instances from TWITTER, COLLAB and IMDB-BINARY benchmarks commonly used in graph machine learning research. A set of 33 generic and 2 problem-specific polynomial-time-computable graph-based features, including several spectral properties, was employed for the ISA. A composite performance mea- sure incorporating both solution quality and algorithm runtime was utilized. The comparative analysis demonstrated that the exact algorithm Mixed Order Maximum Clique (MOMC) exhib- ited superior performance across approximately 74.7% of the instance space constituted by the compiled dataset. Gurobi & CliSAT accounted for superior performance in 13.8% and 11% of the instance space, respectively. The ISA-based algorithm performance prediction model run on 34 challenging test instances compiled from the BHOSLIB and DIMACS datasets yielded top-1 and top-2 best performing algorithm prediction accuracies of 88% and 97%, respectively."}
{"id": "2512.03697", "categories": ["cs.DC", "cs.MS"], "pdf": "https://arxiv.org/pdf/2512.03697", "abs": "https://arxiv.org/abs/2512.03697", "authors": ["Rafael Ravedutti Lucio Machado", "Jan Eitzinger", "Georg Hager", "Gerhard Wellein"], "title": "On the Challenges of Energy-Efficiency Analysis in HPC Systems: Evaluating Synthetic Benchmarks and Gromacs", "comment": "8 pages, 4 figures, conference", "summary": "This paper discusses the challenges encountered when analyzing the energy efficiency of synthetic benchmarks and the Gromacs package on the Fritz and Alex HPC clusters. Experiments were conducted using MPI parallelism on full sockets of Intel Ice Lake and Sapphire Rapids CPUs, as well as Nvidia A40 and A100 GPUs. The metrics and measurements obtained with the Likwid and Nvidia profiling tools are presented, along with the results. The challenges and pitfalls encountered during experimentation and analysis are revealed and discussed. Best practices for future energy efficiency analysis studies are suggested."}
{"id": "2512.03718", "categories": ["cs.DS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03718", "abs": "https://arxiv.org/abs/2512.03718", "authors": ["Robert Ganian", "Hung P. Hoang", "Simon Wietheger"], "title": "Matrix Editing Meets Fair Clustering: Parameterized Algorithms and Complexity", "comment": null, "summary": "We study the computational problem of computing a fair means clustering of discrete vectors, which admits an equivalent formulation as editing a colored matrix into one with few distinct color-balanced rows by changing at most $k$ values. While NP-hard in both the fairness-oblivious and the fair settings, the problem is well-known to admit a fixed-parameter algorithm in the former ``vanilla'' setting. As our first contribution, we exclude an analogous algorithm even for highly restricted fair means clustering instances. We then proceed to obtain a full complexity landscape of the problem, and establish tractability results which capture three means of circumventing our obtained lower bound: placing additional constraints on the problem instances, fixed-parameter approximation, or using an alternative parameterization targeting tree-like matrices."}
{"id": "2512.03825", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03825", "abs": "https://arxiv.org/abs/2512.03825", "authors": ["Aingeru Ramos", "Jose A Pascual", "Javier Navaridas", "Ivan Coluzza"], "title": "Acceleration of Parallel Tempering for Markov Chain Monte Carlo methods", "comment": "14 pages, 7 figures (5 of them composed by 2 subfigures)", "summary": "Markov Chain Monte Carlo methods are algorithms used to sample probability distributions, commonly used to sample the Boltzmann distribution of physical/chemical models (e.g., protein folding, Ising model, etc.). This allows us to study their properties by sampling the most probable states of those systems. However, the sampling capabilities of these methods are not sufficiently accurate when handling complex configuration spaces. This has resulted in the development of new techniques that improve sampling accuracy, usually at the expense of increasing the computational cost. One of such techniques is Parallel Tempering which improves accuracy by running several replicas which periodically exchange their states. Computationally, this imposes a significant slow-down, which can be counteracted by means of parallelization. These schemes enable MCMC/PT techniques to be run more effectively and allow larger models to be studied. In this work, we present a parallel implementation of Metropolis-Hastings with Parallel Tempering, using OpenMP and CUDA for the parallelization in modern CPUs and GPUs, respectively. The results show a maximum speed-up of 52x using OpenMP with 48 cores, and of 986x speed-up with the CUDA version. Furthermore, the results serve as a basic benchmark to compare a future quantum implementation of the same algorithm."}
{"id": "2512.03843", "categories": ["cs.DS", "cs.CG"], "pdf": "https://arxiv.org/pdf/2512.03843", "abs": "https://arxiv.org/abs/2512.03843", "authors": ["Malory Marin", "Jean-Florent Raymond", "Rémi Watrigant"], "title": "Robust Algorithms for Path and Cycle Problems in Geometric Intersection Graphs", "comment": null, "summary": "We study the design of robust subexponential algorithms for classical connectivity problems on intersection graphs of similarly sized fat objects in $\\mathbb{R}^d$. In this setting, each vertex corresponds to a geometric object, and two vertices are adjacent if and only if their objects intersect. We introduce a new tool for designing such algorithms, which we call a $λ$-linked partition. This is a partition of the vertex set into groups of highly connected vertices. Crucially, such a partition can be computed in polynomial time and does not require access to the geometric representation of the graph. We apply this framework to problems related to paths and cycles in graphs. First, we obtain the first robust ETH-tight algorithms for Hamiltonian Path and Hamiltonian Cycle, running in time $2^{O(n^{1-1/d})}$ on intersection graphs of similarly sized fat objects in $\\mathbb{R}^d$. This resolves an open problem of de Berg et al. [STOC 2018] and completes the study of these problems on geometric intersection graphs from the viewpoint of ETH-tight exact algorithms. We further extend our approach to the parameterized setting and design the first robust subexponential parameterized algorithm for Long Path in any fixed dimension $d$. More precisely, we obtain a randomized robust algorithm running in time $2^{O(k^{1-1/d}\\log^2 k)}\\, n^{O(1)}$ on intersection graphs of similarly sized fat objects in $\\mathbb{R}^d$, where $k$ is the natural parameter. Besides $λ$-linked partitions, our algorithm also relies on a low-treewidth pattern covering theorem that we establish for geometric intersection graphs, which may be viewed as a refinement of a result of Marx-Pilipczuk [ESA 2017]. This structural result may be of independent interest."}
{"id": "2512.03927", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03927", "abs": "https://arxiv.org/abs/2512.03927", "authors": ["Liujianfu Wang", "Yuyang Du", "Yuchen Pan", "Soung Chang Liew", "Jiacheng Liu", "Kexin Chen"], "title": "OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference", "comment": null, "summary": "Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era."}
{"id": "2512.03960", "categories": ["cs.DS", "cs.DM", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.03960", "abs": "https://arxiv.org/abs/2512.03960", "authors": ["Noga Alon", "Sabyasachi Basu", "Shweta Jain", "Haim Kaplan", "Jakub Łącki", "Blair D. Sullivan"], "title": "Aggregating maximal cliques in real-world graphs", "comment": null, "summary": "Maximal clique enumeration is a fundamental graph mining task, but its utility is often limited by computational intractability and highly redundant output. To address these challenges, we introduce \\emph{$ρ$-dense aggregators}, a novel approach that succinctly captures maximal clique structure. Instead of listing all cliques, we identify a small collection of clusters with edge density at least $ρ$ that collectively contain every maximal clique.\n  In contrast to maximal clique enumeration, we prove that for all $ρ< 1$, every graph admits a $ρ$-dense aggregator of \\emph{sub-exponential} size, $n^{O(\\log_{1/ρ}n)}$, and provide an algorithm achieving this bound. For graphs with bounded degeneracy, a typical characteristic of real-world networks, our algorithm runs in near-linear time and produces near-linear size aggregators. We also establish a matching lower bound on aggregator size, proving our results are essentially tight. In an empirical evaluation on real-world networks, we demonstrate significant practical benefits for the use of aggregators: our algorithm is consistently faster than the state-of-the-art clique enumeration algorithm, with median speedups over $6\\times$ for $ρ=0.1$ (and over $300\\times$ in an extreme case), while delivering a much more concise structural summary."}
{"id": "2512.04054", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.04054", "abs": "https://arxiv.org/abs/2512.04054", "authors": ["Olasupo Ajayi", "Ryan Grant"], "title": "A Chronological Analysis of the Evolution of SmartNICs", "comment": "8 pages, 13 figures, 2 tables, Southern Africa Telecommunication Networks and Applications Conference (SATNAC) 2025", "summary": "Network Interface Cards (NICs) are one of the key enablers of the modern Internet. They serve as gateways for connecting computing devices to networks for the exchange of data with other devices. Recently, the pervasive nature of Internet-enabled devices coupled with the growing demands for faster network access have necessitated the enhancement of NICs to Smart NICs (SNICs), capable of processing enormous volumes of data at near real-time speed. However, despite their popularity, the exact use and applicability of SNICs remains an ongoing debate. These debates are exacerbated by the incorporation of accelerators into SNIC, allowing them to relieve their host's CPUs of various tasks. In this work, we carry out a chronological analysis of SNICs, using 370 articles published in the past 15 years, from 2010 to 2024, to gain some insight into SNICs; and shed some light on their evolution, manufacturers, use cases, and application domains."}
