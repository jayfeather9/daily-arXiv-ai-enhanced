<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DS](#cs.DS) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Algorithm-Driven On-Chip Integration for High Density and Low Cost](https://arxiv.org/abs/2512.10089)
*Jeongeun Kim,Sabrina Yarzada,Paul Chen,Christopher Torng*

Main category: cs.AR

TL;DR: 该论文与编译器（设计空间的自动化和算法驱动）和硬件描述语言（HLS，硬件设计和流片）相关。

**TLDR 摘要:** 针对大规模项目芯片流片中传统 MPW 的可扩展性问题以及现有方法缺乏系统化原则的现状，本文提出了一种新的三阶段方法：通过结构化设计空间实现项目自动封装（packing），引入利用站点间窄区域的共享通信架构，以及提供实用的片上电源域管理。实验证明，该方法相比于现有技术，可实现高达 13 倍的面积缩减，为大规模芯片流片提供了可扩展且经济高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统的基于物理共置的 MPW 服务在大规模项目数量增长时，可扩展性受限。现有尝试通过共享芯片上的资源来缓解这些限制，但缺乏对密集集成设计站点的布局、连接和验证的基本原则的系统性研究。因此，需要一种新的、系统化的方法来提高大规模芯片流片的效率和可扩展性。

Method: 本文提出了一个包含三个关键技术的新方法：1) 结构化地制定设计空间，用自动化算法取代手动布局，实现多项目的高效封装（packing）；2) 引入一种架构，仅利用设计站点之间的窄区域来提供片外通信和其他共享需求；3) 提供一种实用的片上电源域方法，允许使用标准实验室设备和无需低功耗 ASIC 设计专业知识的情况下进行单项目功耗特性分析。

Result: 实验结果表明，与现有最先进的仅基于物理聚合的方法相比，本文提出的方法实现了高达 13 倍的面积缩减。

Conclusion: 本文提出的方法通过结构化的设计空间表示、基于窄区域的共享资源架构和实用的片上电源域管理，实现了相比现有方法高达 13 倍的面积缩减，为大规模芯片流片环境提供了一条可扩展且经济高效的途径。

Abstract: Growing interest in semiconductor workforce development has generated demand for platforms capable of supporting large numbers of independent hardware designs for research and training without imposing high per-project overhead. Traditional multi-project wafer (MPW) services based solely on physical co-placement have historically met this need, yet their scalability breaks down as project counts rise. Recent efforts towards scalable chip tapeouts mitigate these limitations by integrating many small designs within a shared die and attempt to amortize costly resources such as IO pads and memory macros. However, foundational principles for arranging, linking, and validating such densely integrated design sites have received limited systematic investigation. This work presents a new approach with three key techniques to address this gap. First, we establish a structured formulation of the design space that enables automated, algorithm-driven packing of many projects, replacing manual layout practices. Second, we introduce an architecture that exploits only the narrow-area regions between sites to deliver on off-chip communication and other shared needs. Third, we provide a practical approach for on-chip power domains enabling per-project power characterization at a standard laboratory bench and requiring no expertise in low-power ASIC design. Experimental results show that our approach achieves substantial area reductions of up to 13x over state-of-the-art physical-only aggregation methods, offering a scalable and cost-effective path forward for large-scale tapeout environments.

</details>


### [2] [A Vertically Integrated Framework for Templatized Chip Design](https://arxiv.org/abs/2512.10155)
*Jeongeun Kim,Christopher Torng*

Main category: cs.AR

TL;DR: 该论文与以下领域相关：编译器（涉及高级规范到硬件的翻译）、HLS（High-Level Synthesis，从高级语言生成硬件）。
一个主要从事软件开发的工程师通常难以将定制硬件整合到其应用中。本文提出了一种方法，允许从高级面向对象的软件规范生成芯片，旨在帮助初级芯片设计学习者，并在软件对象与芯片布局之间保持“心智连续性”。通过将软件对象映射到芯片上的对应区域，并使用模块化IP块和基于序列的类型系统来确保硬件通信和效率，该方法成功地维护了从软件到芯片设计的心智连续性，从而降低了软件开发人员参与定制芯片创建所需的专业知识。


<details>
  <summary>Details</summary>
Motivation: 尽管定制硬件（专用芯片）能为机器学习、AI及其应用域带来巨大优势，但主要从事软件开发的工程师通常难以在其应用中整合定制硬件。这篇工作的动机是探索一种方法，使得软件开发人员能够从高级面向对象软件规范生成芯片，旨在降低设计门槛，同时在芯片布局和软件源代码之间保持“心智连续性”（mental continuity）。

Method: 该方法的核心在于建立软件对象与芯片区域的“一对一”结构映射，从而在整个设计流程中保持心智连续性。具体实现包括：1. 采用模块化构造策略，使用垂直组合的IP块来实现软件中表达的行为协议。2. 利用基于序列的正式类型系统来检查硬件模块间的交互是否符合软件模型中描述的通信模式，以解决直接语法翻译导致的效率和通信约束问题。3. 研究适用于这种对象对齐设计风格的硬件互连策略和布局技术。

Result: 研究提出了一种从高级面向对象软件规范生成芯片的方法，该方法通过保持软件对象到芯片区域的一一映射，成功维护了从软件到芯片设计的心智连续性。这使得初级芯片设计学习者（或只有轻微性能要求的应用）能够参与到芯片创建中，显著降低了软件开发人员利用定制硬件所需的技术专业知识。

Conclusion: 本文提出的方法通过在芯片设计中保持软件对象与硬件区域的一一对应关系，有效地降低了软件开发人员参与芯片设计的门槛，尤其适用于初级芯片设计学习者。这种方法通过维护从软件到硬件的心智连续性，将有助于更广泛地应用定制硬件。

Abstract: Developers who primarily engage with software often struggle to incorporate custom hardware into their applications, even though specialized silicon can provide substantial benefits to machine learning and AI, as well as to the application domains that they enable. This work investigates how a chip can be generated from a high-level object-oriented software specification, targeting introductory-level chip design learners with only very light performance requirements, while maintaining mental continuity between the chip layout and the software source program. In our approach, each software object is represented as a corresponding region on the die, producing a one-to-one structural mapping that preserves these familiar abstractions throughout the design flow. To support this mapping, we employ a modular construction strategy in which vertically composed IP blocks implement the behavioral protocols expressed in software. A direct syntactic translation, however, cannot meet hardware-level efficiency or communication constraints. For this reason, we leverage formal type systems based on sequences that check whether interactions between hardware modules adhere to the communication patterns described in the software model. We further examine hardware interconnect strategies for composing many such modules and develop layout techniques suited to this object-aligned design style. Together, these contributions preserve mental continuity from software to chip design for new learners and enables practical layout generation, ultimately reducing the expertise required for software developers to participate in chip creation.

</details>


### [3] [SemanticBBV: A Semantic Signature for Cross-Program Knowledge Reuse in Microarchitecture Simulation](https://arxiv.org/abs/2512.10231)
*Zhenguo Liu,Chengao Shi,Chen Ding,Jiang Xu*

Main category: cs.AR

TL;DR: 该论文与编译器和微架构模拟有关（涉及汇编基本块、程序表示、模拟加速）。
**TLDR**: 针对传统 Basic Block Vector (BBV) 在跨程序重用和性能敏感性上的不足，本文提出了 SemanticBBV 框架。该框架使用轻量级 RWKV 模型将基本块编码为具有深层语义的嵌入（BBE），随后使用顺序不变的 Set Transformer 聚合这些 BBE（按执行频率加权），通过结合三元组损失和 CPI 回归任务进行联合训练，生成具有性能感知能力的程序签名。评估显示，SemanticBBV 在保持单程序准确性的同时，实现了前所未有的跨程序分析能力，仅通过模拟 14 个通用程序点，就在 SPEC CPU 基准测试上实现了 86.3% 的平均准确率和 7143 倍的模拟加速。


<details>
  <summary>Details</summary>
Motivation: 传统的采样加速模拟技术（例如基于 BBV 的方法）存在显著局限性：
1. **BBV 的局限性**: 其 IDs 依赖于顺序，阻碍了跨程序知识的重用。
2. **缺乏性能预测性**: BBV 缺乏对硬件性能有预测性的语义内容。这导致了巨大的优化潜力未被发掘。
  * **目标**: 引入 SemanticBBV 来弥补这些差距，生成稳健且具有性能感知能力的签名，以实现跨程序模拟重用。

Method: SemanticBBV 采用两阶段框架：
1. **语义编码器（Semantic Encoder）**: 使用轻量级的 RWKV 模型将汇编基本块转换为丰富的基本块嵌入（BBEs），捕获深层功能语义。
2. **集合转换器（Set Transformer）**: 聚合这些 BBEsS（按执行频率加权），生成最终的签名。此聚合是顺序不变的。
  * **联合训练**: 该阶段与双重目标共同训练：
    * **三元组损失（Triplet loss）**: 用于确保签名的区分度（distinctiveness）。
    * **CPI 回归任务（CPI regression task）**: 直接赋予签名性能敏感性。

Result: 1. **单程序准确性**: SemanticBBV 在单程序准确性上与传统 BBV 相当。
2. **跨程序分析能力**: 实现了前所未有的跨程序分析。
3. **性能估计**: 通过仅使用 14 个通用程序点进行模拟，估计十个 SPEC CPU 基准测试的性能，平均准确率达到 86.3%。
4. **模拟加速比**: 实现了 7143 倍的模拟加速。
5. **适应性**: 该签名对新的微架构具有很强的适应性，只需极少的微调。

Conclusion: SemanticBBV 是一种新颖的、两阶段的框架，它生成稳健的、具有性能感知能力的签名，用于跨程序模拟重用。它解决了传统 BBV 的局限性，实现了前所未有的跨程序分析和巨大的模拟加速（7143 倍），同时保持了与传统 BBV 相当的单程序准确性，并具有对新微架构的良好适应性。

Abstract: For decades, sampling-based techniques have been the de facto standard for accelerating microarchitecture simulation, with the Basic Block Vector (BBV) serving as the cornerstone program representation. Yet, the BBV's fundamental limitations: order-dependent IDs that prevent cross-program knowledge reuse and a lack of semantic content predictive of hardware performance have left a massive potential for optimization untapped.
  To address these gaps, we introduce SemanticBBV, a novel, two-stage framework that generates robust, performance-aware signatures for cross-program simulation reuse. First, a lightweight RWKV-based semantic encoder transforms assembly basic blocks into rich Basic Block Embeddings (BBEs), capturing deep functional semantics. Second, an order-invariant Set Transformer aggregates these BBEs, weighted by execution frequency, into a final signature. Crucially, this stage is co-trained with a dual objective: a triplet loss for signature distinctiveness and a Cycles Per Instruction (CPI) regression task, directly imbuing the signature with performance sensitivity. Our evaluation demonstrates that SemanticBBV not only matches traditional BBVs in single-program accuracy but also enables unprecedented cross-program analysis. By simulating just 14 universal program points, we estimated the performance of ten SPEC CPU benchmarks with 86.3% average accuracy, achieving a 7143x simulation speedup. Furthermore, the signature shows strong adaptability to new microarchitectures with minimal fine-tuning.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [4] [Intrinsically Correct Algorithms and Recursive Coalgebras](https://arxiv.org/abs/2512.10748)
*Cass Alexandru,Henning Urbat,Thorsten Wißmann*

Main category: cs.PL

TL;DR: The paper is related to **Compiler** and **DSL/Programming Language Theory** in the context of formal methods for algorithm termination (Coalgebras and Category Theory are used to model algorithms and their properties, which is foundational to programming language semantics and verification, essential for robust compilers and domain-specific languages).

Recursive coalgebras are great for modeling recursive algorithms, but proving their recursivity (termination) is hard and ad hoc, which complicates formalization in proof assistants. This paper introduces the novel concept of a **well-founded functor** on a category of families indexed by a well-founded relation. The main result is that **every coalgebra for a well-founded functor is intrinsically recursive**, guaranteeing termination from the type itself. This framework subsumes known termination techniques like ranking functions. Case studies include Quicksort and CYK parsing, with formalization in Cubical Agda.


<details>
  <summary>Details</summary>
Motivation: 现有的递归 coalgebra 模型在建模递归算法时很优雅，但其递归性（对应于算法的终止性）的证明通常是特设（ad hoc）且复杂的，这使得在证明助手（如 Cubical Agda）中形式化这些 coalgebra 定义的递归算法变得困难。作者的动机是找到一种框架，能够**内在**地保证 coalgebra 的递归性，即让 coalgebra 的类型从一开始就保证其递归性，从而简化形式化工作。

Method: 本文首先指出了现有递归 coalgebra 建模的缺点，即证明 coalgebra 的递归性（递归算法的终止性）通常是非平凡和 ad hoc 的。为了解决这个问题，作者引入了 **well-founded functor**（良基函子）这一新概念，它作用于一个由良基关系索引的家族范畴上。作者的主要理论成果是证明了 **well-founded functor 的每个 coalgebra 都是递归的**。此外，作者证明了现有的如 ranking functions（排序函数）等证明递归性和终止性的技术都被这一抽象设置所涵盖。最后，选择 Cubical Agda 对主要理论结果和几个案例进行了形式化验证。

Result: 本文引入了 well-founded functor 的新概念，并证明了其核心成果：作用于良基关系索引的家族范畴上的 **well-founded functor 的每个 coalgebra 都是递归的**。这提供了一个新的、内在保证递归性的框架。作者展示了该框架可以统一并概括现有的终止性证明技术（如 ranking functions）。并通过 Quicksort、欧几里得算法和 CYK 解析器等案例研究验证了方法的有效性。主要理论结果和部分案例已在 Cubical Agda 中形式化。

Conclusion: 本文提出的 well-founded functor 框架为递归算法的建模和形式化提供了一种新的、内在的方式。通过将递归性保证从 ad hoc 的证明转移到 coalgebra 的类型本身，极大地简化了在证明助手（如 Cubical Agda）中对这些算法进行形式化的过程。这种抽象方法统一并概括了现有的终止证明技术，是一个重要的理论贡献。

Abstract: Recursive coalgebras provide an elegant categorical tool for modelling recursive algorithms and analysing their termination and correctness. By considering coalgebras over categories of suitably indexed families, the correctness of the corresponding algorithms follows intrinsically just from the type of the computed maps. However, proving recursivity of the underlying coalgebras is non-trivial, and proofs are typically ad hoc. This layer of complexity impedes the formalization of coalgebraically defined recursive algorithms in proof assistants. We introduce a framework for constructing coalgebras which are intrinsically recursive in the sense that the type of the coalgebra guarantees recursivity from the outset. Our approach is based on the novel concept of a well-founded functor on a category of families indexed by a well-founded relation. We show as our main result that every coalgebra for a well-founded functor is recursive, and demonstrate that well-known techniques for proving recursivity and termination such as ranking functions are subsumed by this abstract setup. We present a number of case studies, including Quicksort, the Euclidian algorithm, and CYK parsing. Both the main theoretical result and selected case studies have been formalized in Cubical Agda.

</details>


### [5] [Towards Cumulative Abstract Semantics via Handlers](https://arxiv.org/abs/2512.10861)
*Cade Lueker,Andrew Fox,Bor-Yuh Evan Chang*

Main category: cs.PL

TL;DR: 关联领域：编译器（Compiler）/静态分析（Static Analysis）。
太长不看（TLDR）：本文提出了一种基于**作用域效应**（Scoped Effects）的**累积抽象语义**（Cumulative Abstract Semantics）方法，旨在解决抽象解释器中**控制流模块化**的问题。传统的解释器将语法和语义紧密耦合，或者需要复杂的 monad 转换器来实现模块化。通过将效应分为“语法消除”和“域语义引入”两组处理程序，作者成功地构建了一个干净、灵活且**模块化**的抽象解释框架，允许从一个解释器派生出多个动态求值器和静态分析器，从而提高了框架的灵活性和可重用性。


<details>
  <summary>Details</summary>
Motivation: 现有的通用抽象解释框架在灵活性上不足，难以支持不同的路径敏感性、流敏感性、前向或后向分析，以及过近似或欠近似的解释。此外，大多数解释器将语法和语义紧密耦合，使得模块化实现非常困难。尽管有使用复杂数据结构（如 monad transformers）实现模块化的方法，但它们往往笨重且难以使用（例如，需要大量的 lifts 操作）。因此，本文的动机是寻找一种更干净、更优雅、更易于管理的机制来解耦语法和语义，实现抽象解释器中控制流的模块化。

Method: 本文的核心方法是定义“累积抽象语义”，并通过利用作用域效应来实现。具体而言，它将效应分为两大类处理程序：1. 语法消除（Syntax Elimination）处理程序，用于解耦语法；2. 域语义引入（Domain-Semantic Introduction）处理程序，用于引入抽象域的语义。这种分组和效应处理机制允许在固定的语法基础上积累语义片段，从而实现从一个解释器创建多个动态求值器和静态分析器的模块化设计。

Result: 通过使用作用域效应和累积抽象语义，本文成功地实现了一个高度模块化的抽象解释框架。具体成果是：1. 实现了语法和语义的解耦，允许在固定语法上积累不同的语义片段。2. 证明了可以从同一个解释器创建出多个动态求值器和静态分析器，体现了高度的模块性。3. 提供了一种比使用复杂 monad 转换器更简洁、更优雅的设计，克服了现有模块化方法的笨重性问题。

Conclusion: 本文提出了一种使用作用域效应（Scoped Effects）来构建累积抽象语义（Cumulative Abstract Semantics）的方法，从而实现了一个干净、优雅且模块化的抽象解释框架。这种方法将效应分组为两个类别：语法消除和域语义引入处理程序，解决了现有抽象解释器中语法和语义紧密耦合、以及模块化设计中需要复杂数据结构（如 monad transformers）带来的不便性问题。这表明使用效应作为工具可以有效地提高抽象解释框架的模块性和灵活性。

Abstract: We consider the problem of modularizing control flow in a generic abstract interpretation framework. A generic abstract interpretation framework is not truly flexible if it does not allow interpreting with different path- and flow-sensitivities, by going forwards or backwards, and over- or under-approximately. Most interpreters inherently intertwine syntax and semantics, making the implementation antagonistic to modularity. Current approaches to modular designs require the use of complex data structures (e.g., monad transformers), providing modularity but often proving unwieldy (e.g., lifts). We observe that leveraging scoped effects within an interpreter facilitates the accumulation of semantic fragments against a fixed syntax. In this paper, we define cumulative abstract semantics, illustrating the potential for creating multiple dynamic evaluators and static analyses from one interpreter. This modularity is achieved by grouping effects into two categories: syntax elimination and domain-semantic introduction handlers. Our contribution shows the benefits of using effects as an instrument for designing a clean, elegant, and modular abstract interpretation framework.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [A study of the spectrum resource leasing method based on ERC4907 extension](https://arxiv.org/abs/2512.09942)
*Zhiming Liang,Bin Chen,Litao Ye,Chen Sun,Shuo Wang,Zhe Peng*

Main category: cs.DC

TL;DR: 该论文与编译器（更广义的区块链智能合约和协议）相关。
多时隙 ERC4907 (M-ERC4907) 扩展了现有的 ERC4907 NFT 租赁标准，解决了其单用户、单时隙授权的限制。M-ERC4907 允许批量配置多时隙和多用户同时授权，消除了顺序授权的约束。在 Remix 平台上的实验结果证明，M-ERC4907 显著减少了链上交易和 Gas 消耗，提高了去中心化多时隙调度场景下的可扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的 ERC4907 标准允许可租赁的非同质化代币（NFTs），但它局限于单用户、单时间时隙的授权，这严重限制了其在去中心化多时隙调度场景中的适用性和效率。因此，需要一种扩展方法来支持多时隙和多用户授权，以提高效率和可扩展性。

Method: 本文提出了多时隙 ERC4907 (M-ERC4907) 扩展方法。该方法引入了新的功能，支持批量配置多个时间时隙和同时授权多个用户，从而有效地消除了 ERC4907 严格的顺序授权约束。实验在 Remix 开发平台进行，通过比对交易和 Gas 消耗来评估 M-ERC4907 的效果。

Result: 实验结果表明，多时隙 ERC4907 (M-ERC4907) 方法显著减少了链上交易次数和总体 Gas 消耗，从而提高了可扩展性和资源分配效率。

Conclusion: 多时隙 ERC4907 (M-ERC4907) 扩展方法成功地通过引入批量配置多时隙和多用户同时授权能力，克服了现有 ERC4907 标准在去中心化多时隙调度场景中的局限性。实验证明，M-ERC4907 能显著减少链上交易和 Gas 消耗，提高了可扩展性和资源分配效率。

Abstract: The ERC4907 standard enables rentable Non-Fungible Tokens (NFTs) but is limited to single-user, single-time-slot authorization, which severely limits its applicability and efficiency in decentralized multi-slot scheduling scenarios. To address this limitation, this paper proposes Multi-slot ERC4907 (M-ERC4907) extension method. The M-ERC4907 method introduces novel functionalities to support the batch configuration of multiple time slots and simultaneous authorization of multiple users, thereby effectively eliminating the rigid sequential authorization constraint of ERC4907. The experiment was conducted on the Remix development platform. Experimental results show that the M-ERC4907 method significantly reduces on-chain transactions and overall Gas consumption, leading to enhanced scalability and resource allocation efficiency.

</details>


### [7] [ELANA: A Simple Energy and Latency Analyzer for LLMs](https://arxiv.org/abs/2512.09946)
*Hung-Yueh Chiang,Bokun Wang,Diana Marculescu*

Main category: cs.DC

TL;DR: 关联：\textbf{编译器/HLS} (因为它是一个性能分析工具，通常用于评估编译器优化或高效硬件实现的基准)；\textbf{图处理} (如果推理过程可以抽象为计算图)。总结：LLM 的高延迟和高功耗限制了其在各种硬件上的部署。为解决这一问题，本文开源了一个名为 \textbf{ELANA} 的轻量级、学术友好的 LLM 性能分析工具，它提供了一个简单的命令行界面，用于评估模型大小、KV 缓存大小、预填充延迟（TTFT）、生成延迟（TPOT）和端到端延迟（TTLT），兼容 Hugging Face 上的所有公开模型、多 GPU 和边缘 GPU 平台，并支持定制以适用于压缩或低比特宽模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的延迟和功耗是将其部署到包括移动边缘设备和云 GPU 集群在内的各种硬件平台上的主要限制。为了在模型部署优化和下一代模型开发中提高效率，迫切需要一个基准测试和性能分析工具。

Method: 作者开源了一个名为 \textbf{ELANA} 的简单性能分析工具。该工具通过命令行界面提供模型大小、KV 缓存大小、预填充延迟（TTFT）、生成延迟（TPOT）和端到端延迟（TTLT）的分析。ELANA 支持 Hugging Face 上的所有公开模型，兼容流行的 Hugging Face API，并支持多 GPU 和边缘 GPU 平台，同时提供可选的能耗记录功能。

Result: ELANA 成功地提供了一个分析 LLM 关键性能指标的工具，包括模型大小、KV 缓存大小，以及三种延迟指标（TTFT、TPOT、TTLT）。它支持广泛的模型和硬件平台，易于使用和定制，使其适用于对高效 LLM 的研究和小型概念验证（PoC）研究。

Conclusion: ELANA 是一个轻量级、学术友好的 LLM 性能分析工具，能够灵活地应用于研究和概念验证场景，特别是在 LLM 效率的优化方面。它的开源和与现有生态的兼容性，使其成为评估和比较不同硬件平台上 LLM 性能的实用工具。

Abstract: The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.

</details>


### [8] [GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference](https://arxiv.org/abs/2512.09963)
*Phuong Tran,Tzu-Hao Liu,Long Tan Le,Tung-Anh Nguyen,Van Quan La,Eason Yu,Han Shu,Choong Seon Hong,Nguyen H. Tran*

Main category: cs.DC

TL;DR: 相关领域：图处理（Graph processing），编译器（Compiler），高层次综合（HLS），MLIR，领域专用语言（DSL）。
本文引入了GOODSPEED，一个用于分布式大型语言模型（LLM）推理的新颖框架，它采用**自适应推测解码**来**优化吞吐量（goodput）**。GOODSPEED使用一个中央验证服务器来协调异步的、异构的草稿服务器，并通过一个**梯度调度算法**来动态分配令牌验证任务，以最大化对数效用函数来确保**比例公平性**。流体样本路径分析证明了该框架在稳态下能收敛到**最优吞吐量分配**，并在动态工作负载下保持接近最优的性能。这种方法提供了一个可扩展、公平且高效的多服务器分布式LLM推理解决方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的计算需求高，对实时推理构成了重大挑战，尤其是在多用户服务器推测解码和资源受限环境中。推测解码通过使用轻量级草稿模型来加速LLM推理，但确保高吞吐量（goodput）和中心验证服务器与多个草稿服务器之间公平性的协同工作仍然是一个开放的挑战。

Method: GOODSPEED是一种新颖的分布式推理框架，它通过自适应推测解码来优化吞吐量。它采用一个中心验证服务器来协调一组异构的草稿服务器（每个服务器运行一个小型语言模型）。GOODSPEED包含一个梯度调度算法，该算法动态分配令牌验证任务，通过最大化对数效用函数来确保服务器之间的比例公平性。该框架并行处理来自所有草稿服务器的推测输出，实现了验证服务器与分布式草稿生成器之间的高效协作。通过流体样本路径分析证明了其最优性。

Result: 通过严格的流体样本路径分析，证明了GOODSPEED在稳态条件下收敛到最优吞吐量分配，并在动态工作负载下以可证明的有界误差保持接近最优的性能。GOODSPEED提供了一个可扩展、公平且高效的多服务器分布式LLM推理系统解决方案。

Conclusion: GOODSPEED提供了一种可扩展、公平且高效的分布式LLM推理解决方案，通过自适应推测解码和梯度调度算法实现了多服务器环境下的最优吞吐量分配和比例公平性，同时在动态和稳态工作负载下都保持了高性能。

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet their high computational demands pose significant challenges for real-time inference, especially in multi-user server speculative decoding and resource-constrained environments. Speculative decoding has emerged as a promising technique to accelerate LLM inference by using lightweight draft models to generate candidate tokens, which are subsequently verified by a larger, more accurate model. However, ensuring both high goodput (the effective rate of accepted tokens) and fairness across multiple draft servers cooperating with a central verification server remains an open challenge. This paper introduces GOODSPEED, a novel distributed inference framework that optimizes goodput through adaptive speculative decoding. GOODSPEED employs a central verification server that coordinates a set of heterogeneous draft servers, each running a small language model to generate speculative tokens. To manage resource allocation effectively, GOODSPEED incorporates a gradient scheduling algorithm that dynamically assigns token verification tasks, maximizing a logarithmic utility function to ensure proportional fairness across servers. By processing speculative outputs from all draft servers in parallel, the framework enables efficient collaboration between the verification server and distributed draft generators, streamlining both latency and throughput. Through rigorous fluid sample path analysis, we show that GOODSPEED converges to the optimal goodput allocation in steady-state conditions and maintains near-optimal performance with provably bounded error under dynamic workloads. These results demonstrate that GOODSPEED provides a scalable, fair and efficient solution for multi- in distributed LLM inference systems.

</details>


### [9] [Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters](https://arxiv.org/abs/2512.10271)
*Shruti Dongare,Redwan Ibne Seraj Khan,Hadeel Albahar,Nannan Zhao,Diego Melendez Maita,Ali R. Butt*

Main category: cs.DC

TL;DR: 关联领域：图处理（Graph processing）：不相关。DSL：不相关。MLIR：不相关。编译器（Compiler）：不相关。HLS：不相关。
TLDR：云平台上的大规模深度学习工作负载对GPU调度提出挑战，现有调度器难以应对集群异构性和应用特性不可知性。本文提出了RLTune，一个应用无关的强化学习（RL）调度框架。RLTune结合RL驱动的优先级划分和基于MILP的任务到节点映射，用于优化异构GPU集群上的系统性目标（JCT、排队延迟、资源利用率）。该框架在生产跟踪数据上训练和验证，结果表明它将GPU利用率提升了20%，排队延迟降低了81%，并将JCT缩短了70%，且无需每个作业的性能分析，使其适合大规模部署。


<details>
  <summary>Details</summary>
Motivation: 现代云平台上的大规模深度学习（DL）工作负载对GPU调度提出了高吞吐量、低延迟的要求。现有的调度器面临的主要挑战是：
1. **GPU集群日益增长的异构性（growing heterogeneity of GPU clusters）。**
2. **对应用程序特性了解有限（limited visibility into application characteristics）。**
现有的调度方法通常依赖于离线性能分析或特定于应用程序的假设，这限制了它们在复杂和动态环境中的效率和通用性。因此，需要一个无需针对每个作业进行性能分析、能够动态适应异构集群的通用调度框架。

Method: RLTune采用应用无关的强化学习框架进行调度，具体方法包括：
1. **RL驱动的优先级划分（RL-driven prioritization）：**利用强化学习动态地为深度学习作业确定优先级。
2. **基于MILP的作业到节点映射（MILP-based job-to-node mapping）：**结合混合整数线性规划来优化作业到异构GPU节点的分配。
这种集成方法旨在优化三个系统性目标：作业完成时间（JCT）、排队延迟和资源利用率。RLTune使用来自微软Philly、Helios和阿里巴巴的大规模生产跟踪数据进行训练。

Result: RLTune在来自微软Philly, Helios和阿里巴巴的大规模生产跟踪数据上进行了训练和验证，结果显示它带来了显著的性能提升：
1. **GPU利用率：**提高高达20%。
2. **排队延迟（queueing delay）：**减少高达81%。
3. **作业完成时间（JCT）：**缩短高达70%。
此外，RLTune的优势在于其泛化能力强，无需进行每个作业的性能分析（per-job profiling），使其能在大规模云环境部署中更具实用价值。

Conclusion: RLTune是一个适用于异构GPU集群的、与应用无关的强化学习（RL）调度框架。它结合了RL驱动的优先级划分和基于MILP（混合整数线性规划）的任务到节点映射，以优化如作业完成时间（JCT）、排队延迟和资源利用率等系统性目标。RLTune在生产环境中展现了显著的性能提升，提高了GPU利用率，减少了排队延迟和JCT，且无需针对每个作业进行性能分析，具备良好的泛化能力和部署实用性，能够实现更高效、公平和可持续的深度学习工作负载管理。

Abstract: Modern cloud platforms increasingly host large-scale deep learning (DL) workloads, demanding high-throughput, low-latency GPU scheduling. However, the growing heterogeneity of GPU clusters and limited visibility into application characteristics pose major challenges for existing schedulers, which often rely on offline profiling or application-specific assumptions. We present RLTune, an application-agnostic reinforcement learning (RL)-based scheduling framework that dynamically prioritizes and allocates DL jobs on heterogeneous GPU clusters. RLTune integrates RL-driven prioritization with MILP-based job-to-node mapping to optimize system-wide objectives such as job completion time (JCT), queueing delay, and resource utilization. Trained on large-scale production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens JCT by as much as 70 percent. Unlike prior approaches, RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management.

</details>


### [10] [High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments](https://arxiv.org/abs/2512.10312)
*Julian Rodriguez,Piotr Lopez,Emiliano Lerma,Rafael Medrano,Jacobo Hernandez*

Main category: cs.DC

TL;DR: 该论文与 DSL、图处理、MLIR、编译器或 HLS **无关**。
**太长不看（TLDR）:** 这篇文档报告了一个大数据课程中的实践和方法论序列，详细描述了完整的工作流程，包括使用分组和个体策略处理 Epsilon 数据集，使用 RestMex 进行文本分析和分类，使用 IMDb 进行电影特征分析，最后描述了在 Linux 上使用 Apache Spark 和 Scala 构建分布式计算集群的具体技术实现。


<details>
  <summary>Details</summary>
Motivation: 本文档的动机是报告在大数据课程中实施的实践序列和方法论，从而详细记录从数据集处理到分布式计算系统技术实现的完整工作流程。

Method: 在大数据课程中实施了一系列实践和方法，涵盖了数据集处理、文本分析、分类以及分布式计算集群的实现。具体包括：
1. **数据集处理**：对 Epsilon 数据集进行分组和个体策略处理。
2. **文本分析和分类**：使用 RestMex 进行文本分析和分类。
3. **特征分析**：使用 IMDb 进行电影特征分析。
4. **分布式计算实现**：使用 Apache Spark 和 Scala 在 Linux 上实现分布式计算集群。

Result: 工作流程的最终成果包括：
1. Epsilon 数据集的处理（通过分组和个体策略）。
2. 使用 RestMex 完成了文本分析和分类。
3. 使用 IMDb 完成了电影特征分析。
4. 使用 Apache Spark 和 Scala 在 Linux 上成功实现了分布式计算集群。

Conclusion: 本文报告了在大数据课程中实施的实践和方法序列，包括数据集处理、文本分析、分类以及分布式计算集群的实施，最终的技术实现是使用 Apache Spark 和 Scala 在 Linux 上构建分布式计算集群。

Abstract: This document reports the sequence of practices and methodologies implemented during the Big Data course. It details the workflow beginning with the processing of the Epsilon dataset through group and individual strategies, followed by text analysis and classification with RestMex and movie feature analysis with IMDb. Finally, it describes the technical implementation of a distributed computing cluster with Apache Spark on Linux using Scala.

</details>


### [11] [Making Wide Stripes Practical: Cascaded Parity LRCs for Efficient Repair and High Reliability](https://arxiv.org/abs/2512.10425)
*Fan Yu,Guodong Li,Si Wu,Weijun Fang,Sihuang Hu*

Main category: cs.DC

TL;DR: 该论文与图处理、MLIR、编译器或HLS无关。与DSL相关，因为纠删码（Erasure Coding）涉及到编码和解码的系统设计，可能需要特定的域特定语言来描述和实现复杂的编解码逻辑。该论文提出了级联校验LRCs（CP-LRCs），一个新型的宽条带纠删码家族，通过在局部和全局校验块之间建立结构化依赖，解决了现有LRCs在宽条带存储系统中修复效率低、可靠性下降的问题。CP-LRCs实现了带宽高效率的单节点和多节点修复，并在实际部署中显著减少了故障修复时间。


<details>
  <summary>Details</summary>
Motivation: 现有的局部可修复码（LRCs）在采用宽条带的大规模存储系统中存在局限性：局部组扩大导致单节点修复成本增加，多节点故障频繁触发昂贵的全局修复，且可靠性急剧下降。其根本原因在于局部和全局校验块是独立设计的，无法在修复过程中协同工作。

Method: 本文提出了一种新的宽条带LRC家族——级联校验LRCs（CP-LRCs），通过将一个全局校验块分解到所有局部校验块中，嵌入了奇偶校验块之间的结构化依赖关系。该方法提出了一种通用的系数生成框架，并开发了利用级联特性的修复算法，并以CP-Azure和CP-Uniform进行了实例演示。

Result: CP-LRCs在保持MDS级容错能力的同时，实现了低带宽的单节点和多节点修复。在阿里云上的评估显示，单节点故障的修复时间减少了高达41%，双节点故障的修复时间减少了26%。

Conclusion: CP-LRCs通过将全局校验块分解到所有局部校验块中，创建了级联校验组，在保持MDS级容错能力的同时，实现了低带宽的单节点和多节点修复。实验评估表明，CP-LRCs能够有效地减少修复时间，最高可达41%（单节点）和26%（双节点），使其成为解决宽条带下现有LRC局限性的有效方案。

Abstract: Erasure coding with wide stripes is increasingly adopted to reduce storage overhead in large-scale storage systems. However, existing Locally Repairable Codes (LRCs) exhibit structural limitations in this setting: inflated local groups increase single-node repair cost, multi-node failures frequently trigger expensive global repair, and reliability degrades sharply. We identify a key root cause: local and global parity blocks are designed independently, preventing them from cooperating during repair. We present Cascaded Parity LRCs (CP-LRCs), a new family of wide stripe LRCs that embed structured dependency between parity blocks by decomposing a global parity block across all local parity blocks. This creates a cascaded parity group that preserves MDS-level fault tolerance while enabling low-bandwidth single-node and multi-node repairs. We provide a general coefficient-generation framework, develop repair algorithms exploiting cascading, and instantiate the design with CP-Azure and CP-Uniform. Evaluations on Alibaba Cloud show reductions in repair time of up to 41% for single-node failures and 26% for two-node failures.

</details>


### [12] [Clustered Federated Learning with Hierarchical Knowledge Distillation](https://arxiv.org/abs/2512.10443)
*Sabtain Ahmad,Meerzhan Kanatbekova,Ivona Brandic,Atakan Aral*

Main category: cs.DC

TL;DR: This content has not passed the compliance test and has been hidden.


<details>
  <summary>Details</summary>
Motivation: 传统的集群联邦学习（CFL）方法存在学习碎片化的问题，即为每个集群训练独立的全局模型，未能利用集群的集体洞察力（collective cluster insights）。因此，本文提出转向分层 CFL，以提高训练效率并解决由此可能引入的通信挑战。

Method: 本文提出了一种名为 CFLHKD 的新型个性化方案，用于将分层集群知识整合到 CFL 中。CFLHKD 基于多教师知识蒸馏（multi-teacher knowledge distillation），实现了集群间的知识共享，同时保留了集群特定的个性化。它采用双层聚合（bi-level aggregation）来弥合本地学习和全局学习之间的差距。

Result: 在标准基准数据集上的广泛评估表明，CFLHKD 在集群特定和全局模型准确性方面优于具有代表性的基线方法，实现了 3.32% 至 7.57% 的性能改进。

Conclusion: CFLHKD通过双层聚合和多教师知识蒸馏，实现了联邦学习中客户端集群知识的有效共享和个性化，从而显著提升了模型性能。这一工作展现了通过分层和知识蒸馏来优化联邦学习中客户端异构性处理和模型性能的潜力。

Abstract: Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\%.

</details>


### [13] [ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp](https://arxiv.org/abs/2512.10576)
*Xinhang Chen,Chao Zhang,Jiahuan He,Wei Liu,Jianming Zhang,Wenlong Zhou,Xiao Li,Pai Zeng,Shiyong Li,Yuanpan Qian,Dong Li,Zhaogeng Li*

Main category: cs.DC

TL;DR: 该论文与 DSL、图处理、MLIR、编译器、HLS 均不直接相关。
太长不读：论文提出了一个名为 ESS（Extended Sparse Server）的卸载系统，用于解决 DeepSeek-V3.2-Exp 模型在长上下文推理时，因 Latent-Cache 占用过多 GPU 内存而导致的 Decode 阶段吞吐量瓶颈。ESS 通过将 Latent-Cache 选择性地卸载到 CPU 内存来释放 GPU 空间，从而在 Batch-size 受限时显著提升吞吐量。在 128K 上下文长度下，ESS 带来了高达 123% 的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: DeepSeek-V3.2-Exp 引入了稀疏注意力机制，显著降低了长上下文推理的延迟，但 PD（Pre-fill/Decode）解耦中的 Decode 阶段仍然是主要的瓶颈。这个瓶颈主要源于 Latent-Cache 随序列长度线性增长与有限的 GPU 内存容量之间的矛盾，这限制了可行的批处理大小，并抑制了 Decode 阶段的吞吐量。

Method: 提出并实现了 ESS（Extended Sparse Server）系统设计，这是一个以卸载为中心的系统，专门为 DeepSeek-V3.2-Exp 设计。ESS 的核心机制是选择性地将 Latent-Cache 卸载到 CPU 内存，同时将对延迟敏感的组件保留在 GPU 上。这种方法有效地解耦了批处理大小的扩展与 GPU 内存限制之间的关系，从而显著提高了 Decode 阶段的吞吐量。

Result: 高保真模拟结果显示，ESS 在 32K 上下文长度下提供了 69.4% 的吞吐量提升，在 128K 上下文长度下提供了高达 123% 的吞吐量提升。

Conclusion: ESS 极大提升了长上下文场景下 LLM 的服务吞吐量，尤其在上下文长度增加时表现更佳，成为长上下文 LLM 服务的一个实用且可扩展的解决方案。

Abstract: DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.
  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.
  Our high-fidelity simulations show that ESS delivers 69.4\% throughput improvement at 32K context length and up to 123\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [14] [Universal Hirschberg for Width Bounded Dynamic Programs](https://arxiv.org/abs/2512.10132)
*Logan Nye*

Main category: cs.DS

TL;DR: 本文与图处理和编译器相关，因为它讨论的是一种涉及动态规划（本质上是一种图处理）和算法优化的方法，这在编译器优化中是常见的。
Hirschberg算法的空间压缩思想可推广到广泛的具有局部依赖性的动态规划无环图（DP DAG）。在宽度$\omega$受限的DP DAG上，通过向前重新计算和递归树组织，实现了$O(\omega\log T + (\log T)^{O(1)})$的确定性回溯空间复杂度，表明空间高效回溯是这类DP DAG的结构特性。


<details>
  <summary>Details</summary>
Motivation: Hirschberg算法将最长公共子序列问题的空间复杂度从$O(N^2)$降低到了$O(N)$。本文的动机在于探究Hirschberg算法的底层思想是否可以推广到更广泛的具有局部依赖性的动态规划问题（DP DAG），从而在其他应用中实现空间高效的回溯。

Method: 本文将动态规划建模为在具有拓扑顺序、有限前沿宽度$\omega$和有界入度的DP DAG上的确定性时间演化，并假设使用带确定性破平局机制的最大型半环。然后，通过用仅向前重新计算取代向后动态规划，并将时间顺序组织成高度压缩的递归树，在$O(\omega\log T + (\log T)^{O(1)})$空间内实现确定性回溯。

Result: 证明了对于宽度为$\omega$、状态数为$T$的DP DAG，可以在$O(\omega\log T + (\log T)^{O(1)})$的空间复杂度下实现确定性回溯。这一框架适用于非对称和带状序列比对、一维递推以及在有界路径宽度图上的动态规划。同时，还表明在正向单遍模型中$\Omega(\omega)$的空间项（以比特计）是不可避免的，并讨论了在流式设置中推测的$\sqrt{T}$类型的障碍。

Conclusion: 本文表明，Hirschberg算法的基本思想可以推广到一类具有局部依赖性的定向无环图（DP DAG）上的动态规划问题。通过引入最大型半环和确定性打破平局的假设，证明了对于宽度为$\omega$的DP DAG，可以在$O(\omega\log T + (\log T)^{O(1)})$的空间复杂度下实现确定性回溯。这表明空间高效的回溯是宽度受限DP DAG的固有结构特性，而非网格算法的独有现象。

Abstract: Hirschberg's algorithm (1975) reduces the space complexity for the longest common subsequence problem from $O(N^2)$ to $O(N)$ via recursive midpoint bisection on a grid dynamic program (DP). We show that the underlying idea generalizes to a broad class of dynamic programs with local dependencies on directed acyclic graphs (DP DAGs). Modeling a DP as deterministic time evolution over a topologically ordered DAG with frontier width $ω$ and bounded in-degree, and assuming a max-type semiring with deterministic tie breaking, we prove that in a standard offline random-access model any such DP admits deterministic traceback in space $O(ω\log T + (\log T)^{O(1)})$ cells over a fixed finite alphabet, where $T$ is the number of states. Our construction replaces backward dynamic programs by forward-only recomputation and organizes the time order into a height-compressed recursion tree whose nodes expose small "middle frontiers'' across which every optimal path must pass. The framework yields near-optimal traceback bounds for asymmetric and banded sequence alignment, one-dimensional recurrences, and dynamic-programming formulations on graphs of bounded pathwidth. We also show that an $Ω(ω)$ space term (in bits) is unavoidable in forward single-pass models and discuss conjectured $\sqrt{T}$-type barriers in streaming settings, supporting the view that space-efficient traceback is a structural property of width-bounded DP DAGs rather than a peculiarity of grid-based algorithms.

</details>


### [15] [Approximate Counting in Local Lemma Regimes](https://arxiv.org/abs/2512.10134)
*Ryan L. Mann,Gabriel Waite*

Main category: cs.DS

TL;DR: 否，本论文内容与 DSL、图处理、MLIR、编译器或 HLS 无关。

太长不看：本研究基于簇展开（cluster expansion）方法，在局部引理机制下，为事件交集概率和子空间交集维度问题提供了高效的近似计数算法，包括针对可交换投影算子的完全多项式时间近似方案（FPTAS），以及针对一般投影算子在不同假设下的 FPTAS 或高效仿射近似。这些结果可应用于近似计算 CNF 公式的可满足赋值数和量子可满足性公式的可满足子空间维度。


<details>
  <summary>Details</summary>
Motivation: 在局部引理机制（local lemma regimes）下，需要为事件交集概率和子空间交集维度等自然问题建立高效的近似计数算法。

Method: 本文方法基于 cluster expansion（簇展开）方法，特别是对于可交换投影算子，获得 FPTAS；对于一般投影算子，在全局容斥稳定性条件下使用 FPTAS，在谱隙假设下使用高效仿射近似。

Result: 1. 对于可交换投影算子：事件交集概率和子空间交集维度的 FPTAS（完全多项式时间近似方案）。
2. 对于一般投影算子：
    a. 在全局容斥稳定性条件（global inclusion-exclusion stability condition）下，提供 FPTAS。
    b. 在谱隙假设（spectral gap assumption）下，提供高效仿射近似（efficient affine approximation）。
3. 推论：得到了近似计算合取范式（CNF）公式可满足赋值数和量子可满足性（quantum satisfiability）公式可满足子空间维度的高效算法。

Conclusion: 本文基于 cluster expansion 方法，为局部引理机制下的事件交集概率和子空间交集维度问题建立了高效的近似计数算法。针对可交换投影算子，提供了 FPTAS；针对一般投影算子，在全局容斥稳定性条件下提供了 FPTAS，在谱隙假设下提供了高效仿射近似。作为推论，这些算法可以高效地近似计算 CNF 公式的可满足赋值数和量子可满足性公式的可满足子空间维度。

Abstract: We establish efficient approximate counting algorithms for several natural problems in local lemma regimes. In particular, we consider the probability of intersection of events and the dimension of intersection of subspaces. Our approach is based on the cluster expansion method. We obtain fully polynomial-time approximation schemes for both the probability of intersection and the dimension of intersection for commuting projectors. For general projectors, we provide two algorithms: a fully polynomial-time approximation scheme under a global inclusion-exclusion stability condition, and an efficient affine approximation under a spectral gap assumption. As corollaries of our results, we obtain efficient algorithms for approximating the number of satisfying assignments of conjunctive normal form formulae and the dimension of satisfying subspaces of quantum satisfiability formulae.

</details>


### [16] [Semi-Robust Communication Complexity of Maximum Matching](https://arxiv.org/abs/2512.10532)
*Gabriel Cipriani Huete,Adithya Diddapur,Pavel Dvořák,Christian Konrad*

Main category: cs.DS

TL;DR: 该论文与DSL、图处理、MLIR、编译器或HLS无关。
太长不看（TLDR）：本文研究了在边集随机和对抗性混合划分的半鲁棒设置下，最大匹配问题的单向双边通信复杂度。研究发现一个简单协议（Alice只发送其边集的字典序优先最大匹配）在半鲁棒设置下能达到惊人的 $3/4$ 预期近似比（该分析是紧致的）。虽然这个简单协议在完全鲁棒设置下也能达到 $3/4$ 近似，但作者给出了一个实例，证明它不能超越现有完全鲁棒设置下基于边度约束子图的更复杂的 $5/6$ 最先进结果。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是研究在半鲁棒性设置下最大匹配问题的通信复杂性，并探索是否存在一个简单且有效的协议。半鲁棒性设置的难度不低于完全鲁棒性设置，而对于完全鲁棒性设置，现有最先进的协议是相对复杂的 $5/6$ 近似协议。研究人员希望通过引入一个简单协议（Alice仅通信一个词典序优先的最大匹配）来评估其惊人的有效性。

Method: 本文提出的方法是一个简单的单向通信协议：Alice将其边集的一个字典序优先的最大匹配发送给Bob。研究人员分析了这个简单协议在半鲁棒性设置下的性能，并证明它可以达到预期 $3/4$ 的近似比，且该分析是紧致的。同时，作者也将该协议应用于完全鲁棒性设置，并给出了一个实例表明该协议不能达到完全鲁棒性设置下的5/6最先进结果。

Result: 研究结果表明，在半鲁棒性设置中，所提出的简单协议可以达到预期 $3/4$ 的近似比，并且该 $3/4$ 的分析是紧致的。该协议也立即在完全鲁棒性设置中实现了 $3/4$ 的近似比。然而，研究人员通过给出一个实例证明，该简单协议在完全鲁棒性设置中仅能达到 $0.832 < 5/6 \approx 0.833$ 的近似比，故不能超越现有的 $5/6$ 最先进结果。

Conclusion: 本文研究了半鲁棒性设置下，Alice和Bob之间最大匹配问题的单向双边通信复杂度，并提出了一个简单的协议。研究表明，虽然提出的简单协议在半鲁棒和完全鲁棒设置中都能达到3/4的预期近似比，但它不能超越完全鲁棒设置中现有的5/6最先进结果。这一发现有助于理解不同鲁棒性设置下匹配问题的近似能力和通信效率。

Abstract: We study the one-way two-party communication complexity of Maximum Matching in the semi-robust setting where the edges of a maximum matching are randomly partitioned between Alice and Bob, but all remaining edges of the input graph are adversarially partitioned between the two parties.
  We show that the simple protocol where Alice solely communicates a lexicographically-first maximum matching of their edges to Bob is surprisingly powerful: We prove that it yields a $3/4$-approximation in expectation and that our analysis is tight.
  The semi-robust setting is at least as hard as the fully robust setting. In this setting, all edges of the input graph are randomly partitioned between Alice and Bob, and the state-of-the-art result is a fairly involved $5/6$-approximation protocol that is based on the computation of edge-degree constrained subgraphs [Azarmehr, Behnezhad, ICALP'23]. Our protocol also immediately yields a $3/4$-approximation in the fully robust setting. One may wonder whether an improved analysis of our protocol in the fully robust setting is possible: While we cannot rule this out, we give an instance where our protocol only achieves a $0.832 < 5/6 = 0.83$-approximation. Hence, while our simple protocol performs surprisingly well, it cannot be used to improve over the state-of-the-art in the fully robust setting.

</details>
