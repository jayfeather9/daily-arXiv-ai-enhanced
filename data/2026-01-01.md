<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Towards representation agnostic probabilistic programming](https://arxiv.org/abs/2512.23740)
*Ole Fenske,Maximilian Popko,Sebastian Bader,Thomas Kirste*

Main category: cs.PL

TL;DR: 该论文与DSL（概率编程语言）、编译器（指PPL工具链）和图处理（概率图模型）相关。它提出了一种因子抽象方法，用于解耦概率编程中的模型表示和推理算法。该抽象提供了一个通用接口，允许在统一的框架中混合使用不同的表示（如离散/高斯/采样），从而能够对现有工具难以处理的复杂混合离散-连续模型进行有效的推断。


<details>
  <summary>Details</summary>
Motivation: 现有的概率编程语言和工具（probabilistic programming languages and tools, PPLs）将模型表示与特定的推理算法紧密耦合。这种耦合阻碍了对新模型表示的探索，也难以有效处理混合离散-连续模型。因此，需要一种机制来解耦表示与推理，以提高灵活性和表达能力。

Method: 本文的核心方法是引入了一个具有五种基本操作的因子抽象（factor abstraction）。这个抽象充当了一个通用接口，使得不论底层表示如何（例如离散表、高斯分布或基于样本的方法），都可以对因子进行操作。这种抽象实现了表示无关性（representation-agnostic），从而允许在同一个框架中混合使用不同的模型表示。

Result: 通过引入因子抽象，本文成功构建了一个表示无关的（representation-agnostic）概率编程框架。这个框架允许用户在一个统一的系统中自由地混合和使用不同的模型表示（如离散表、高斯分布和基于样本的方法）。这使得对复杂混合模型的推断成为可能，解决了现有工具包无法充分表达和处理这些模型的局限性。

Conclusion: 本文提出了一种新的因子抽象方法，并展示了如何利用这一抽象实现一个与表示无关的概率编程框架。主要的结论是，通过将模型表示与推理算法解耦，该框架能够统一处理混合离散-连续模型，并且用户可以在一个统一的框架内自由混合不同的表示，从而解决现有工具包在处理复杂混合模型时的局限性。

Abstract: Current probabilistic programming languages and tools tightly couple model representations with specific inference algorithms, preventing experimentation with novel representations or mixed discrete-continuous models. We introduce a factor abstraction with five fundamental operations that serve as a universal interface for manipulating factors regardless of their underlying representation. This enables representation-agnostic probabilistic programming where users can freely mix different representations (e.g. discrete tables, Gaussians distributions, sample-based approaches) within a single unified framework, allowing practical inference in complex hybrid models that current toolkits cannot adequately express.

</details>


### [2] [VGC: A High-Performance Zone-Based Garbage Collector Architecture for Python with Partitioning and Parallel Execution](https://arxiv.org/abs/2512.23768)
*Abdulla M*

Main category: cs.PL

TL;DR: 该论文与编译器相关。它介绍了一种新的垃圾回收机制，其中包含编译时的静态内存分配优化方法（Passive VGC）。VGC（Virtual Garbage Collector）是一种新颖的内存管理框架，采用Active VGC（运行时并发标记-清除）和Passive VGC（编译时预测性内存映射）的双层架构来优化性能。它将暂停时间减少了高达30%，将总内存使用量减少了高达25%，并提高了内存密集型并行应用的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的内存管理和垃圾回收机制（如传统的分代收集器）在处理多样化系统（从资源受限的嵌入式设备到高性能并行架构）以及应对并行负载时，存在性能瓶颈、高开销和较长的暂停时间。VGC旨在提供一个高效、低开销的内存管理框架来解决这些问题，尤其是在多线程和内存密集型系统中。

Method: VGC 采用双层架构：
1. **Active VGC (运行时):** 使用并发标记-清除（concurrent mark and sweep）策略，针对并行负载进行优化，动态管理运行时对象，减少暂停时间。
2. **Passive VGC (编译时):** 通过预测性内存映射优化静态对象分配，将对象与缓存边界对齐，从而最大限度地减少内存碎片。
该方法通过分离职责，实现了可预测的内存访问模式。

Result: VGC 实现了显著的性能提升：
1. **减少暂停时间:** 在多线程基准测试中，与分代收集器相比，暂停时间减少了高达30%。
2. **减少内存使用:** 总内存使用量减少了高达25%。
3. **提高可扩展性:** 确保了现代并行应用的可扩展性。
4. **减少内存碎片:** 通过编译时的预测性内存映射优化，将碎片最小化。

Conclusion: VGC 通过结合编译时和运行时的优化，为内存密集型系统提供了一个强大且适应性强的内存管理解决方案，适用于低级和高级编程环境，能够提高并行应用的可扩展性。

Abstract: The Virtual Garbage Collector (VGC) introduces a novel memory management framework designed to optimize performance across diverse systems, ranging from resource constrained embedded devices to high performance parallel architectures. Unlike conventional garbage collectors, VGC employs a dual layer architecture consisting of Active VGC and Passive VGC to enable efficient, low overhead memory management. Active VGC dynamically manages runtime objects using a concurrent mark and sweep strategy tailored for parallel workloads, reducing pause times by up to 30 percent compared to generational collectors in multithreaded benchmarks. Passive VGC operates at compile time and optimizes static object allocation through predictive memory mapping, minimizing fragmentation by aligning objects to cache boundaries. This separation of responsibilities ensures predictable memory access patterns, reduces total memory usage by up to 25 percent, and improves scalability for modern parallel applications. By integrating compile time and runtime optimizations, VGC provides a robust and adaptable solution for memory intensive systems across both low level and high level programming environments.

</details>


### [3] [State Space Estimation for DPOR-based Model Checkers](https://arxiv.org/abs/2512.23996)
*A. R. Balasubramanian,Mohammad Hossein Khoshechin Jorshari,Rupak Majumdar,Umang Mathur,Minjian Zhang*

Main category: cs.PL

TL;DR: 该论文与**编译器（Compiler）** 领域的**模型检查（Model Checking）** 技术相关。模型检查是验证并发程序正确性的重要工具，本文工作聚焦于优化模型检查的资源分配和效率估计。

**TLDR:** 本文研究并发程序中Mazurkiewicz迹等价类数量的估计问题，该数量可用于指导基于枚举的模型检查的耗时和覆盖率。研究首先证明该计数问题是#P-hard且难以在理论上近似。然后，提出了一种基于DPOR算法和经典Knuth估计器的Monte Carlo多项式时间无偏估计器，并通过随机枚举来控制方差。实验证明，该方法在有限预算内能对大规模状态空间提供稳定且准确的估计，为模型检查资源的分配提供了首个有理论保障的高效工具。


<details>
  <summary>Details</summary>
Motivation: 1. **实际需求：** 在基于枚举的模型检查中，有两大实际问题需要解决：
    a. 估计模型检查运行可能需要多长时间（即时间成本）。
    b. 确定目前已覆盖的搜索空间比例。
2. **核心问题：** 这些问题都依赖于估计并发程序诱导的Mazurkiewicz迹等价类的数量。
3. **挑战与价值：** 尽管该计数问题具有重要性，但目前缺乏可用于有效分配模型检查资源的、具有理论保证的多项式时间无偏估计器。

Method: 1. 证明并发程序中Mazurkiewicz迹等价类的计数问题是#P-hard的，且在假设P≠NP的情况下，无法进行有效的近似。
2. 提出一种Monte Carlo方法来获得多项式时间无偏估计器：
    a. 将一个无状态（stateless）的最优DPOR（Dynamic Partial Order Reduction）算法转换为无偏估计器。
    b. 将DPOR的探索过程视为一个有限深度、有限宽度的树，其叶子是最大Mazurkiewicz迹。
    c. 在这棵树上运行经典的Knuth估计器以获得无偏估计。
    d. 应用随机枚举（stochastic enumeration）技术来控制方差，通过维护一个少量耦合的、每层深度的部分路径群体。
3. 扩展该机制以估计模型检查成本，方法是对所有已探索的图（不仅是完整的迹）进行加权。
4. 在JMC模型检查器中实现并评估了该估计器。

Result: 1. **理论结果：** 证明了对于受限程序而言，Mazurkiewicz迹等价类的计数问题是#P-hard的，并且在$P \neq NP$的假设下，无法在任何亚指数因子内近似。
2. **算法结果：** 提出了一种基于修改的DPOR和Knuth估计器、利用随机枚举来控制方差的Monte Carlo方法，得到了第一个有理论证明的多项式时间无偏估计器。
3. **实验结果：** 在共享内存基准测试中，使用JMC模型检查器对所提估计器进行了评估。在适度的预算内（仅数百次试验），即使状态空间包含$10^5$到$10^6$个迹类，估计器仍能提供稳定的估计，通常在20%的误差范围内。
4. **扩展应用：** 展示了如何使用相同的机制，通过对所有探索过的图进行加权，来估计模型检查的总成本。

Conclusion: 本文解决了并发程序中Mazurkiewicz迹等价类的数量估计问题，首次提供了有理论保证的多项式时间无偏估计器。该估计器基于修改后的DPOR算法，利用经典的Knuth估计器和随机枚举技术来控制方差。实验证明，该方法在有限预算内能提供稳定且准确的估计，对于分配模型检查资源具有重要的实践意义。

Abstract: We study the estimation problem for concurrent programs: given a bounded program $P$, estimate the number of Mazurkiewicz trace-equivalence classes induced by its interleavings. This quantity informs two practical questions for enumeration-based model checking: how long a model checking run is likely to take, and what fraction of the search space has been covered so far. We first show the counting problem is #P-hard even for restricted programs and, unless $P=NP$, inapproximable within any subexponential factor, ruling out efficient exact or randomized approximation algorithms. We give a Monte Carlo approach to obtain a poly-time unbiased estimator: we convert a stateless optimal DPOR algorithm into an unbiased estimator by viewing its exploration as a bounded-depth, bounded-width tree whose leaves are the maximal Mazurkiewicz traces. A classical estimator by Knuth, when run on this tree, yields an unbiased estimate. To control the variance, we apply stochastic enumeration by maintaining a small population of partial paths per depth whose evolution is coupled. We have implemented our estimator in the JMC model checker and evaluated it on shared-memory benchmarks. With modest budgets, our estimator yields stable estimates, typically within a 20% band, within a few hundred trials, even when the state space has $10^5$--$10^6$ classes. We also show how the same machinery estimates model-checking cost by weighting all explored graphs, not only complete traces. Our algorithms provide the first provable poly-time unbiased estimators for counting traces, a problem of considerable importance when allocating model checking resources.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [4] [Sparse Random Matrices for Dimensionality Reduction](https://arxiv.org/abs/2512.23756)
*Pierre Mackenzie*

Main category: cs.DS

TL;DR: 否，本论文与DSL、图处理、MLIR、编译器或HLS不直接相关。本文讨论的是降维技术（Johnson-Lindenstrauss 定理）中的随机矩阵结构，属于理论计算机科学/应用数学/机器学习的领域。

摘要：Johnson-Lindenstrauss (JL) 定理允许在高维空间中近似保持距离地将点集嵌入到低维空间。标准方法使用密集的 Gaussian 随机矩阵，但稀疏随机矩阵因能加速矩阵-向量乘法而更受青睐。本文概述并比较了 Achlioptas (2003) 和 Kane & Nelson (2014) 提出的稀疏 JL 构造的理论与证明，并通过实证将这些稀疏结构与标准 Gaussian JL 矩阵进行了性能比较。


<details>
  <summary>Details</summary>
Motivation: Johnson-Lindenstrauss (JL) 定理允许将高维数据嵌入到低维空间，同时近似保持成对距离。标准JL使用密集的高斯随机矩阵，但在某些应用中，为了加速矩阵-向量乘法，需要使用稀疏随机矩阵。本文的动机在于研究和比较这些稀疏JL矩阵的设计、证明及其在实践中的性能。

Method: 本文概述、分析并实现了两种稀疏Johnson-Lindenstrauss (JL) 嵌入结构：Achlioptas (2003) 的方法与 Kane 和 Nelson (2014) 的标准方法。通过实验比较这些稀疏结构与标准的高斯JL矩阵在距离保持方面的性能和矩阵向量乘法的速度。

Result: 本文概述了 Achlioptas 和 Kane & Nelson 的稀疏JL矩阵构造和证明，并进行了实现。实证比较了这些稀疏构造与标准的高斯JL矩阵在近似距离保持方面的效果和矩阵-向量乘积的效率。具体的性能数据和比较结果在文中有所体现。

Conclusion: 本文分析了Johnson-Lindenstrauss (JL)定理及其在降维中对距离保持的应用。重点比较了 Achlioptas 和 Kane & Nelson 提出的稀疏随机矩阵结构与标准的密集高斯JL矩阵。实证结果对稀疏结构在保持近似距离方面的表现与效率提供了洞察。

Abstract: The Johnson-Lindenstrauss (JL) theorem states that a set of points in high-dimensional space can be embedded into a lower-dimensional space while approximately preserving pairwise distances with high probability Johnson and Lindenstrauss (1984). The standard JL theorem uses dense random matrices with Gaussian entries. However, for some applications, sparse random matrices are preferred as they allow for faster matrix-vector multiplication. I outline the constructions and proofs introduced by Achlioptas (2003) and the contemporary standard by Kane and Nelson (2014). Further, I implement and empirically compare these sparse constructions with standard Gaussian JL matrices.

</details>


### [5] [Kidney Exchange: Faster Parameterized Algorithms and Tighter Lower Bounds](https://arxiv.org/abs/2512.24037)
*Aritra Banik,Sujoy Bhore,Palash Dey,Abhishek Sahu*

Main category: cs.DS

TL;DR: This paper is not related to DSL, graph processing (though it involves graph notions like treewidth/pathwidth, the core focus is not graph processing algorithms in the common sense), MLIR, compiler, or HLS. The paper focuses on improving algorithms and proving complexity for the Kidney Exchange Problem.
Too long; didn't read: The Kidney Exchange Problem is NP-complete, and the fastest deterministic FPT algorithm is $O^\star(14^t)$. This work improves the FPT algorithm to $O^\star((4e)^t) \approx O^\star(10.88^t)$. Furthermore, while the problem is W[1]-hard parameterized by treewidth, this paper proves it is also W[1]-hard parameterized by the pathwidth of the underlying graph, providing new parameterized intractability results.


<details>
  <summary>Details</summary>
Motivation: 肾脏交换机制允许不兼容的患者-供者对进行肾脏交换，但通常仅限于小循环。现实中存在利他供者，使得交换路径可以从利他供者开始。然而，肾脏交换的计算任务是 NP-完全的，因此需要设计更快的算法来克服计算障碍。当前最快的确定性 FPT 算法的时间复杂度是 $O^\star\left(14^t\right)$（其中 $t$ 是接受肾脏的患者数量），存在改进空间。此外，该问题在图的树宽（treewidth）参数化下是 W[1]-hard 的，但路径宽度（pathwidth）参数化下的计算难度尚不明确，需要进一步探讨。

Method: 本文采用了两个主要方法：首先，基于参数化复杂性理论，作者设计并提出了一个改进的确定性 FPT 算法，用于解决肾脏交换问题，该算法的运行时间复杂度有所降低。其次，通过证明肾脏交换问题在路径宽度参数化下是 W[1]-hard 的，来确定其在特定参数化条件下的计算不可解性，并给出了参数化不可解性的结果。

Result: 本文取得了两个主要结果：第一，提出了一个改进的确定性 FPT 算法，将肾脏交换问题的运行时间复杂度从 $O^\star\left(14^t\right)$ 提高到 $O^\star\left((4e)^t\right) \approx O^\star\left(10.88^t\right)$。第二，证明了肾脏交换问题在底层无向图的路径宽度参数化下是 W[1]-hard 的，从而否定了路径宽度参数化存在 FPT 算法的可能性，并给出了关于参数化不可解性的改进结果。

Conclusion: 本文提出了一个确定性的固定参数可解（FPT）算法，将肾脏交换问题的运行时间复杂度从 $O^\star\left(14^t\right)$ 改进到 $O^\star\left((4e)^t\right) \approx O^\star\left(10.88^t\right)$，其中 $t$ 是接受健康肾脏的患者数量。同时，本文通过证明该问题在底层无向图的路径宽度（pathwidth）参数化下是 W[1]-hard 的，从而否决了是否存在 FPT 算法的可能性，并对该问题在参数化复杂性框架下的不可解性给出了改进的认识。

Abstract: The kidney exchange mechanism allows many patient-donor pairs who are otherwise incompatible with each other to come together and exchange kidneys along a cycle. However, due to infrastructure and legal constraints, kidney exchange can only be performed in small cycles in practice. In reality, there are also some altruistic donors who do not have any paired patients. This allows us to also perform kidney exchange along paths that start from some altruistic donor. Unfortunately, the computational task is NP-complete. To overcome this computational barrier, an important line of research focuses on designing faster algorithms, both exact and using the framework of parameterized complexity.
  The standard parameter for the kidney exchange problem is the number $t$ of patients that receive a healthy kidney. The current fastest known deterministic FPT algorithm for this problem, parameterized by $t$, is $O^\star\left(14^t\right)$. In this work, we improve this by presenting a deterministic FPT algorithm that runs in time $O^\star\left((4e)^t\right)\approx O^\star\left(10.88^t\right)$. This problem is also known to be W[1]-hard parameterized by the treewidth of the underlying undirected graph. A natural question here is whether the kidney exchange problem admits an FPT algorithm parameterized by the pathwidth of the underlying undirected graph. We answer this negatively in this paper by proving that this problem is W[1]-hard parameterized by the pathwidth of the underlying undirected graph. We also present some parameterized intractability results improving the current understanding of the problem under the framework of parameterized complexity.

</details>


### [6] [Faster Algorithms for Global Minimum Vertex-Cut in Directed Graphs](https://arxiv.org/abs/2512.24355)
*Julia Chuzhoy,Ron Mosenzon,Ohad Trabelsi*

Main category: cs.DS

TL;DR: This paper is related to graph processing and compiler (as graph algorithms are fundamental to many compiler analyses and optimizations, though not directly focused on compiler in this abstract). The paper studies the directed global minimum vertex-cut problem and breaks the 29-year-old $\Theta(mn)$ time complexity barrier. It presents a randomized algorithm for the weighted version with $O\left(mn^{0.976}\cdot\operatorname{polylog} W\right)$ running time, and a randomized $O\left(\min\left\{m^{1+o(1)}\cdot k,n^{2+o(1)}\right\}\right)$-time algorithm for the unweighted version, significantly improving the state-of-the-art.


<details>
  <summary>Details</summary>
Motivation: 有向图全局最小顶点割问题是图论和算法设计中的一个基本且重要的研究课题。尽管其边切割的变体和无向图的变体在过去几十年中都有了显著的算法改进，但对于有向图的顶点割版本，目前最快的算法仍然是 Henzinger, Rao 和 Gabow 在 1996 年提出的 $\tilde{O}(mn)$ 时间复杂度的算法，这一状况已经持续了 29 年。因此，打破 $\Theta(mn)$ 的运行时间瓶颈，设计出更高效的算法，是当前该领域急需解决的科学问题。

Method: 本文通过设计和提出了一个新的随机化算法来解决有向图全局最小顶点割问题。对于加权版本，该算法的运行时间是 $O\left(mn^{0.976}\cdot\operatorname{polylog} W\right)$。对于无权版本，该算法的运行时间是 $O\left(\min\left\{m^{1+o(1)}\cdot k,n^{2+o(1)}\right\}\right)$，其中 $k$ 是最优解的值。通过这些新的算法，本文首次打破了该问题长达 29 年的 $\Theta(mn)$ 运行时间壁垒。尽管摘要中没有详细描述算法的具体机制，但其核心在于通过创新性的随机化方法，以更低的复杂度找到全局最小顶点割。

Result: 本文取得了两个主要结果：
1. **加权版本成果：** 提出了一个用于有向图全局最小顶点割的随机化算法，运行时间为 $O\left(mn^{0.976}\cdot\operatorname{polylog} W\right)$，其中 $W$ 是最大/最小顶点权重比。这一结果首次突破了该问题长达 29 年的 $\Theta(mn)$ 运行时间壁垒。
2. **无权版本成果：** 提出了一个用于无权有向图全局最小顶点割的随机化算法，运行时间为 $O\left(\min\left\{m^{1+o(1)}\cdot k,n^{2+o(1)}\right\}\right)$，其中 $k$ 是最优解的值。该算法优于现有最佳算法 $\tilde O\left(\min\left\{k^2 \cdot m, mn^{11/12+o(1)}, n^{2+o(1)}\right\}\right)$。

Conclusion: 本文首次突破了有向图全局最小顶点割问题的 $\Theta(mn)$ 运行时间瓶颈，提出了一个随机化算法，运行时间为 $O\left(mn^{0.976}\cdot\operatorname{polylog} W\right)$，其中 $W$ 为最大/最小顶点权重比。对于无权有向图全局最小顶点割问题，本文提供了一个运行时间为 $O\left(\min\left\{m^{1+o(1)}\cdot k,n^{2+o(1)}\right\}\right)$ 的随机化算法，与现有最佳算法相比也有显著提升。这些结果为解决图论中的基本问题提供了新的思路，并为后续更高效的算法研究奠定了基础。

Abstract: We study the directed global minimum vertex-cut problem: given a directed vertex-weighted graph $G$, compute a vertex-cut $(L,S,R)$ in $G$ of minimum value, which is defined to be the total weight of all vertices in $S$. The problem, together with its edge-based variant, is one of the most basic in graph theory and algorithms, and has been studied extensively. The fastest currently known algorithm for directed global minimum vertex-cut (Henzinger, Rao and Gabow, FOCS 1996 and J. Algorithms 2000) has running time $\tilde{O}(mn)$, where $m$ and $n$ denote the number of edges and vertices in the input graph, respectively. A long line of work over the past decades led to faster algorithms for other main versions of the problem, including the undirected edge-based setting (Karger, STOC 1996 and J. ACM 2000), directed edge-based setting (Cen et al., FOCS 2021), and undirected vertex-based setting (Chuzhoy and Trabelsi, STOC 2025). However, for the vertex-based version in directed graphs, the 29 year-old $\tilde{O}(mn)$-time algorithm of Henzinger, Rao and Gabow remains the state of the art to this day, in all edge-density regimes. In this paper we break the $Θ(mn)$ running time barrier for the first time, by providing a randomized algorithm for directed global minimum vertex-cut, with running time $O\left(mn^{0.976}\cdot\operatorname{polylog} W\right)$ where $W$ is the ratio of largest to smallest vertex weight. Additionally, we provide a randomized $O\left(\min\left\{m^{1+o(1)}\cdot k,n^{2+o(1)}\right\}\right)$-time algorithm for the unweighted version of directed global minimum vertex-cut, where $k$ is the value of the optimal solution. The best previous algorithm for the problem achieved running time $\tilde O\left(\min\left\{k^2 \cdot m, mn^{11/12+o(1)}, n^{2+o(1)}\right\}\right)$ (Forster et al., SODA 2020, Li et al., STOC 2021).

</details>


### [7] [Approximations for the Weighted Reversal, Transposition, and Indel Distance Problem with Intergenic Region Information](https://arxiv.org/abs/2512.25016)
*Gabriel Siqueira,Alexsandro Oliveira Alexandrino,Zanoni Dias*

Main category: cs.DS

TL;DR: 该论文与（和其每个部分） **图处理** 有关，因为它提出了一种名为“带标签的基因间断点图”（Labeled Intergenic Breakpoint Graph）的数据结构来解决基因组重排距离问题。
**太长不看（TLDR）**: 本文提出了一个考虑基因序列、方向和基因间区域长度的基因组模型，研究了“带权逆转、转座和插入缺失距离”问题，该问题寻求通过逆转、转座和插入缺失操作将一个基因组转换为另一个的最小成本序列。研究利用“带标签的基因间断点图”结构，提出了一种算法，该算法在特定权重设置下能够保证提供近似解。


<details>
  <summary>Details</summary>
Motivation: 基因组重排距离是基因组比较中的既定方法。现有研究通常会考虑各种重排操作、基因方向信息、基因间区域的核苷酸数量以及反映预期操作频率的权重。本文的动机是建立一个更精细的基因组模型：它包含每个基因至多一个拷贝，考虑了基因序列（带方向）并根据核苷酸长度表示基因间区域。在这一模型下，寻求找出最小成本序列来解释基因组间的演化转换。

Method: 本文研究了“带权逆转、转座和插入缺失距离”（Weighted Reversal, Transposition, and Indel Distance）问题。该问题旨在找到将一个基因组转换到另一个基因组所需的、由逆转（reversals）、转座（transposition）和插入缺失（indels）操作组成的最小成本序列。研究利用了一种称为“带标签的基因间断点图”（Labeled Intergenic Breakpoint Graph）的结构，并基于此结构提出了一种算法，该算法在考虑某些操作权重集合时，能够提供有保证的近似解。

Result: 研究针对所提出的“带权逆转、转座和插入缺失距离”问题提出了一种算法。通过利用“带标签的基因间断点图”结构，该算法在考虑特定的操作权重集合时，能够保证产生近似解（Guaranteed Approximations）。这为解决考虑了基因间区域长度和不同操作成本的基因组比较问题提供了有效的计算方法。

Conclusion: 本文研究了带权基因组重排距离问题，该距离是基因组比较中的一种重要方法。研究提出了一种考虑基因序列、方向和基因间区域核苷酸长度的基因组模型，并提出了一种基于“带标签的基因间断点图”的算法，在特定权重设置下，该算法能够保证近似结果。这推动了在保持精确度或可接受的近似度下，对考虑了基因间区域长度和不同操作成本的基因组比较问题的解决。

Abstract: Genome rearrangement distances are an established method in genome comparison. Works in this area may include various rearrangement operations representing large-scale mutations, gene orientation information, the number of nucleotides in intergenic regions, and weights reflecting the expected frequency of each operation. In this article, we model genomes containing at most one copy of each gene by considering gene sequences, with orientations, and representing intergenic regions according to their nucleotide lengths. We looked at a problem called Weighted Reversal, Transposition, and Indel Distance, which seeks the minimal cost sequence composed by the rearrangement operations of reversals, transposition, and indels, capable of transforming one genome into another. We leverage a structure called Labeled Intergenic Breakpoint Graph to show an algorithm for that problem with guaranteed approximations considering some sets of weights for the operations.

</details>


### [8] [Approximation Algorithms for Fair Repetitive Scheduling](https://arxiv.org/abs/2512.25020)
*Danny Hermelin,Danny Segev,Dvir Shabtay*

Main category: cs.DS

TL;DR: 这篇论文与DSL、图处理、MLIR、编译器、HLS均不直接相关。

论文研究了一个公平重复调度问题，目标是最小化任何客户的最大总完成时间。该问题在限制性设置下也是NP-难的。作者主要贡献是设计了两种设置下的近似算法：对于日期依赖的处理时间，提出了基于LP的 $2$-近似算法和针对常数天数的 PTAS；对于日期不变的处理时间，提出了一个简单的 $(\frac{1+\sqrt{2}}{2}+ε)$-近似算法和针对任意天数的 QP-TAS。关键技术依赖于新颖的批处理技术，以及新的下界机制。


<details>
  <summary>Details</summary>
Motivation: 现有的公平重复调度问题研究表明，即使在限制性设置下，该问题也是NP-难的，且先前的工作仅针对高度结构化的特殊情况提供了精确解法。因此，这篇论文的动机是设计具有可证明性能保证的近似算法，以解决更一般情况下的问题。该问题的目标是最小化任何客户经历的最大总完成时间。

Method: 论文设计了具有可证明性能保证的近似算法来解决公平重复调度问题。方法包括：1. 针对作业处理时间随日期变化的情况，提出了基于LP的多项式时间 $2$-近似算法，以及针对常数天数的 PTAS。2. 针对作业处理时间不随日期变化的情况，提出了多项式时间的 $(\frac{1+\sqrt{2}}{2}+ε)$-近似算法，以及针对任意天数的 QP-TAS。3. 关键技术组件是新颖的批处理技术，用于构建低维动态规划或紧凑配置LP，同时提出了多种下界机制。

Result: 论文取得了以下成果：1. 当作业处理时间随日期变化时，提出了基于LP的多项式时间 $2$-近似算法，以及针对常数天数的常数时间近似方案（PTAS）。2. 当作业处理时间不随日期变化时，提出了多项式时间的 $(\frac{1+\sqrt{2}}{2}+ε)$-近似算法，以及针对任意天数的准多项式时间近似方案（QP-TAS）。3. 提出了新的批处理技术，用于引导构建低维动态规划或紧凑配置LP。4. 提出了多种可能具有更广泛价值的下界机制。

Conclusion: 这篇论文设计了具有可证明性能保证的近似算法来解决公平重复调度问题。主要结论包括针对作业处理时间随日期变化和不变化两种情况的近似算法，并提出了新颖的批处理技术和下界机制。

Abstract: We consider a recently introduced fair repetitive scheduling problem involving a set of clients, each asking for their associated job to be daily scheduled on a single machine across a finite planning horizon. The goal is to determine a job processing permutation for each day, aiming to minimize the maximum total completion time experienced by any client. This problem is known to be NP-hard for quite restrictive settings, with previous work offering exact solution methods for highly-structured special cases.
  In this paper, we focus on the design of approximation algorithms with provable performance guarantees. Our main contributions can be briefly summarized as follows:
  (i) When job processing times are day-dependent, we devise a polynomial-time LP-based $2$-approximation, as well as a polynomial-time approximation scheme for a constant number of days.
  (ii) With day-invariant processing times, we obtain a surprisingly simple $(\frac{1+\sqrt{2}}{2}+ε)$-approximation in polynomial time. This setting is also shown to admit a quasi-polynomial-time approximation scheme for an arbitrary number of days.
  The key technical component driving our approximation schemes is a novel batching technique, where jobs are conceptually grouped into batches, subsequently leading either to a low-dimensional dynamic program or to a compact configuration LP. Concurrently, while developing our constant-factor approximations, we propose a host of lower-bounding mechanisms that may be of broader interest.

</details>


### [9] [EF(X) Orientations: A Parameterized Complexity Perspective](https://arxiv.org/abs/2512.25033)
*Sotiris Kanellopoulos,Edouard Nemery,Christos Pergaminelis,Minas Marios Sotiriou,Manolis Vasilakis*

Main category: cs.DS

TL;DR: 不是（不相关）。本文研究图中的公平定向问题，特别是无嫉妒（EF）定向的参数化复杂性。


<details>
  <summary>Details</summary>
Motivation: 图形中的公平定向是受公平分配情景启发的，其中资源仅由相邻的参与者竞争。资源（边）必须分配给其相邻的参与者（顶点）之一。虽然EFX定向（无嫉妒直到任何商品）已被广泛研究，但EF定向（无嫉妒）在现有工作中仍未被探索，因此作者旨在对EF定向进行系统的研究。

Method: 本文研究了EF定向的参数化复杂性，包括各种可追踪案例、难解性结果和参数化。研究对象涵盖了简单图和多重图。作者还研究了在保留公平的条件下，可以从图中移除的最少边的数量，即“慈善”度量。

Result: 研究结果主要集中在EF定向的参数化复杂性分析上，包括在简单图和多重图上的可追踪案例、难解性结果和参数化。许多结果也适用于EFX定向，补充和改进了先前的工作，并回答了关于多项式有界估值图上EFX定向的结构参数化复杂性的一个开放问题。此外，作者展示了在某些情况下（如二元估值），EF定向比EFX定向更易处理。最后，提出了寻找最小需要移除的边集以实现EF(X)定向的算法。

Conclusion: 本文研究了图中的公平定向问题，特别是EF（无嫉妒）定向。研究结果包括EF定向在简单图和多重图上的参数化复杂性分析，许多结果也适用于EFX（无嫉妒直到任何商品），补充和改进了现有工作。特别地，回答了关于多项式有界估值图上EFX定向的结构参数化复杂性问题。此外，作者还提出了处理二元估值EF定向问题比EFX定向更容易的案例，并研究了在定向设置中移除最少边以实现EF(X)定向的慈善问题。这为公平分配模型中的无嫉妒定向提供了一个全面的复杂性分析和可追踪性研究。

Abstract: The concept of fair orientations in graphs was introduced by Christodoulou, Fiat, Koutsoupias, and Sgouritsa in 2023, naturally modeling fair division scenarios in which resources are only contested by neighbors. In this model, vertices represent agents and undirected edges represent goods; edges have to be oriented towards one of their endpoints, i.e., allocated to one of their adjacent agents. Although EFX orientations (envy-free up to any good) have been extensively studied in this setting, EF orientations (envy-free) remain unexplored. In this work, we initiate their study, mostly under the lens of parameterized complexity, presenting various tractable cases, hardness results, and parameterizations. Our results concern both simple graphs and multigraphs. Interestingly, many of our results transfer to EFX orientations, thus complementing and improving upon previous work; notably, we answer an open question regarding the structural parameterized complexity of the latter problem on graphs of polynomially-bounded valuations. We also show that EF orientations are tractable in cases in which EFX orientations are not, particularly for binary valuations. Lastly, we consider charity in the orientation setting, establishing algorithms for finding the minimum amount of edges that have to be removed from a graph in order for EF(X) orientations to exist.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [HERO-Sign: Hierarchical Tuning and Efficient Compiler-Time GPU Optimizations for SPHINCS+ Signature Generation](https://arxiv.org/abs/2512.23969)
*Yaoyun Zhou,Qian Wang*

Main category: cs.AR

TL;DR: This paper is related to compiler because it proposes a compiler time optimization "adaptive compilation strategy" and a "Tree Tuning search algorithm" which adapts fusion schemes to different GPU architectures for SPHINCS+ kernels. The paper is also related to cryptography and security optimization/acceleration.

SPHINCS+ 是一种慢速的后量子签名方案，该论文提出了 HERO Sign，一个 GPU 加速实现，通过层次调优、Tree Fusion 策略和自适应编译策略来充分挖掘 SPHINCS+ 的并行性并优化其计算内核。实验结果显示，HERO Sign 在不同参数集和 GPU 架构上实现了显著的吞吐量提升（最高达 3.13 倍）和两个数量级的内核启动延迟降低。


<details>
  <summary>Details</summary>
Motivation: SPHINCS+是一种提供强后量子安全性的无状态基于哈希的签名方案，但其签名生成过程由于大量的哈希计算而运行缓慢。GPU提供大规模并行性，有潜力加速SPHINCS+签名，然而，现有的GPU优化未能充分利用SPHINCS+ Merkle树结构的固有并行性，或者缺乏针对其多样化计算内核的细粒度、编译器级别的定制，这是该研究的主要动机。

Method: 该文提出了一种名为HERO Sign的GPU加速SPHINCS+实现方案。其采用的主要方法包括：重新审视SPHINCS+组件（FORS, MSS, WOTS+）中的数据独立性以发掘并行性；针对FORS引入了一种Tree Fusion策略，并通过自动化的Tree Tuning搜索算法来指导和适应不同GPU架构；采用自适应编译策略，根据不同SPHINCS+内核的特点自动选择PTX或本地代码路径以最大化效率；为批量签名生成，使用基于任务图的构建优化内核级别的重叠，以减少多流空闲时间和内核启动开销。

Result: 实验结果表明，与最先进的GPU实现相比，HERO Sign在RTX 4090上的SPHINCS+ 128f、192f和256f参数集下，吞吐量分别提高了1.28-3.13倍、1.28-2.92倍和1.24-2.60倍。在A100、H100和GTX 2080上也观察到类似的性能提升，并且内核启动延迟降低了两个数量级。

Conclusion: HERO Sign通过层次优化和高效的编译时优化，实现了SPHINCS+签名方案在GPU上的加速。它提出的Tree Fusion策略和自适应编译策略显著提高了签名吞吐量并减少了内核启动延迟。这些改进使得SPHINCS+在保持后量子安全性的同时，更具实用性。

Abstract: SPHINCS+ is a stateless hash-based signature scheme that provides strong post quantum security, but its signature generation is slow due to intensive hash computations. GPUs offer massive parallelism that can potentially accelerate SPHINCS+ signatures. However, existing GPU-based optimizations either fail to fully exploit the inherent parallelism of SPHINCS+'s Merkle tree structure or lack fine-grained, compiler-level customization across its diverse computational kernels. This paper proposes HERO Sign, a GPU-accelerated SPHINCS+ implementation that adopts hierarchical tuning and efficient compiler time optimizations. HERO Sign reexamines the parallelization opportunities enabled by data independence across SPHINCS+ components, including FORS, MSS, and WOTS+. It introduces a Tree Fusion strategy for FORS, which contains a large number of independent branches. The fusion strategy is guided by an automated Tree Tuning search algorithm that adapts fusion schemes to different GPU architectures. To further improve performance, HERO Sign employs an adaptive compilation strategy that accounts for the varying effectiveness of compiler optimizations across SPHINCS+ kernels such as FORS Sign, TREE Sign, and WOTS+ Sign. During compilation, the strategy automatically selects between PTX and native code paths to maximize efficiency. For batched signature generation, HERO Sign optimizes kernel-level overlapping using a task graph-based construction to reduce multi-stream idle time and kernel launch overhead. Experimental results show that, compared to state of the art GPU implementations, HERO Sign achieves throughput improvements of 1.28-3.13, 1.28-2.92, and 1.24-2.60 under the SPHINCS+ 128f, 192f, and 256f parameter sets on RTX 4090. Similar gains are observed on A100, H100, and GTX 2080, along with a two orders of magnitude reduction in kernel launch latency.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [11] [Governing Cloud Data Pipelines with Agentic AI](https://arxiv.org/abs/2512.23737)
*Aswathnarayan Muthukrishnan Kirubakaran,Adithya Parthasarathy,Nitin Saksena,Ram Sekhar Bodala,Akshay Deshpande,Suhas Malempati,Shiva Carimireddy,Abhirup Mazumder*

Main category: cs.DC

TL;DR: 该论文与编译器、DSL、图处理、MLIR、HLS等**不相关**。
**太长不看(TLDR)总结:** 本文提出了 Agentic Cloud Data Engineering 平台，这是一个将受策略约束的AI智能体整合到云数据管道治理中的控制架构。通过智能体分析遥测和元数据，并根据成本和合规策略采取行动（如自动恢复和资源配置），该平台将平均恢复时间最多减少45%，运营成本降低25%，手动干预减少70%以上，证明了这种受策略约束的智能体控制是管理企业云数据管道的有效方法。


<details>
  <summary>Details</summary>
Motivation: 云数据管道目前面临动态工作负载、不断变化的模式、成本约束和严格的治理要求。尽管云原生编排框架有所进步，但大多数生产管道仍依赖静态配置和被动的操作实践，导致恢复时间长、资源利用效率低和手动开销高。为解决这些问题，本文旨在引入智能体（agentic）控制来改进云数据管道的治理和运营效率。

Method: 本文提出了“Agentic Cloud Data Engineering”这一政策感知控制架构，它将有边界的AI智能体整合到云数据管道的治理和控制平面中。在Agentic Cloud Data Engineering平台中，专用智能体分析管道遥测数据和元数据，根据声明性的成本和合规策略进行推理，并提出受约束的操作行动（如自适应资源重新配置、模式协调和自动化故障恢复）。所有智能体行动都会根据治理策略进行验证，以确保行为的可预测性和可审计性。

Result: Agentic Cloud Data Engineering平台在代表性的批处理和流分析工作负载上的评估结果显示：与静态编排相比，它将平均管道恢复时间最多减少了45%，运营成本降低了约25%，手动干预事件减少了70%以上，同时保持了数据新鲜度和策略合规性。

Conclusion: Agentic Cloud Data Engineering平台通过引入受策略约束的智能体控制，为企业环境中的云数据管道提供了一种实用且有效的方法，实现了更快的恢复时间、更低的运营成本和更少的干预，同时保证了数据的新鲜度和策略合规性。

Abstract: Cloud data pipelines increasingly operate under dynamic workloads, evolving schemas, cost constraints, and strict governance requirements. Despite advances in cloud-native orchestration frameworks, most production pipelines rely on static configurations and reactive operational practices, resulting in prolonged recovery times, inefficient resource utilization, and high manual overhead. This paper presents Agentic Cloud Data Engineering, a policy-aware control architecture that integrates bounded AI agents into the governance and control plane of cloud data pipelines. In Agentic Cloud Data Engineering platform, specialized agents analyze pipeline telemetry and metadata, reason over declarative cost and compliance policies, and propose constrained operational actions such as adaptive resource reconfiguration, schema reconciliation, and automated failure recovery. All agent actions are validated against governance policies to ensure predictable and auditable behavior. We evaluate Agentic Cloud Data Engineering platform using representative batch and streaming analytics workloads constructed from public enterprise-style datasets. Experimental results show that Agentic Cloud Data Engineering platform reduces mean pipeline recovery time by up to 45%, lowers operational cost by approximately 25%, and decreases manual intervention events by over 70% compared to static orchestration, while maintaining data freshness and policy compliance. These results demonstrate that policy-bounded agentic control provides an effective and practical approach for governing cloud data pipelines in enterprise environments.

</details>


### [12] [Squeezing Edge Performance: A Sensitivity-Aware Container Management for Heterogeneous Tasks](https://arxiv.org/abs/2512.23952)
*Yongmin Zhang,Pengyu Huang,Mingyi Dong,Jing Yao*

Main category: cs.DC

TL;DR: 该论文与编译器、HLS、MLIR、DSL 或者图处理不直接相关，但它属于**编译/系统/调度**领域相关的“资源管理和优化”范畴。

该论文提出了一种测量驱动的、基于容器的资源管理框架 CRMS，用于解决边缘服务器上托管多个异构应用的节点内资源分配优化问题。通过性能剖析建立了一个 CPU/内存分配与延迟关系的非线性模型，并据此将最小化系统延迟和功耗的目标建模为 NP-难的 MINLP 问题。CRMS 通过将问题分解并采用结合凸优化和贪婪细化的两阶段方法来解决，实现了多项式时间复杂度。仿真结果表明，CRMS 相较于基线方法能将延迟降低 14% 以上，并提高能效，为动态异构边缘环境提供了一个实用的解决方案。


<details>
  <summary>Details</summary>
Motivation: 边缘计算（Edge Computing）环境中的任务异构性和有限资源对高效的资源调度构成了重大挑战。在单个边缘服务器上，需要一种有效的**节点内优化**框架，来管理托管的多个异构应用的资源，以最小化系统延迟和功耗。传统的启发式或基于搜索的基线方法往往效率不足。

Method: 1. **测量驱动的模型构建**：通过广泛的性能剖析实验，导出了一个非线性拟合模型，用于表征 CPU/内存分配与处理延迟之间的关系。 2. **问题建模**：利用该性能模型和基于排队的延迟公式，将联合最小化系统延迟和功耗的问题建模为一个混合整数非线性规划（MINLP）问题，并证明其为 NP-难问题。 3. **问题分解与求解**：将复杂的 MINLP 问题分解为可处理的凸子问题，并通过一个两阶段的容器化资源管理方案（CRMS）来解决，该方案结合了**凸优化**（用于资源分配）和**贪婪细化**（用于优化和调整）。 4. **实现和验证**：该方案实现了多项式时间复杂度和在全局资源约束下的准动态执行。通过仿真，验证了 CRMS 的性能。

Result: 1. **性能模型**：建立了一个可靠的非线性拟合模型，能够根据不同的配置（CPU/内存分配）可靠地估计不同工作负载下的性能（处理延迟），为后续优化提供了量化支持。 2. **优化效果**：仿真结果表明，与启发式和基于搜索的基线方法相比，CRMS 方案将系统延迟**降低了超过 14%**，并提高了**能效**。 3. **复杂度和可行性**：该方案实现了**多项式时间复杂度**，并支持在全局资源约束下的**准动态执行**，表明其具有实用性和可扩展性。

Conclusion: 本文提出了一种两阶段的容器化资源管理方案 CRMS，它结合了凸优化和贪婪细化，旨在解决边缘计算环境中多异构应用的资源分配问题，以最小化系统延迟和功耗。该方案在多项式时间内可解，并在仿真中表现出优于基线方法的性能，为动态工作负载下的异构边缘环境提供了一个实用且可扩展的解决方案。

Abstract: Edge computing enables latency-critical applications to process data close to end devices, yet task heterogeneity and limited resources pose significant challenges to efficient orchestration. This paper presents a measurement-driven, container-based resource management framework for intra-node optimization on a single edge server hosting multiple heterogeneous applications. Extensive profiling experiments are conducted to derive a nonlinear fitting model that characterizes the relationship among CPU/memory allocations and processing latency across diverse workloads, enabling reliable estimation of performance under varying configurations and providing quantitative support for subsequent optimization. Using this model and a queueing-based delay formulation, we formulate a mixed-integer nonlinear programming (MINLP) problem to jointly minimize system latency and power consumption, which is shown to be NP-hard. The problem is decomposed into tractable convex subproblems and solved through a two-stage container-based resource management scheme (CRMS) combining convex optimization and greedy refinement. The proposed scheme achieves polynomial-time complexity and supports quasi-dynamic execution under global resource constraints. Simulation results demonstrate that CRMS reduces latency by over 14\% and improves energy efficiency compared with heuristic and search-based baselines, offering a practical and scalable solution for heterogeneous edge environments with dynamic workload characteristics.

</details>


### [13] [PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression](https://arxiv.org/abs/2512.24449)
*Bo Jiang,Taolue Yang,Youyuan Liu,Xubin He,Sheng Di,Sian Jin*

Main category: cs.DC

TL;DR: 涉及到的领域：编译器, DSL, 图处理。
过长的摘要：PackKV是一个用于优化长上下文生成的通用高效KV缓存管理框架。它引入了专门针对KV缓存数据的有损压缩技术，结合了压缩算法和系统架构的协同设计。实验结果表明，在相同的最小精度损失下，PackKV相比现有量化方法，K缓存和V缓存的内存减少率分别平均提高了 $\mathbf{153.2}\%$ 和 $\mathbf{179.6}\%$。此外，PackKV还显著提高了执行吞吐量（K提高 $\mathbf{75.7}\%$，V提高 $\mathbf{171.7}\%$），有效地避免了解压缩开销，并加速了矩阵-向量乘法操作。


<details>
  <summary>Details</summary>
Motivation: Transformer-based大型语言模型（LLMs）在广泛的实际应用中展现了巨大的潜力。然而，长上下文推理仍然是一个重大挑战，原因是键值（KV）缓存的内存需求巨大，随着序列长度和批次大小的增加，可以扩展到数千兆字节。

Method: 本文提出了PackKV，这是一个通用的、高效的KV缓存管理框架，专为长上下文生成优化。PackKV引入了针对KV缓存数据特性的新型有损压缩技术，并且仔细地协同设计了压缩算法和系统架构。该方法与KV缓存动态增长的性质兼容，同时保持了高计算效率。

Result: 在相同的最小精度下降（与最先进的量化方法相比）下，PackKV实现了平均高出 $\mathbf{153.2}\%$ 的K缓存内存减少率和高出 $\mathbf{179.6}\%$ 的V缓存内存减少率。此外，PackKV提供了极高的执行吞吐量，有效地消除了解压缩开销，并加速了矩阵-向量乘法操作。具体来说，与cuBLAS矩阵-向量乘法内核相比，PackKV在A100和RTX Pro 6000 GPU上，K的平均吞吐量提高了 $\mathbf{75.7}\%$，V的平均吞吐量提高了 $\mathbf{171.7}\%$，同时对GPU内存带宽的要求更低。

Conclusion: PackKV是一个通用的、高效的KV缓存管理框架，它通过针对KV缓存数据特性设计的新的有损压缩技术，解决了长上下文推理中KV缓存内存需求大的问题。该框架在实现显著的内存节省的同时，保持了高计算效率，并有效提高了执行吞吐量，超越了现有的量化方法和cuBLAS内核。

Abstract: Transformer-based large language models (LLMs) have demonstrated remarkable potential across a wide range of practical applications. However, long-context inference remains a significant challenge due to the substantial memory requirements of the key-value (KV) cache, which can scale to several gigabytes as sequence length and batch size increase. In this paper, we present \textbf{PackKV}, a generic and efficient KV cache management framework optimized for long-context generation. %, which synergistically supports both latency-critical and throughput-critical inference scenarios. PackKV introduces novel lossy compression techniques specifically tailored to the characteristics of KV cache data, featuring a careful co-design of compression algorithms and system architecture. Our approach is compatible with the dynamically growing nature of the KV cache while preserving high computational efficiency. Experimental results show that, under the same and minimum accuracy drop as state-of-the-art quantization methods, PackKV achieves, on average, \textbf{153.2}\% higher memory reduction rate for the K cache and \textbf{179.6}\% for the V cache. Furthermore, PackKV delivers extremely high execution throughput, effectively eliminating decompression overhead and accelerating the matrix-vector multiplication operation. Specifically, PackKV achieves an average throughput improvement of \textbf{75.7}\% for K and \textbf{171.7}\% for V across A100 and RTX Pro 6000 GPUs, compared to cuBLAS matrix-vector multiplication kernels, while demanding less GPU memory bandwidth. Code available on https://github.com/BoJiang03/PackKV

</details>


### [14] [Understanding LLM Checkpoint/Restore I/O Strategies and Patterns](https://arxiv.org/abs/2512.24511)
*Mikaila J. Gossman,Avinash Maurya,Bogdan Nicolae,Jon C. Calhoun*

Main category: cs.DC

TL;DR: 是，这篇论文与 **编译器**（I/O 优化，性能工程），**图处理**（涉及 I/O 栈优化，虽然主题不是图，但其 I/O 优化对图处理的检查点也适用）以及 **DSL**（虽然没有提及具体的 DSL，但高性能 I/O 库 \texttt{liburing} 的使用和优化方法属于系统级性能优化的范畴）相关。
**太长不看（TLDR）:** 随着大规模语言模型 (LLM) 检查点成为一个“大数据 I/O 问题”，本文研究了内核加速 I/O 库 \texttt{liburing} 在 LLM 检查点中的有效性。通过微基准测试发现，未合并的小缓冲区操作是性能瓶颈。作者开发了一种文件系统感知的聚合和合并策略，将 LLM 检查点写入吞吐量提高到比 DataStates-LLM 高 3.9 倍，比 TorchSnapshot 高 7.6 倍，强调 I/O 聚合和对齐对现代存储栈的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）和基础模型的规模不断扩大，检查点/恢复（checkpoint/restore）成为了训练和推理的关键环节。由于涉及到三维并行（张量、管道、数据），检查点过程涉及大量进程和张量，需要频繁地持久化到稳定存储（如并行文件系统）。这个过程演变成一个特征为“大数据 I/O 问题”，即数据量（volume）、种类（variety）和速度（velocity）都非常大。
I/O 工作流必须遍历从 GPU 内存到主机内存、本地存储再到外部存储库的整个存储栈。存储栈的各层之间性能差异巨大（相差数量级），即使采用异步刷新/预取也会在并发下造成瓶颈。
现有的 I/O 机制（如 POSIX）可能效率不足，而像 \texttt{liburing} 这样的内核加速 I/O 库的有效性尚未在 LLM 检查点场景中得到充分探索。
因此，研究动机在于：量化 \texttt{liburing} 的潜在效益；探索 I/O 策略（如聚合、对齐和合并）如何影响性能；并开发出一种能够显著提高 LLM 检查点 I/O 吞吐量的策略，以解决现有方案的瓶颈。

Method: 这篇论文的方法是：
1. **识别问题：** 确定 LLM 检查点/恢复是一个“大数据 I/O 问题”，其特点是数据量大、种类多、速度快，且涉及多层存储栈的性能瓶颈。
2. **选择工具：** 选用内核加速的 I/O 库 \texttt{liburing} 作为潜在的缓解方案，因为其在 LLM 检查点中的有效性尚未被充分探索。
3. **微基准测试：** 开发微基准测试来量化使用 \texttt{liburing} 时的权衡，评估聚合、对齐和 I/O 合并（coalescing）在缓冲 I/O 和直接 I/O 下的相互作用。
4. **发现关键瓶颈：** 发现未合并的小缓冲区操作会导致吞吐量减半，而文件系统感知的聚合可以恢复带宽并减少元数据开销。
5. **比较评估：** 将提出的方法（结合了 \texttt{liburing} 和聚合/合并策略）与现有最先进的 LLM 检查点引擎（如 DataStates-LLM 和 TorchSnapshot）进行吞吐量比较。

Result: 研究结果表明：
1. **小操作瓶颈：** 与合成工作负载相比，未合并（uncoalesced）的小缓冲区操作会导致吞吐量减半。
2. **聚合的有效性：** 文件系统感知（file system-aware）的聚合策略能够恢复带宽并减少元数据开销。
3. **性能提升：** 与现有最先进的 LLM 检查点引擎相比，作者提出的方法（结合了 \texttt{liburing} 和优化的策略）实现了显著的写吞吐量提升：
    * 比 DataStates-LLM 高出 $3.9\times$。
    * 比 TorchSnapshot 高出 $7.6\times$。
这些结果强调了在现代文件系统和 I/O 后端环境下，对 I/O 聚合和合并策略进行优化的必要性。

Conclusion: 这篇论文的结论是，在大规模语言模型（LLM）的检查点/恢复过程中，聚合和合并策略至关重要，这些策略需要与现代文件系统和I/O后端对齐。作者的方法通过文件系统感知的聚合和合并，显著提高了检查点写入的吞吐量，最高可达现有先进引擎的 3.9 倍（DataStates-LLM）和 7.6 倍（TorchSnapshot）。研究结果强调了解决小缓冲区操作带宽损失和元数据开销的重要性，并证明了 \texttt{liburing} 在结合适当策略时能够提高效率。

Abstract: As LLMs and foundation models scale, checkpoint/restore has become a critical pattern for training and inference. With 3D parallelism (tensor, pipeline, data), checkpointing involves many processes, each managing numerous tensors of varying shapes and sizes, that must be persisted frequently to stable storage (e.g., parallel file systems). This turns checkpoint/restore into a big-data I/O problem characterized by volume, variety, and velocity. The workflow must traverse the full storage stack -- from GPU memory through host memory and local storage to external repositories -- whose tiers differ by orders of magnitude in performance, creating bottlenecks under concurrency even with asynchronous flush/prefetch. Kernel-accelerated I/O libraries such as \texttt{liburing} may mitigate these issues versus POSIX, but their effectiveness for LLM checkpointing remains underexplored. We develop microbenchmarks to quantify trade-offs when using \texttt{liburing}, evaluating how aggregation, alignment, and I/O coalescing interact under buffered and direct I/O. We find that uncoalesced small-buffer operations halve throughput relative to synthetic workloads, while file system-aware aggregation restores bandwidth and reduces metadata overhead. Compared to state-of-the-art LLM checkpointing engines, our approach achieves up to $3.9\times$ higher write throughput than DataStates-LLM and $7.6\times$ higher than TorchSnapshot. These results highlight the need for aggregation and coalescing strategies that align with modern file systems and I/O backends.

</details>


### [15] [Distributed Bilevel Optimization with Dual Pruning for Resource-limited Clients](https://arxiv.org/abs/2512.24667)
*Mingyi Li,Xiao Zhang,Ruisheng Zheng,Hongjian Shi,Yuan Yuan,Xiuzhen Cheng,Dongxiao Yu*

Main category: cs.DC

TL;DR: 这个论文与图处理和机器学习（优化算法）相关。太长不读：本文提出了RABO和RAFBO，是首个资源自适应的分布式双层优化框架，使用了无二阶信息的超梯度估计器，解决传统方法在低资源客户端计算量大的问题。理论分析证明了其能达到渐近最优收敛率$O(1/\sqrt{C_x^{\ast}Q})$，实验证实了其有效性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着大规模模型的发展，传统的分布式双层优化算法难以直接应用于低资源客户端，主要原因是优化内部和外部函数所需的计算量过大。因此，需要开发一种资源自适应的分布式双层优化框架。

Method: 本文提出了两种资源自适应的分布式双层优化算法（RABO 和 RAFBO），其核心在于使用二阶无关的超梯度估计器，使得每个客户端能够根据其可用资源优化子模型。在理论分析上，重新制定了内部参数的误差界，并分析了全局平均超梯度误差的界限。

Result: 所提出的 RABO 和 RAFBO 算法在理论上证明可以达到渐近最优的收敛速度，即 $O(1/\sqrt{C_x^{\ast}Q})$，其中收敛速度受外部参数最小覆盖范围 $C_x^{\ast}$ 的支配。在两个不同的任务上进行的广泛实验验证了所提方法的有效性和计算效率。

Conclusion: 本文提出了一种资源自适应的分布式双层优化框架（RABO 和 RAFBO），并通过理论分析证明了它们在收敛速度上可以达到渐近最优。实验结果验证了所提方法的有效性和计算效率。

Abstract: With the development of large-scale models, traditional distributed bilevel optimization algorithms cannot be applied directly in low-resource clients. The key reason lies in the excessive computation involved in optimizing both the lower- and upper-level functions. Thus, we present the first resource-adaptive distributed bilevel optimization framework with a second-order free hypergradient estimator, which allows each client to optimize the submodels adapted to the available resources. Due to the coupled influence of partial outer parameters x and inner parameters y, it's challenging to theoretically analyze the upper bound regarding the globally averaged hypergradient for full model parameters. The error bound of inner parameter also needs to be reformulated since the local partial training. The provable theorems show that both RABO and RAFBO can achieve an asymptotically optimal convergence rate of $O(1/\sqrt{C_x^{\ast}Q})$, which is dominated by the minimum coverage of the outer parameter $C_x^{\ast}$. Extensive experiments on two different tasks demonstrate the effectiveness and computation efficiency of our proposed methods.

</details>


### [16] [AI-Driven Cloud Resource Optimization for Multi-Cluster Environments](https://arxiv.org/abs/2512.24914)
*Vinoth Punniyamoorthy,Akash Kumar Agarwal,Bikesh Kumar,Abhirup Mazumder,Kabilan Kannan,Sumit Saha*

Main category: cs.DC

TL;DR: 与DSL或图处理或MLIR或编译器或HLS均不相关。
本文提出了一个AI驱动的框架来解决多集群云系统中现有反应式资源管理导致的低效问题。该框架利用预测学习和策略感知决策，实现跨集群资源的主动协调优化，从而提高了资源效率，加速了系统稳定，并降低了性能变异性。


<details>
  <summary>Details</summary>
Motivation: 现代云原生系统越来越依赖**多集群部署**来支持可扩展性、弹性（韧性）和地理分布。然而，现有的资源管理方法大多是**反应式**和**以集群为中心**的，这限制了它们在动态工作负载下优化系统级行为的能力。由此导致在分布式环境中出现**资源利用效率低下**、**适应延迟**和**操作开销增加**等问题。因此，需要一种能够实现主动和协调资源优化的新方法。

Method: 本文提出的框架整合了**预测学习**、**策略感知决策制定**和**持续反馈**机制。它通过分析跨集群遥测数据和历史执行模式，动态调整资源分配，以平衡性能、成本和可靠性目标。这使得系统能够实现主动、协调的跨集群资源管理。

Result: 该框架的实现原型展示了：1. **提高资源效率**；2. **在工作负载波动期间更快的稳定速度**；3. **性能变异性降低**。这些改进是相对于传统的反应式方法而言的，证明了该智能、自适应方法在资源管理中的有效性。

Conclusion: 本文提出了一个AI驱动的框架，用于多集群云系统中的自适应资源优化。通过整合预测学习、策略感知决策和持续反馈，该框架实现了跨集群的资源主动和协调管理。实验结果表明，与传统的反应式方法相比，该框架在资源效率、工作负载波动期间的稳定速度和性能变异性方面均有所改善，凸显了智能、自适应基础设施管理对可扩展和弹性云平台的重要性。

Abstract: Modern cloud-native systems increasingly rely on multi-cluster deployments to support scalability, resilience, and geographic distribution. However, existing resource management approaches remain largely reactive and cluster-centric, limiting their ability to optimize system-wide behavior under dynamic workloads. These limitations result in inefficient resource utilization, delayed adaptation, and increased operational overhead across distributed environments. This paper presents an AI-driven framework for adaptive resource optimization in multi-cluster cloud systems. The proposed approach integrates predictive learning, policy-aware decision-making, and continuous feedback to enable proactive and coordinated resource management across clusters. By analyzing cross-cluster telemetry and historical execution patterns, the framework dynamically adjusts resource allocation to balance performance, cost, and reliability objectives. A prototype implementation demonstrates improved resource efficiency, faster stabilization during workload fluctuations, and reduced performance variability compared to conventional reactive approaches. The results highlight the effectiveness of intelligent, self-adaptive infrastructure management as a key enabler for scalable and resilient cloud platforms.

</details>
