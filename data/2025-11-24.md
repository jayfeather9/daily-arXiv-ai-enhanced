<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.DS](#cs.DS) [Total: 6]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [NX-CGRA: A Programmable Hardware Accelerator for Core Transformer Algorithms on Edge Devices](https://arxiv.org/abs/2511.17235)
*Rohit Prasad*

Main category: cs.AR

TL;DR: 该论文与编译器（NX-CGRA采用软件驱动的可编程性，涉及如何将Transformer算法映射到硬件上执行，这属于编译或运行时系统优化的范畴）和HLS（硬件加速器的设计和实现，特别是CGRA这类可重构架构，与高层次综合HLS密切相关）相关。
太多了；没空看：本文介绍了一种名为NX-CGRA的可编程粗粒度可重构阵列（CGRA）硬件加速器，旨在高效且灵活地支持边缘侧多样化的Transformer推理，包括线性和非线性函数。NX-CGRA通过软件驱动的可编程性，在基准测试中展示了高效率和优异的能效面积权衡，证明了其在受限边缘计算环境下的潜力。


<details>
  <summary>Details</summary>
Motivation: 边缘侧Transformer工作负载日益多样化和复杂化，给性能、能效和架构灵活性带来了平衡的严峻挑战。现有的固定功能加速器通常仅针对特定的窄用例进行优化，缺乏足够的灵活性和通用性。因此，需要一种可扩展、可适应的硬件解决方案来高效支持边缘Transformer部署，尤其是在受限的功耗和芯片预算下。

Method: 本文提出了一种名为NX-CGRA的可编程硬件加速器，它采用粗粒度可重构阵列（CGRA）架构，旨在支持包括线性和非线性函数在内的各种Transformer推理算法。与针对狭窄用例优化的固定功能加速器不同，NX-CGRA通过软件驱动的可编程性实现对各种内核模式的高效执行。

Result: 通过使用源自真实Transformer模型的代表性基准进行评估，NX-CGRA在不同类型的运算中展示了高整体效率和有利的能效-面积权衡。这些结果表明NX-CGRA在受限的功耗和芯片预算下，作为边缘Transformer部署的可扩展和可适应的硬件解决方案具有巨大潜力。

Conclusion: NX-CGRA作为一种可编程的粗粒度可重构阵列（CGRA）架构，为边缘侧Transformer推理提供了高性能、高能效和灵活的硬件解决方案。它通过软件驱动的可编程性，能够高效执行各种内核模式，并在不同类型的运算上表现出优越的能效和面积权衡，证明了其在受限功耗和芯片预算下部署边缘Transformer模型的潜力。

Abstract: The increasing diversity and complexity of transformer workloads at the edge present significant challenges in balancing performance, energy efficiency, and architectural flexibility. This paper introduces NX-CGRA, a programmable hardware accelerator designed to support a range of transformer inference algorithms, including both linear and non-linear functions. Unlike fixed-function accelerators optimized for narrow use cases, NX-CGRA employs a coarse-grained reconfigurable array (CGRA) architecture with software-driven programmability, enabling efficient execution across varied kernel patterns. The architecture is evaluated using representative benchmarks derived from real-world transformer models, demonstrating high overall efficiency and favorable energy-area tradeoffs across different classes of operations. These results indicate the potential of NX-CGRA as a scalable and adaptable hardware solution for edge transformer deployment under constrained power and silicon budgets.

</details>


### [2] [DISCA: A Digital In-memory Stochastic Computing Architecture Using A Compressed Bent-Pyramid Format](https://arxiv.org/abs/2511.17265)
*Shady Agwa,Yikang Shen,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: 与 DSL、图处理、MLIR、编译器或 HLS 无关。

太长不看简要概括：随着 AI 模型的规模增大和向边缘迁移，传统的冯·诺依曼架构和现有内存计算方案（无论是模拟还是数字）都面临挑战。本文提出了一种名为 DISCA 的新型数字内存随机计算架构，它采用压缩的准随机 Bent-Pyramid 数据格式，结合了模拟计算的简单性和数字系统的优点（可扩展性、生产力、可靠性）。DISCA 在 180nm CMOS 技术下，能效达到 3.59 TOPS/W/bit，显著提高了矩阵乘法工作负载的能效。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能革命的深入，大规模人工智能模型需要执行大量数据密集型的矩阵乘法任务，但传统的冯·诺依曼架构面临内存墙和摩尔定律终结的挑战。此外，人工智能应用正在快速向边缘迁移，对边缘设备的人工智能架构硬件预算提出了更高的要求。虽然内存计算被认为是解决内存墙的有前景的方案，但模拟和数字内存计算架构都因各种设计限制而导致其效益大打折扣。

Method: 提出了一种新的数字内存随机计算架构（DISCA），它采用了压缩的准随机 Bent-Pyramid 数据格式。这种架构旨在结合模拟计算的简单性和数字系统的可扩展性、生产力及可靠性。

Result: DISCA的后布局建模结果表明，在使用商用 180nm CMOS 技术和 500 MHz 时，每个位元的能效为 3.59 TOPS/W。这表明 DISCA 显著提高了矩阵乘法工作负载的能效，如果进行扩展并与对应架构进行比较，能效将有数量级的提升。

Conclusion: DISCA，一种新的数字内存随机计算架构，利用压缩的准随机 Bent-Pyramid 数据格式，在不牺牲数字系统可扩展性、生产力和可靠性的前提下，实现了与模拟计算相同的计算简单性。DISCA 在 500 MHz 下，使用商用 180nm CMOS 技术，每个位元的能效为 3.59 TOPS/W。与现有架构相比，DISCA 在矩阵乘法工作负载的能效方面有了数量级的显著提升。

Abstract: Nowadays, we are witnessing an Artificial Intelligence revolution that dominates the technology landscape in various application domains, such as healthcare, robotics, automotive, security, and defense. Massive-scale AI models, which mimic the human brain's functionality, typically feature millions and even billions of parameters through data-intensive matrix multiplication tasks. While conventional Von-Neumann architectures struggle with the memory wall and the end of Moore's Law, these AI applications are migrating rapidly towards the edge, such as in robotics and unmanned aerial vehicles for surveillance, thereby adding more constraints to the hardware budget of AI architectures at the edge. Although in-memory computing has been proposed as a promising solution for the memory wall, both analog and digital in-memory computing architectures suffer from substantial degradation of the proposed benefits due to various design limitations. We propose a new digital in-memory stochastic computing architecture, DISCA, utilizing a compressed version of the quasi-stochastic Bent-Pyramid data format. DISCA inherits the same computational simplicity of analog computing, while preserving the same scalability, productivity, and reliability of digital systems. Post-layout modeling results of DISCA show an energy efficiency of 3.59 TOPS/W per bit at 500 MHz using a commercial 180nm CMOS technology. Therefore, DISCA significantly improves the energy efficiency for matrix multiplication workloads by orders of magnitude if scaled and compared to its counterpart architectures.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling](https://arxiv.org/abs/2511.16947)
*Chenqi Zhao,Wenfei Wu,Linhai Song,Yuchen Xu*

Main category: cs.DC

TL;DR: 该论文与图处理（在负载均衡的背景下，可能涉及底层系统结构或任务调度图）和编译器（作为潜在的系统优化或底层实现工具）和 HLS（作为潜在的硬件加速器优化）和 DSL（作为潜在的系统描述语言）领域相关。

太长不看版：混合专家模型（MoE）的动态特性导致负载不平衡，严重影响训练效率，而现有方法无法实现细粒度负载平衡。本文提出了 MicroEP 这一新的并行化策略，通过高效的令牌调度在每个微批次中实现细粒度甚至最优的负载平衡。在此基础上构建了分布式训练系统 MicroMoE，实验结果显示相比现有技术，MicroMoE 的端到端训练吞吐量提升了高达 47.6%，并几乎始终保持最优负载平衡。


<details>
  <summary>Details</summary>
Motivation: MoE 模型虽然能显著减少计算资源开销，但其固有的动态特性导致专家（experts）之间存在负载不平衡问题，严重影响了训练效率。现有解决方案要么牺牲模型精度，要么引入额外的系统开销，未能实现对优化训练效率至关重要的细粒度负载平衡。

Method: 提出了 MicroEP，一种新的并行化策略，旨在通过高效的令牌调度在每个微批次中实现 GPU 间的最优负载平衡。基于此，进一步提出了 MicroMoE，这是一个集成了 MicroEP 负载平衡能力的分布式 MoE 训练系统。

Result: MicroMoE（集成了 MicroEP 的分布式 MoE 训练系统）与现有技术相比，将端到端训练吞吐量提高了高达 47.6%，并且几乎始终在 GPU 之间实现了最优的负载平衡。

Conclusion: 我们提出了 MicroEP，这是一种新的并行化策略，用于实现 MoE 系统中的细粒度负载平衡。通过在 GPU 之间进行高效的令牌调度，MicroEP 能够在每个微批次中实现最优的负载平衡。我们的实验结果表明，与现有技术相比，MicroMoE（一个集成了 MicroEP 的分布式 MoE 训练系统）将端到端训练吞吐量提高了高达 47.6%，并且几乎始终在 GPU 之间实现了最优的负载平衡。

Abstract: Mixture-of-Experts (MoE) has emerged as a promising approach to scale up deep learning models due to its significant reduction in computational resources. However, the dynamic nature of MoE leads to load imbalance among experts, severely impacting training efficiency. While previous research has attempted to address the load balancing challenge, existing solutions either compromise model accuracy or introduce additional system overhead. As a result, they fail to achieve fine-grained load balancing, which is crucial to optimizing training efficiency.
  We propose MicroEP, a novel parallelization strategy to achieve fine-grained load balancing in MoE systems. MicroEP is capable of achieving optimal load balancing in every micro-batch through efficient token scheduling across GPUs. Furthermore, we propose MicroMoE, an efficient distributed MoE training system with MicroEP's load balancing capabilities. Our experimental results demonstrate that MicroMoE improves the end-to-end training throughput by up to 47.6% compared with the state-of-the-art system, and almost consistently achieves optimal load balance among GPUs.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [4] [Efficient Algorithms and Implementations for Extracting Maximum-Size $(k,\ell)$-Sparse Subgraphs](https://arxiv.org/abs/2511.16877)
*Péter Madarasi*

Main category: cs.DS

TL;DR: 本论文与图处理（Graph Processing）、组合优化和算法相关。
寻找最大 $(k, \ell)$-稀疏子图是一个经典的图论和组合优化问题。本文实现了基于增广路径方法的高效算法，并通过应用各种启发式方法（如排序、两阶段策略、伪森林初始化）显著提高了运行速度，实验证明其性能比现有工具快了几个数量级。此外，还提出了一个渐进更快的算法，用于提取包含意义上最大的 $(k, 2k)$-稀疏子图，该稀疏性条件仅对大小至少为三的节点集有效，这在 3D 刚性中具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 寻找最大化 $(k, \ell)$-稀疏子图是刚性理论和组合优化领域的一个经典且重要的问题，虽然其存在已知的多项式时间算法，但现有的实现工具在效率上可能存在不足。
本文的动机在于：
1.  **提高效率：** 显著提高求解最大 $(k, \ell)$-稀疏子图问题的算法的运行效率，使其在实际应用中更具可行性。
2.  **灵活性和实用性：** 开发一个高度高效且灵活的**增广路径方法**的实现，并集成一系列实用的启发式方法，以应对大型图的挑战。
3.  **解决特定刚性问题：** 针对 $k=3$ 时的 3D 刚性问题，提出并实现一个渐进更快的算法，用于提取包含意义上最大的 $(k, 2k)$-稀疏子图。

Method: 本文的核心方法是实现并优化了一种基于**增广路径**的现有多项式时间算法来解决最大 $(k, \ell)$-稀疏子图问题。
优化的关键在于引入了一系列强大的实用**启发式方法**：
*   **边排序 (edge-ordering)**
*   **节点排序 (node-ordering)**
*   **两阶段策略 (two-phase strategies)**
*   **基于伪森林的初始化 (pseudoforest-based initialization)**
这些启发式方法旨在使算法在执行早期接受更多的边，从而避免耗时的增广操作。
此外，本文还提出了一个**渐进更快（asymptotically faster）**的算法，用于提取**包含意义上最大 (inclusion-wise maximal)** 的 $(k, 2k)$-稀疏子图，其中稀疏性条件仅对大小至少为三个的节点集要求，这与 $k=3$ 时的 3D 刚性问题相关。

Result: 1.  **性能提升：** 基于增广路径方法的实现，结合一系列实用启发式方法，在合成图和实际图上的综合实验评估表明，其性能比现有工具**提高了几个数量级**。
2.  **新算法提出：** 提出了一个渐进更快的新算法，用于提取包含意义上最大的 $(k, 2k)$-稀疏子图，特别适用于 $k=3$ 时的 3D 刚性问题。
3.  **代码可用性：** 提供了精心设计的优化实现，可供公开获取，并建议将其纳入 LE-MON 图形库。

Conclusion: 本文实现并优化了一种基于增广路径的方法，用于求解最大化 $(k, \ell)$-稀疏子图问题，并通过实验证明其性能远超现有工具。此外，提出了一个渐进更快的新算法用于提取包含意义上最大的 $(k, 2k)$-稀疏子图，尤其适用于 $k=3$ 时的 3D 刚性问题。所实现的这两种算法的高效实现公开发布，并建议纳入 LEMON 图形库。

Abstract: A multigraph $G = (V, E)$ is $(k, \ell)$-sparse if every subset $X \subseteq V$ induces at most $\max\{k|X| - \ell, 0\}$ edges. Finding a maximum-size $(k, \ell)$-sparse subgraph is a classical problem in rigidity theory and combinatorial optimization, with known polynomial-time algorithms. This paper presents a highly efficient and flexible implementation of an augmenting path method, enhanced with a range of powerful practical heuristics that significantly reduce running time while preserving optimality. These heuristics $\unicode{x2013}$ including edge-ordering, node-ordering, two-phase strategies, and pseudoforest-based initialization $\unicode{x2013}$ steer the algorithm toward accepting more edges early in the execution and avoiding costly augmentations. A comprehensive experimental evaluation on both synthetic and real-world graphs demonstrates that our implementation outperforms existing tools by several orders of magnitude. We also propose an asymptotically faster algorithm for extracting an inclusion-wise maximal $(k,2k)$-sparse subgraph with the sparsity condition required only for node sets of size at least three, which is particularly relevant to 3D rigidity when $k = 3$. We provide a carefully engineered implementation, which is publicly available online and is proposed for inclusion in the LEMON graph library.

</details>


### [5] [Low-Sensitivity Matching via Sampling from Gibbs Distributions](https://arxiv.org/abs/2511.16918)
*Yuichi Yoshida,Zihan Zhang*

Main category: cs.DS

TL;DR: 本文与 DSL 或图处理或 MLIR 或编译器或 HLS 无关。
该研究从敏感度的角度考察了最大匹配问题，敏感度定义为删除一条边后算法输出分布（使用 Hamming 距离下的 Wasserstein 距离）的最大变化。研究提出了基于 Gibbs 分布采样的算法：对于最大度为 $\Delta$ 的图，设计了敏感度为 $\Delta^{O(1/\varepsilon)}$ 的 $(1-\varepsilon)$-近似算法，显著优于现有结果，运行时间为 $O_{\varepsilon, \Delta}(m \log m)$；对于平面图和二分图，设计了运行时间更快的算法 $\mathrm{poly}(n/\varepsilon)$；对于一般图，提出了敏感度为 $\sqrt{n} \cdot (\varepsilon^{-1} \log n)^{O(1/\varepsilon)}$ 的 $(1-\varepsilon)$-近似算法，改进了先前最好的界限 $O(n^{1/(1+\varepsilon^2)})$。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于研究最大匹配算法在图结构发生微小变化（如删除单个边）时的鲁棒性，即算法的“敏感度”。敏感度是使用 Wasserstein 距离（以 Hamming 距离为基础）度量算法输出分布的稳定性。目标是设计在保证近似质量（$(1-\varepsilon)$ 近似）的同时，能显著降低敏感度界限的算法，特别是在图的度 $\Delta$ 有界或图为特定类型（平面图、二分图）时，以及对于一般图。提高敏感度界限对于实际应用中需要稳定输出的匹配算法具有重要意义。

Method: 本文的方法主要基于从最大匹配的 Gibbs 分布中进行采样。
1.  **对于有界度图 ($\Delta$)**：通过从 Gibbs 分布采样，设计了一个在 $O_{\varepsilon, \Delta}(m \log m)$ 时间内运行的 $(1 - \varepsilon)$-近似算法，实现了 $O(\Delta^{O(1/\varepsilon)})$ 的敏感度。
2.  **对于平面图和二分图**：通过设计更有效的 Gibbs 分布采样算法，实现了 $\mathrm{poly}(n/\varepsilon)$ 的运行时间，从而提供了显著更快的算法。
3.  **对于一般图**：通过使用基于 Gibbs 分布采样的方法，得到了一个敏感度为 $\sqrt{n} \cdot (\varepsilon^{-1} \log n)^{O(1/\varepsilon)}$ 的 $(1 - \varepsilon)$-近似算法（多项式时间）。
核心技术是利用 Gibbs 分布的特性来确保算法输出的稳定性，同时在各种图类中优化采样效率。

Result: 本文取得了以下主要结果：
1.  **有界度图 ($\Delta$)**：提出了一个多项式时间 $(1 - \varepsilon)$-近似算法，其敏感度显著改进为 $\Delta^{O(1/\varepsilon)}$，运行时间为 $O_{\varepsilon, \Delta}(m \log m)$。
2.  **平面图和二分图**：设计了运行时间显著更快的算法，其时间复杂度为 $\mathrm{poly}(n/\varepsilon)$，特别是在 $\varepsilon$ 和 $\Delta$ 方面更快。
3.  **一般图（无界度）**：提出了一个多项式时间 $(1 - \varepsilon)$-近似算法，敏感度界限改进为 $\sqrt{n} \cdot (\varepsilon^{-1} \log n)^{O(1/\varepsilon)}$，优于先前最好的 $O(n^{1/(1+\varepsilon^2)})$ 界限。

Conclusion: 本文研究了最大匹配问题的敏感度界限，并提出了一系列改进的算法。对于一般的图，提出了敏感度为 $\sqrt{n} \cdot (\varepsilon^{-1} \log n)^{O(1/\varepsilon)}$ 的 $(1-\varepsilon)$-近似算法，显著优于先前的 $O(n^{1/(1+\varepsilon^2)})$。对于有界度图，提出了敏感度为 $\Delta^{O(1/\varepsilon)}$ 的 $(1-\varepsilon)$-近似算法。此外，对于平面图和二分图，还提出了运行时间更快的算法。这些结果为在边扰动下保持稳定性的最大匹配算法提供了新的理论界限和实用的高效方法。

Abstract: In this work, we study the maximum matching problem from the perspective of sensitivity. The sensitivity of an algorithm $A$ on a graph $G$ is defined as the maximum Wasserstein distance between the output distributions of $A$ on $G$ and on $G - e$, where $G - e$ is the graph obtained by deleting an edge $e$ from $G$. The maximum is taken over all edges $e$, and the underlying metric for the Wasserstein distance is the Hamming distance.
  We first show that for any $\varepsilon > 0$, there exists a polynomial-time $(1 - \varepsilon)$-approximation algorithm with sensitivity $Δ^{O(1/\varepsilon)}$, where $Δ$ is the maximum degree of the input graph. The algorithm is based on sampling from the Gibbs distribution over matchings and runs in time $O_{\varepsilon, Δ}(m \log m)$, where $m$ is the number of edges in the graph. This result significantly improves the previously known sensitivity bounds.
  Next, we present significantly faster algorithms for planar and bipartite graphs as a function of $\varepsilon$ and $Δ$, which run in time $\mathrm{poly}(n/\varepsilon)$. This improvement is achieved by designing a more efficient algorithm for sampling matchings from the Gibbs distribution in these graph classes, which improves upon the previous best in terms of running time.
  Finally, for general graphs with potentially unbounded maximum degree, we show that there exists a polynomial-time $(1 - \varepsilon)$-approximation algorithm with sensitivity $\sqrt{n} \cdot (\varepsilon^{-1} \log n)^{O(1/\varepsilon)}$, improving upon the previous best bound of $O(n^{1/(1+\varepsilon^2)})$.

</details>


### [6] [Merging RLBWTs adaptively](https://arxiv.org/abs/2511.16953)
*Travis Gagie*

Main category: cs.DS

TL;DR: 相关领域：图处理、编译器、HLS、MLIR、DSL。
太长不看（TLDR）：本文提出了一种**快速**合并**游程编码 Burrows-Wheeler 变换 (RLBWT)** 的算法。该算法能够在 $O(R)$ 空间复杂度（$R$ 为总游程数）和准线性时间复杂度 $\tilde{O} (L + \sigma + R)$ 内完成合并，其中关键参数 $L$ 是在合并后的扩展 BWT (eBWT) 边界上 LCP 值的总和。作者猜想，当原始字符串（或字符串集合）重复但**不相似**时，$L$ 倾向于较小，从而使该算法极度高效。


<details>
  <summary>Details</summary>
Motivation: 现有的 Burrows-Wheeler 变换（BWT）及其压缩版本游程编码 BWT（RLBWT）在处理大规模或大量的字符串数据集时面临挑战。为了高效地对多个原始 RLBWTs 进行索引、处理或分析，需要一种快速且空间高效的方法将它们合并成一个单一的 RLBWT（即对合并后的字符串集合进行 eBWT）。现有的方法在时间或空间上可能不够高效，尤其是在处理具有重复结构的字符串时。因此，本文的动机是找到一种新的 RLBWT 合并算法，使其在空间和时间上都更有效，尤其关注当合并产生的 $L$ 参数较小时，能达到准线性时间复杂度。

Method: 本文提出了一种快速合并 RLBWTs 的方法。该方法利用了合并后的扩展 Burrows-Wheeler 变换（eBWT）中，来自相同原始 RLBWT 的字符块之间的边界。通过计算这些边界上的最长公共前缀（LCP）值之和 $L$，本文展示了如何在 $\tilde{O} (L + \sigma + R)$ 时间内完成合并，其中 $R$ 是合并前总的游程数，$ \sigma$ 是字母表大小。算法的重点在于利用 $L$ 这一参数来控制合并的复杂度。

Result: 本文提出的 RLBWT 合并算法能在 $O (R)$ 空间内完成，其中 $R$ 是总游程数。在假设 $L$ 较小的前提下（即原始字符串重复但不同），算法可以在 $\tilde{O} (L + \sigma + R)$ 的时间复杂度内完成合并。这里的 $\tilde{O}$ 隐藏了对数因子。$L$ 是一个关键参数，它定义为合并后的扩展 BWT 中，来自不同原始 RLBWTs 的字符块边界处的最长公共前缀（LCP）值的总和。结果表明，当 $L$ 较小时，此合并方法非常快速和高效。

Conclusion: 本文提出了一种新的 RLBWT 合并算法，其时间复杂度为 $\tilde{O} (L + \sigma + R)$。此算法在 $L$ 较小的情况下（例如，当原始字符串重复但不同时）非常高效。虽然 $L$ 的界限依赖于重复程度和字符串相似性尚未在所有情况下得到严格证明，但本文的方法为处理大型或分布式的字符串数据集提供了一个新的、高效且空间利用率高的工具。

Abstract: We show how to merge run-length compressed Burrows-Wheeler Transforms (RLBWTs) quickly and in $O (R)$ space, where $R$ is the total number of runs in them, when a certain parameter is small. Specifically, we consider the boundaries in their combined extended Burrows-Wheeler Transform (eBWT) between blocks of characters from the same original RLBWT, and denote by $L$ the sum of the longest common prefix (LCP) values at those boundaries. We show how to merge the RLBWTs in $\tilde{O} (L + σ+ R)$ time, where $σ$ is the alphabet size. We conjecture that $L$ tends to be small when the strings (or sets of strings) underlying the original RLBWTs are repetitive but dissimilar.

</details>


### [7] [Triangle Detection in H-Free Graphs](https://arxiv.org/abs/2511.17224)
*Amir Abboud,Ron Safier,Nathan Wallheimer*

Main category: cs.DS

TL;DR: 该论文与图处理相关。
论文开创性地研究了在约束图（无 $H$ 子图）中检测三角形的组合算法，旨在将允许次立方加速的模式进行分类。通过下界和上界分析，论文证明了某些模式下无加速可能，而对于“可嵌入”的模式，设计了强次立方组合算法 $\tilde O(n^{3-\frac{1}{2^{k-3}}})$。这些结果促成了对小模式的二分定理分类。此外，还提出了 $H$-敏感算法以及针对奇圈图的专门高效算法（例如，针对 $C_{2k+1}$-free 图的 $\tilde O(m+n^{1+2/k})$ 算法）。所有的算法都排除了快速矩阵乘法。


<details>
  <summary>Details</summary>
Motivation: 研究约束图（即无固定模式 $H$ 作为子图的图，$H$-free 图）上的三角形检测问题。目标是使用“组合”方法（排除快速矩阵乘法）来决定 $H$-free 图是否包含三角形。研究旨在对哪些模式 $H$ 允许次立方加速进行分类，以求得一个二分定理。

Method: 下界分析：证明了如果 $H$ 不可 $3$ 着色或包含不止一个三角形，则算法复杂度不变，组合加速的可能性较低。上界分析（嵌入方法）：为一类“可嵌入”模式 $H$ 设计了一种强次立方组合算法，运行时间为 $\tilde O(n^{3-\frac{1}{2^{k-3}}})$。算法推广：推广到“可嵌入但带有一个三角形障碍”的模式，据此实现了对大小至多为八的所有模式的二分定理分类。H-敏感算法：设计了 $H$-敏感算法，当图中 $H$ 的副本数目 $t$ 远小于最大可能值 $\Omega(n^k)$ 时，可以更快运行。奇圈的特殊情况：针对 $C_{2k+1}$-free 图和 $C_5$-sensitive 图，分别提出了专门的、高效率的组合三角形检测算法，运行时间分别为 $\tilde O(m+n^{1+2/k})$ 和 $\tilde O(n^2+n^{4/3}t^{1/3})$。所有算法都属于“组合”方法，排除了快速矩阵乘法。

Result: 证明了如果 $H$ 不可 $3$ 着色或包含不止一个三角形，则不太可能有组合加速。为大小为 $k$ 的“可嵌入”模式设计了运行时间为 $\tilde O(n^{3-\frac{1}{2^{k-3}}})$ 的强次立方组合算法，该算法也适用于列出所有三角形。推广的算法完成了对至多八个顶点的所有模式的二分定理分类。提出了 $H$-敏感算法，当 $H$ 的副本数较少时运行更快。针对奇圈 $C_{2k+1}$-free 图，提出了运行时间为 $\tilde O(m+n^{1+2/k})$ 的高效组合算法。针对 $C_5$-sensitive 图，提出了运行时间为 $\tilde O(n^2+n^{4/3}t^{1/3})$ 的 $C_5$-敏感算法。

Conclusion: 这篇论文开创了在无 $H$ 子图约束图上研究三角形检测的组合算法，并旨在将其复杂性分类。作者们通过下界分析确定了某些模式下无组合加速的可能性，并通过上界分析为“可嵌入”模式设计了强次立方组合算法，并给出了具体的时间复杂度。此外，论文还推广了算法以处理带有一个障碍物的模式，并通过 $H$-敏感算法利用 $H$ 副本数的稀疏性来提高效率，并对奇圈图给出了专门的高效三角形检测算法。总之，论文通过组合方法在约束图上的三角形检测问题上取得了分类和复杂性改进方面的实质性进展。

Abstract: We initiate the study of combinatorial algorithms for Triangle Detection in $H$-free graphs. The goal is to decide if a graph that forbids a fixed pattern $H$ as a subgraph contains a triangle, using only "combinatorial" methods that notably exclude fast matrix multiplication. Our work aims to classify which patterns admit a subcubic speedup, working towards a dichotomy theorem. On the lower bound side, we show that if $H$ is not $3$-colorable or contains more than one triangle, the complexity of the problem remains unchanged, and no combinatorial speedup is likely possible. On the upper bound side, we develop an embedding approach that results in a strongly subcubic, combinatorial algorithm for a rich class of "embeddable" patterns. Specifically, for an embeddable pattern of size $k$, our algorithm runs in $\tilde O(n^{3-\frac{1}{2^{k-3}}})$ time, where $\tilde O(\cdot)$ hides poly-logarithmic factors. This algorithm also extends to listing all the triangles within the same time bound. We supplement this main result with two generalizations: 1) A generalization to patterns that are embeddable up to a single obstacle that arises from a triangle in the pattern. This completes our classification for small patterns, yielding a dichotomy theorem for all patterns of size up to eight. 2) An $H$-sensitive algorithm for embeddable patterns, which runs faster when the number of copies of $H$ is significantly smaller than the maximum possible $Ω(n^k)$. Finally, we focus on the special case of odd cycles. We present specialized Triangle Detection algorithms that are very efficient: 1) A combinatorial algorithm for $C_{2k+1}$-free graphs that runs in $\tilde O(m+n^{1+2/k})$ time for every $k\geq2$, where $m$ is the number of edges in the graph. 2) A combinatorial $C_5$-sensitive algorithm that runs in $\tilde O(n^2+n^{4/3}t^{1/3})$ time, where $t$ is the number of $5$-cycles in the graph.

</details>


### [8] [Spectral Clustering with Side Information](https://arxiv.org/abs/2511.17326)
*Hendrik Fichtenberger,Michael Kapralov,Ekaterina Kochetkova,Silvio Lattanzi,Davide Mazzali,Weronika Wrzos-Kaminska*

Main category: cs.DS

TL;DR: Paper relation: This paper is related to **Graph Processing** (specifically graph clustering and community detection) and **Algorithm Design** (sublinear-time and polynomial-time algorithms).
TLDR: The paper addresses graph clustering with implanted solutions where vertices also have noisy labels (error rate $\delta$) in addition to graph structure ($\epsilon$-sparse cuts and $\Omega(1)$-expanders). Traditional methods achieve $\min\{\epsilon, \delta\}$ error. The key contribution is showing how to combine these two sources of information to achieve a nearly optimal $\approx \widetilde O(\epsilon\delta)$ misclassification rate. The authors propose a sublinear-time classifier based on a new observation about "spectrally ambiguous" vertices. Furthermore, they provide a polynomial-time algorithm to refine the graph structure by reweighting edges, transforming the original graph into one (for constant $k$) with improved sparsity ($\widetilde O(\epsilon\delta)$) while preserving its expansion properties.


<details>
  <summary>Details</summary>
Motivation: 传统的图聚类问题（具有植入解）研究仅依赖图结构信息，假设簇通过 $\Omega(1)$-扩展子图连接良好，并形成 $\epsilon$-稀疏割。虽然针对这种“朴素”版本已经有高效算法能达到 $\approx \epsilon$ 的错误率，但在实践中，图的顶点通常带有提供额外信息的标签（如独立的、以 $\delta$ 概率损坏的簇标签）。单独使用两种信息源（图结构或顶点标签）导致的最小错误率为 $\min\{\epsilon, \delta\}$。
**核心动机：** 探索是否可以有效地结合这两种信息源（图结构和带噪标签），以实现接近最优的聚类错误率，即 $\approx \epsilon\delta$。同时，寻求开发在顶点数 $n$ 上高效（次线性时间）的算法。

Method: 1. **提出次线性时间分类器：** 提出了一个次线性时间（相对于顶点数 $n$）的算法，旨在结合图结构信息（$\epsilon$-稀疏割、$\Omega(1)$-扩展子图）和带噪顶点标签信息（$\delta$ 错误率）来提高聚类精度。这是基于对“谱模糊”（spectrally ambiguous）顶点的新观察。该分类器实现了接近最优的 $\approx \widetilde O(εδ)$ 错误率。
2. **提出多项式时间图结构优化算法：** 提出了一个多项式时间算法，用于重新加权原始 $(k, \epsilon, \Omega(1))$-可聚类图的边。目标是将原图转化为一个 $(k, \widetilde O(εδ), \Omega(1))$-可聚类图（对于常数 $k$），从而接近最优地改善割的稀疏性，同时保留社区的扩展性。这个算法可以看作是对输入图社区结构的精炼。

Result: 1. **实现接近最优的错误率：** 提出了一个次线性时间分类器，成功地结合了图结构和带噪标签信息，实现了接近最优的 $\approx \widetilde O(\epsilon\delta)$ 错误率。
2. **发现“谱模糊”顶点：** 算法的关键洞察是源自对良好可聚类图中“谱模糊”顶点的新的观察。
3. **图结构精炼：** 提出了一个多项式时间算法，可以将原始的 $(k, \epsilon, \Omega(1))$-可聚类图转换为一个具有更优稀疏度 $(k, \widetilde O(\epsilon\delta), \Omega(1))$ 且保持扩展性的图，从而精炼了输入图的社区结构。但是，次线性时间分类器输出的近似簇本身并不一定在图 $G$ 中引起扩展子图。

Conclusion: 本文通过理论分析和算法设计，解决了如何在图聚类问题中有效结合图结构信息与带噪声的顶点标签信息以提高聚类精度的问题。作者提出的次线性时间分类器和多项式时间图结构优化算法，证明了可以实现接近最优的 $\approx \widetilde O(εδ)$ 聚类错误率，并在改善稀疏度的同时保留了社区的内部扩展性。这为处理带有辅助信息的图聚类问题提供了新的高效方法。

Abstract: In the graph clustering problem with a planted solution, the input is a graph on $n$ vertices partitioned into $k$ clusters, and the task is to infer the clusters from graph structure. A standard assumption is that clusters induce well-connected subgraphs (i.e. $Ω(1)$-expanders), and form $ε$-sparse cuts. Such a graph defines the clustering uniquely up to $\approx ε$ misclassification rate, and efficient algorithms for achieving this rate are known. While this vanilla version of graph clustering is well studied, in practice, vertices of the graph are typically equipped with labels that provide additional information on cluster ids of the vertices. For example, each vertex could have a cluster label that is corrupted independently with probability $δ$. Using only one of the two sources of information leads to misclassification rate $\min\{ε, δ\}$, but can they be combined to achieve a rate of $\approx εδ$?
  In this paper, we give an affirmative answer to this question and present a sublinear-time algorithm in the number of vertices $n$. Our key algorithmic insight is a new observation on ``spectrally ambiguous'' vertices in a well-clusterable graph.
  While our sublinear-time classifier achieves the nearly optimal $\approx \widetilde O(εδ)$ misclassification rate, the approximate clusters that it outputs do not necessarily induce expanders in the graph $G$. In our second result, we give a polynomial-time algorithm that reweights edges of the original $(k, ε, Ω(1))$-clusterable graph to transform it into a $(k, \widetilde O(εδ), Ω(1))$-clusterable one (for constant $k$), improving sparsity of cuts nearly optimally and preserving expansion properties of the communities - an algorithm for refining community structure of the input graph.

</details>


### [9] [Relative Error Streaming Quantiles with Seamless Mergeability via Adaptive Compactors](https://arxiv.org/abs/2511.17396)
*Tomáš Domes,Pavel Veselý*

Main category: cs.DS

TL;DR: 该论文与 DSL、图处理、MLIR、编译器或 HLS **无关**。
ReqSketch 是一种用于估计大型分布式数据集中属性分布的节省空间的可合并数量纲摘要，由于其过于复杂的合并性证明而存在问题。本文提出了一个精简版的 ReqSketch，即自适应压缩器，它在保留 ReqSketch 空间效率、更新时间和算法简洁性的同时，显著简化了相对误差保证的证明，并在合并规模相当的草图时实现了接近最优的空间界限。


<details>
  <summary>Details</summary>
Motivation: 现有的 ReqSketch 是最节省空间的数量纲摘要（quantile summary），具有相对误差保证和可合并性等关键特性，并已被实际采用。然而，ReqSketch 中可合并性的证明过于复杂，需要复杂的收费论证和方差分析。因此，本文的动机是提供一个精简版的 ReqSketch，以简化在最通用可合并性设置下的相对误差保证证明，同时保持其空间效率、更新时间和算法简洁性。

Method: 本文通过开发“自适应压缩器”（adaptive compactors）提供了 ReqSketch 的改进版本。这种改进保留了原始的空间界限、更新时间和算法简洁性，同时显著简化了相对误差保证的证明（特别是在最通用的可合并设置中）。此外，这种自适应特性及其证明技术在特定场景（例如合并规模相当的草图）中实现了接近最佳空间界限。

Result: 所提出的自适应压缩器版本的 ReqSketch 显著简化了相对误差保证的证明，尤其是在最通用的可合并性设置下。它保留了 ReqSketch 原始的空间界限、更新时间以及算法简洁性。此外，由于自适应性和新的证明技术，它在特定场景（特别是合并规模相当的草图时）能够获得接近最优的空间界限。

Conclusion: 本文提出了 ReqSketch 的改进版本，即自适应压缩器。它保留了 ReqSketch 的空间效率、更新时间和算法简洁性，同时在最通用的可合并设置中大大简化了相对误差保证的证明。此外，这种自适应特性及其证明技术在特定场景（特别是合并具有可比大小的草图时）实现了接近最佳的空间界限。

Abstract: Quantile summaries provide a scalable way to estimate the distribution of individual attributes in large datasets that are often distributed across multiple machines or generated by sensor networks. ReqSketch (arXiv:2004.01668) is currently the most space-efficient summary with two key properties: relative error guarantees, offering increasingly higher accuracy towards the distribution's tails, and mergeability, allowing distributed or parallel processing of datasets. Due to these features and its simple algorithm design, ReqSketch has been adopted in practice, via implementation in the Apache DataSketches library. However, the proof of mergeability in ReqSketch is overly complicated, requiring an intricate charging argument and complex variance analysis.
  In this paper, we provide a refined version of ReqSketch, by developing so-called adaptive compactors. This enables a significantly simplified proof of relative error guarantees in the most general mergeability setting, while retaining the original space bound, update time, and algorithmic simplicity. Moreover, the adaptivity of our sketch, together with the proof technique, yields near-optimal space bounds in specific scenarios - particularly when merging sketches of comparable size.

</details>
