<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DS](#cs.DS) [Total: 6]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling](https://arxiv.org/abs/2511.16947)
*Chenqi Zhao,Wenfei Wu,Linhai Song,Yuchen Xu*

Main category: cs.DC

TL;DR: 相关领域：图处理（Graph processing）、MLIR、编译器（Compiler）、HLS、DSL、Mixture-of-Experts (MoE) 系统、分布式训练系统。
太长不看（TLDR）：本文提出了 MicroEP 策略和 MicroMoE 分布式训练系统，旨在解决 MoE 模型动态性导致的专家间负载不均衡问题。MicroEP通过高效的 token 调度在每个微批次中实现细粒度负载均衡，MicroMoE系统利用此能力，将端到端训练吞吐量提高了高达 47.6%，并实现了近乎最优的 GPU 负载平衡。


<details>
  <summary>Details</summary>
Motivation: MoE 模型虽然能显著减少计算资源，但其动态特性导致专家间负载不均衡，严重影响训练效率。现有解决方案要么牺牲模型精度，要么引入额外系统开销，未能实现对优化训练效率至关重要的细粒度负载均衡。

Method: 提出了 MicroEP 并行化策略，通过跨 GPU 的高效 token 调度，在每个微批次中实现细粒度负载均衡。在此基础上，提出了 MicroMoE 分布式 MoE 训练系统，利用 MicroEP 的负载均衡能力。

Result: MicroMoE 相较于现有先进系统，端到端训练吞吐量提高了高达 47.6%，并且几乎一直能实现 GPU 间的最优负载平衡。

Conclusion: MicroMoE 通过引入 MicroEP 并行化策略，解决了 MoE 系统中专家负载不均衡的问题，实现在每个微批次中的细粒度负载均衡，显著提升了训练效率，并且几乎一直能保持 GPU 间的最优负载平衡。

Abstract: Mixture-of-Experts (MoE) has emerged as a promising approach to scale up deep learning models due to its significant reduction in computational resources. However, the dynamic nature of MoE leads to load imbalance among experts, severely impacting training efficiency. While previous research has attempted to address the load balancing challenge, existing solutions either compromise model accuracy or introduce additional system overhead. As a result, they fail to achieve fine-grained load balancing, which is crucial to optimizing training efficiency.
  We propose MicroEP, a novel parallelization strategy to achieve fine-grained load balancing in MoE systems. MicroEP is capable of achieving optimal load balancing in every micro-batch through efficient token scheduling across GPUs. Furthermore, we propose MicroMoE, an efficient distributed MoE training system with MicroEP's load balancing capabilities. Our experimental results demonstrate that MicroMoE improves the end-to-end training throughput by up to 47.6% compared with the state-of-the-art system, and almost consistently achieves optimal load balance among GPUs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [NX-CGRA: A Programmable Hardware Accelerator for Core Transformer Algorithms on Edge Devices](https://arxiv.org/abs/2511.17235)
*Rohit Prasad*

Main category: cs.AR

TL;DR: 与DSL或图处理或MLIR或编译器或HLS相关联的部分：编译器，HLS（硬件加速器设计和CGRA）。
太长不看：本文提出了NX-CGRA，一个采用粗粒度可重构阵列（CGRA）架构的可编程硬件加速器，用以高效支持边缘侧的各种Transformer推理算法（包括线性和非线性函数）。NX-CGRA通过软件驱动的可编程性实现对各种内核模式的高效执行，在评估中展示了高效率和良好的能耗-面积权衡，表明其是受限功率和硅预算下边缘Transformer部署的可扩展和适应性解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着边缘侧Transformer工作负载的多样性和复杂性不断增加，在性能、能效和架构灵活性之间取得平衡成为了一个重大挑战。现有的固定功能加速器往往只针对狭窄用例进行优化。因此，需要一个可编程、高效、灵活的硬件加速器来支持范围更广的Transformer推理算法，特别是在受限的功耗和硅预算下。

Method:  NX-CGRA采用粗粒度可重构阵列（CGRA）架构，旨在支持Transformer推理算法中的线性和非线性函数。这种架构提供了软件驱动的可编程性，使其能够高效地执行各种内核模式。该方法通过使用源自真实世界Transformer模型的代表性基准进行评估。

Result:  NX-CGRA在源自真实世界Transformer模型的代表性基准上进行了评估，结果表明它在不同类别的操作中展现出较高的整体效率和良好的能耗-面积权衡。这表明NX-CGRA有潜力成为一种可扩展、适应性强的硬件解决方案，适用于在功率和硅预算受限的情况下在边缘部署Transformer。

Conclusion: NX-CGRA是一种可编程硬件加速器，专为支持包括线性和非线性功能的各种Transformer推理算法而设计。它采用粗粒度可重构阵列（CGRA）架构，通过软件驱动的可编程性，能够高效执行各种内核模式。评估结果表明，NX-CGRA在不同的操作类别中具有较高的整体效率和良好的能耗-面积权衡，显示了其作为在受限功率和硅预算下的边缘Transformer部署的可扩展和适应性硬件解决方案的潜力。

Abstract: The increasing diversity and complexity of transformer workloads at the edge present significant challenges in balancing performance, energy efficiency, and architectural flexibility. This paper introduces NX-CGRA, a programmable hardware accelerator designed to support a range of transformer inference algorithms, including both linear and non-linear functions. Unlike fixed-function accelerators optimized for narrow use cases, NX-CGRA employs a coarse-grained reconfigurable array (CGRA) architecture with software-driven programmability, enabling efficient execution across varied kernel patterns. The architecture is evaluated using representative benchmarks derived from real-world transformer models, demonstrating high overall efficiency and favorable energy-area tradeoffs across different classes of operations. These results indicate the potential of NX-CGRA as a scalable and adaptable hardware solution for edge transformer deployment under constrained power and silicon budgets.

</details>


### [3] [DISCA: A Digital In-memory Stochastic Computing Architecture Using A Compressed Bent-Pyramid Format](https://arxiv.org/abs/2511.17265)
*Shady Agwa,Yikang Shen,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: 涉及的领域有：编译器、HLS、图处理、MLIR、DSL。
不是。

当前的AI革命，特别是大规模AI模型的矩阵乘法任务，对能效提出了挑战，尤其是在边缘计算场景下（如机器人和无人机）。传统的冯·诺依曼架构和现有的内存计算（in-memory computing）方案都存在局限性。本文提出了一种名为DISCA的数字内存随机计算架构，它利用了一种压缩的准随机Bent-Pyramid数据格式。DISCA旨在结合模拟计算的计算简便性以及数字系统的可扩展性、生产力和可靠性。后布局建模结果显示，使用180nm CMOS技术实现的DISCA在500 MHz下能达到每位3.59 TOPS/W的能效，相对于现有架构显著提高了矩阵乘法工作负载的能效。


<details>
  <summary>Details</summary>
Motivation: 当前的AI革命需要处理大规模AI模型中的数据密集型矩阵乘法任务，这些模型的参数数量巨大（数百万甚至数十亿）。传统的冯·诺伊曼架构面临内存墙和摩尔定律终结的挑战。AI应用正快速向边缘迁移（如机器人和无人机），进一步增加了硬件预算的限制。现有的内存计算架构（包括模拟和数字）由于各种设计限制，其预期的优势遭受了严重的性能下降。因此，需要一种新的高效架构来解决这些挑战。

Method: 提出了一种新的数字内存随机计算（DISCA）架构。DISCA利用压缩版本的准随机Bent-Pyramid数据格式，结合了模拟计算的计算简便性以及数字系统的可扩展性、生产力和可靠性。使用商业180nm CMOS技术进行了后布局建模，评估了DISCA的能效。

Result: DISCA的后布局建模结果显示，在使用商业180nm CMOS技术以500 MHz运行时，其能效为每位3.59 TOPS/W。与同类架构相比，DISCA能在能效方面显著提升矩阵乘法工作负载的能效，提高了数个数量级。

Conclusion: DISCA在能效方面显著优于同类架构，通过利用准随机Bent-Pyramid数据格式和数字随机计算，为解决冯·诺依曼瓶颈提供了一个有前景的解决方案，特别适用于边缘AI应用中的矩阵乘法。

Abstract: Nowadays, we are witnessing an Artificial Intelligence revolution that dominates the technology landscape in various application domains, such as healthcare, robotics, automotive, security, and defense. Massive-scale AI models, which mimic the human brain's functionality, typically feature millions and even billions of parameters through data-intensive matrix multiplication tasks. While conventional Von-Neumann architectures struggle with the memory wall and the end of Moore's Law, these AI applications are migrating rapidly towards the edge, such as in robotics and unmanned aerial vehicles for surveillance, thereby adding more constraints to the hardware budget of AI architectures at the edge. Although in-memory computing has been proposed as a promising solution for the memory wall, both analog and digital in-memory computing architectures suffer from substantial degradation of the proposed benefits due to various design limitations. We propose a new digital in-memory stochastic computing architecture, DISCA, utilizing a compressed version of the quasi-stochastic Bent-Pyramid data format. DISCA inherits the same computational simplicity of analog computing, while preserving the same scalability, productivity, and reliability of digital systems. Post-layout modeling results of DISCA show an energy efficiency of 3.59 TOPS/W per bit at 500 MHz using a commercial 180nm CMOS technology. Therefore, DISCA significantly improves the energy efficiency for matrix multiplication workloads by orders of magnitude if scaled and compared to its counterpart architectures.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [4] [Efficient Algorithms and Implementations for Extracting Maximum-Size $(k,\ell)$-Sparse Subgraphs](https://arxiv.org/abs/2511.16877)
*Péter Madarasi*

Main category: cs.DS

TL;DR: 该论文与图处理（最大 $(k,\ell)$-稀疏子图问题、图算法、图库）相关。该论文提出了一种高度高效且灵活的增强路径算法实现，用于解决最大 $(k, \ell)$-稀疏子图问题，并引入了多种实用启发式方法（如边排序、节点排序、两阶段策略）以显著提高运行效率，同时保持最优性。此外，还提出了一个渐近更快的算法来提取一个包含上极大的 $(k, 2k)$-稀疏子图。实验表明，该实现比现有工具快了数个数量级，并建议将其纳入 LEMON 图形库。


<details>
  <summary>Details</summary>
Motivation: 最大 $(k, \ell)$-稀疏子图问题是刚性理论和组合优化领域的一个经典问题。虽然已知的多项式时间算法可以解决此问题，但在实际应用中，需要一个更高效和灵活的方法。尤其是对于大规模图和 3D 刚性等特定应用场景，现有工具可能效率不足。因此，本文的动机是开发一个高度高效且灵活的实现，以显著减少运行时间。

Method: 本文的核心方法是基于增强路径（augmenting path）算法，用于解决最大 $(k, \ell)$-稀疏子图问题。为了提高效率，作者引入了一系列实用启发式方法，包括边排序、节点排序、两阶段策略和基于伪森林的初始化。这些启发式方法旨在优化算法的执行流程，例如，倾向于在早期接受更多边并避免代价高昂的增广操作。此外，对于提取一个包含上极大的 $(k, 2k)$-稀疏子图（稀疏性条件仅对大小至少为三的节点集要求），作者提出了一种渐近更快的算法。

Result: 通过结合增强路径算法与各种实用启发式方法（如边排序、节点排序、两阶段策略、基于伪森林的初始化），本文成功实现了一个最大 $(k, \ell)$-稀疏子图问题的求解器。实验评估显示，该实现相较于现有工具，在运行时间上提高了数个数量级。此外，还提出了一个渐近更快的算法，用于提取包含上极大的 $(k, 2k)$-稀疏子图，并为 3D 刚性应用提供了特别高效的工具（当 $k=3$ 时）。该精心打造的实现已公开发布，并建议集成到 LEMON 图形库中。

Conclusion: 本文介绍了实现一个增强路径算法来解决最大 $(k, \ell)$-稀疏子图问题的成果。通过结合一系列实用启发式方法，该实现显著提高了计算效率，并保持了最优性。例如，在 3D 刚性问题中，本文提出了一个渐近更快的算法来提取一个极大 $(k,2k)$-稀疏子图。实验评估表明，该实现相较于现有工具，在运行时间上提高了数个数量级。作者公开发布了该实现，并建议将其纳入 LEMON 图形库。

Abstract: A multigraph $G = (V, E)$ is $(k, \ell)$-sparse if every subset $X \subseteq V$ induces at most $\max\{k|X| - \ell, 0\}$ edges. Finding a maximum-size $(k, \ell)$-sparse subgraph is a classical problem in rigidity theory and combinatorial optimization, with known polynomial-time algorithms. This paper presents a highly efficient and flexible implementation of an augmenting path method, enhanced with a range of powerful practical heuristics that significantly reduce running time while preserving optimality. These heuristics $\unicode{x2013}$ including edge-ordering, node-ordering, two-phase strategies, and pseudoforest-based initialization $\unicode{x2013}$ steer the algorithm toward accepting more edges early in the execution and avoiding costly augmentations. A comprehensive experimental evaluation on both synthetic and real-world graphs demonstrates that our implementation outperforms existing tools by several orders of magnitude. We also propose an asymptotically faster algorithm for extracting an inclusion-wise maximal $(k,2k)$-sparse subgraph with the sparsity condition required only for node sets of size at least three, which is particularly relevant to 3D rigidity when $k = 3$. We provide a carefully engineered implementation, which is publicly available online and is proposed for inclusion in the LEMON graph library.

</details>


### [5] [Low-Sensitivity Matching via Sampling from Gibbs Distributions](https://arxiv.org/abs/2511.16918)
*Yuichi Yoshida,Zihan Zhang*

Main category: cs.DS

TL;DR: 相关领域：图处理。
太长不看：本文从敏感度的角度研究了最大匹配问题。敏感度是通过最大 Wasserstein 距离（基于 Hamming 距离）来衡量的，它量化了删除一条边对算法输出分布的影响。作者提出了 $(\mathbf{1}-\varepsilon)$-近似的多项式时间算法，显著改善了已知敏感度界限：对于最大度为 $\Delta$ 的图，敏感度降至 $\Delta^{O(1/\varepsilon)}$；对于平面图和二分图，算法运行时间更快且敏感度更优；对于一般图，敏感度降至 $\sqrt{n} \cdot (\varepsilon^{-1} \log n)^{O(1/\varepsilon)}$。这些新算法主要基于匹配的 Gibbs 分布采样。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于研究最大匹配问题的**敏感度**（Sensitivity）。敏感度衡量了算法输出分布对图结构微小变化（即删除一条边）的鲁棒性。具体来说，本文旨在为最大匹配问题设计出具有较低敏感度界限的 $(1-\varepsilon)$-近似多项式时间算法，以显著改善先前已知的敏感度界限，从而提升算法的鲁棒性。

Method: 本文通过利用匹配的 Gibbs 分布采样，设计和分析改进的近似算法，从而改进了最大匹配问题的敏感度界限。主要方法包括：1. 基于 Gibbs 分布采样设计适用于有界最大度图的 $(1-\varepsilon)$-近似多项式时间算法，达到 $O_{\varepsilon, \Delta}(m \log m)$ 的运行时间。2. 针对平面图和二分图，设计更高效的 Gibbs 分布匹配采样算法，实现 $\mathrm{poly}(n/\varepsilon)$ 的运行时间，从而加快了 $\varepsilon$ 和 $\Delta$ 函数下的算法速度。3. 针对无界最大度的一般图，提出了一个改进敏感度界限的 $(1-\varepsilon)$-近似多项式时间算法。

Result: 本文取得了以下结果：1. 对于任意 $\varepsilon > 0$，存在一个多项式时间 $(1 - \varepsilon)$-近似算法，其敏感度上界为 $\Delta^{O(1/\varepsilon)}$，其中 $\Delta$ 是图的最大度。该算法基于 Gibbs 分布采样，运行时间为 $O_{\varepsilon, \Delta}(m \log m)$，显著优于先前的敏感度界限。2. 对于平面图和二分图，提出了运行时间显著加快（作为 $\varepsilon$ 和 $\Delta$ 的函数）的算法，其运行时间为 $\mathrm{poly}(n/\varepsilon)$。3. 对于具有无界最大度的一般图，存在一个多项式时间 $(1 - \varepsilon)$-近似算法，其敏感度上界为 $\sqrt{n} \cdot (\varepsilon^{-1} \log n)^{O(1/\varepsilon)}$，改善了先前的最佳界限 $O(n^{1/(1+\varepsilon^2)})$。

Conclusion: 本文研究了最大匹配问题的敏感度界限，并提出了一系列改进后的 $(1-\varepsilon)$-近似算法。对于有界最大度 $\Delta$ 的图，敏感度界限显著改善至 $\Delta^{O(1/\varepsilon)}$。对于平面图和二分图，我们给出了运行时间更快、敏感度界限更好的算法。对于无界最大度的一般图，敏感度界限改善至 $\sqrt{n} \cdot (\varepsilon^{-1} \log n)^{O(1/\varepsilon)}$。这些结果在算法的鲁棒性方面向前迈出了重要一步。

Abstract: In this work, we study the maximum matching problem from the perspective of sensitivity. The sensitivity of an algorithm $A$ on a graph $G$ is defined as the maximum Wasserstein distance between the output distributions of $A$ on $G$ and on $G - e$, where $G - e$ is the graph obtained by deleting an edge $e$ from $G$. The maximum is taken over all edges $e$, and the underlying metric for the Wasserstein distance is the Hamming distance.
  We first show that for any $\varepsilon > 0$, there exists a polynomial-time $(1 - \varepsilon)$-approximation algorithm with sensitivity $Δ^{O(1/\varepsilon)}$, where $Δ$ is the maximum degree of the input graph. The algorithm is based on sampling from the Gibbs distribution over matchings and runs in time $O_{\varepsilon, Δ}(m \log m)$, where $m$ is the number of edges in the graph. This result significantly improves the previously known sensitivity bounds.
  Next, we present significantly faster algorithms for planar and bipartite graphs as a function of $\varepsilon$ and $Δ$, which run in time $\mathrm{poly}(n/\varepsilon)$. This improvement is achieved by designing a more efficient algorithm for sampling matchings from the Gibbs distribution in these graph classes, which improves upon the previous best in terms of running time.
  Finally, for general graphs with potentially unbounded maximum degree, we show that there exists a polynomial-time $(1 - \varepsilon)$-approximation algorithm with sensitivity $\sqrt{n} \cdot (\varepsilon^{-1} \log n)^{O(1/\varepsilon)}$, improving upon the previous best bound of $O(n^{1/(1+\varepsilon^2)})$.

</details>


### [6] [Merging RLBWTs adaptively](https://arxiv.org/abs/2511.16953)
*Travis Gagie*

Main category: cs.DS

TL;DR: 本文与**图处理 (Graph Processing)**、**编译器 (Compiler)**、**MLIR** 和 **HLS** 均不相关。它与 **DSL (Domain-Specific Language)** 的关系很弱，因为它涉及特定于数据压缩和字符串处理的数据结构（BWT/RLBWT），可以被视为特定领域的底层工具，但不是传统意义上的 DSL 本身。
**TLDR:** 本文提出了一种高效合并行程压缩 BWT (RLBWT) 的方法，仅需 $O(R)$ 空间和 $\tilde{O} (L + σ+ R)$ 时间。其中 $R$ 是总行程数，$L$ 是合并后 eBWT 中**边界 LCP 值的和**，$σ$ 是字母表大小。该方法特别适用于底层字符串具有重复性但不相似，导致关键参数 $L$ 较小的情况。


<details>
  <summary>Details</summary>
Motivation: 现有的字符串处理和压缩技术中，Burrows-Wheeler 变换（BWT）及其行程压缩版本（RLBWT）是重要的数据结构。然而，在一个常见的需求场景中——即需要**合并**来自不同源的多个 RLBWTs（例如，在分布式处理或增量构建中），**快速且空间高效的合并方法**是缺乏的。特别地，当底层字符串具有重复性时，RLBWT 的压缩效率很高，但合并这些压缩结构可能会很耗时。因此，本文的动机是找到一种高效的合并算法，使得合并时间与总行程数 $R$ 呈线性关系，同时在**某些特定条件下**（即边界 LCP 值之和 $L$ 较小时）保持高效率。

Method: 本文提出了一种算法，用于在 $O(R)$ 空间内合并两个或多个行程压缩的 Burrows-Wheeler 变换（RLBWTs）。该方法的核心创新是利用在**合并后的扩展 Burrows-Wheeler 变换 (eBWT) 中，来自不同原始 RLBWT 的字符块边界处的 LCP（最长公共前缀）值之和 $L$**，来界定合并过程的时间复杂度。具体地，算法实现了 $\tilde{O} (L + σ+ R)$ 的时间复杂度，其中 $R$ 是所有 RLBWTs 中的总行程数，$σ$ 是字母表的大小。这种方法利用了 $L$ 较小时的效率优势，针对性地优化了数据结构聚合操作。

Result: 本文展示了一种在 $O(R)$ 空间内快速合并行程压缩的 Burrows-Wheeler 变换（RLBWTs）的方法，条件是参数 $L$ 较小。该方法的具体时间复杂度为 $\tilde{O} (L + σ+ R)$，其中 $R$ 是总行程数，$σ$ 是字母表大小。这里的 $L$ 定义为合并后的扩展 BWT（eBWT）中，**来自同一原始 RLBWT 的字符块边界上的最长公共前缀（LCP）值之和**。作者推测，当底层字符串（或字符串集）具有重复性但不相似时，$L$ 倾向于较小，从而保证了合并的高效性。

Conclusion: 本文提出了一种**在特定条件下**快速且空间高效地合并行程压缩的 Burrows-Wheeler 变换（RLBWTs）的方法。其核心是利用合并后的扩展 BWT（eBWT）中，**原始 RLBWTs 边界处的 LCP 值之和 L** 作为时间复杂度的关键参数。该方法在 $\tilde{O} (L + σ+ R)$ 时间内完成合并，其中 $R$ 是总行程数，$σ$ 是字母表大小。作者推测，当底层字符串（或字符串集）具有重复性但不相似时，$L$ 通常会较小，从而确保合并过程的高效率。这项工作为处理和聚合压缩字符串数据提供了一种新的、优化的工具。

Abstract: We show how to merge run-length compressed Burrows-Wheeler Transforms (RLBWTs) quickly and in $O (R)$ space, where $R$ is the total number of runs in them, when a certain parameter is small. Specifically, we consider the boundaries in their combined extended Burrows-Wheeler Transform (eBWT) between blocks of characters from the same original RLBWT, and denote by $L$ the sum of the longest common prefix (LCP) values at those boundaries. We show how to merge the RLBWTs in $\tilde{O} (L + σ+ R)$ time, where $σ$ is the alphabet size. We conjecture that $L$ tends to be small when the strings (or sets of strings) underlying the original RLBWTs are repetitive but dissimilar.

</details>


### [7] [Triangle Detection in H-Free Graphs](https://arxiv.org/abs/2511.17224)
*Amir Abboud,Ron Safier,Nathan Wallheimer*

Main category: cs.DS

TL;DR: 该论文与图处理（Graph Processing）相关。它开创了对$H$-free图上的三角形检测组合算法的研究。该研究旨在建立一个二分定理，用于分类哪些禁忌子图$H$可以在不使用快速矩阵乘法的情况下实现亚立方加速。作者证明了某些模式（非3可着色的或含多个三角形）没有组合加速的可能，并为“可嵌入”模式设计了一个强亚立方时间复杂度$\tilde O(n^{3-\frac{1}{2^{k-3}}})$的组合算法。这还推广到了小模式的完整分类和高效的奇数循环专用算法。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是探索在不使用快速矩阵乘法等“非组合”工具的情况下，是否可以对$H$-free图上的三角形检测问题实现亚立方（subcubic）加速。这旨在建立一个分类定理（二分定理），明确哪些禁忌模式$H$能允许这种速度提升，哪些不能。这对于在特定结构的图上设计高效的组合算法具有重要的理论意义。

Method: 论文采用的方法主要集中在两个方面：理论分析和算法设计。在理论分析方面，作者使用下界证明来确定哪些禁忌模式不可能通过组合方法获得亚立方加速（即非3可着色或包含多个三角形的模式）。在上界方面，作者开发了一个基于“嵌入（embedding）”的组合算法框架，这允许他们为“可嵌入”模式获得强亚立方时间复杂度的算法$\tilde O(n^{3-\frac{1}{2^{k-3}}})$。此外，论文还通过推广嵌入方法来处理带有“单个障碍”的模式，并提出了$H$-敏感算法和针对奇数循环的定制高效算法。

Result: 主要结果包括：
1. 下界：如果禁忌模式$H$不是3-可着色的或包含多于一个三角形，则问题的复杂性保持不变，组合加速的可能性不大。
2. 上界（一般模式）：对于一类“可嵌入”的模式$H$，提出了一个组合亚立方算法，运行时间为$\tilde O(n^{3-\frac{1}{2^{k-3}}})$，且该算法可用于列举所有三角形。
3. 推广和分类：推广了嵌入方法以处理带有单个障碍的模式，完成了对阶数最大至八的小模式的二分法分类。同时，提出了$H$-敏感算法。
4. 特殊模式（奇数循环）：提出了高效的定制组合算法：对于$C_{2k+1}$-free图，时间复杂度为$\tilde O(m+n^{1+2/k})$；对于$C_5$-敏感算法，时间复杂度为$\tilde O(n^2+n^{4/3}t^{1/3})$。

Conclusion: 这篇论文开创性地研究了在$H$-free图上进行三角形检测的组合算法，并朝着为该问题建立二分定理的目标迈进。作者成功地为一类“可嵌入”和“可嵌入且带有一个障碍”的模式开发了强亚立方（strongly subcubic）的组合算法，并在小模式（最大至八阶）上完成了二分法分类。特别地，对于奇数循环模式，论文提出了高效的专用组合算法。这为图算法领域中，在无禁忌子图约束下，实现超越传统矩阵乘法界限的效率提供了新的理论基础和方法。

Abstract: We initiate the study of combinatorial algorithms for Triangle Detection in $H$-free graphs. The goal is to decide if a graph that forbids a fixed pattern $H$ as a subgraph contains a triangle, using only "combinatorial" methods that notably exclude fast matrix multiplication. Our work aims to classify which patterns admit a subcubic speedup, working towards a dichotomy theorem. On the lower bound side, we show that if $H$ is not $3$-colorable or contains more than one triangle, the complexity of the problem remains unchanged, and no combinatorial speedup is likely possible. On the upper bound side, we develop an embedding approach that results in a strongly subcubic, combinatorial algorithm for a rich class of "embeddable" patterns. Specifically, for an embeddable pattern of size $k$, our algorithm runs in $\tilde O(n^{3-\frac{1}{2^{k-3}}})$ time, where $\tilde O(\cdot)$ hides poly-logarithmic factors. This algorithm also extends to listing all the triangles within the same time bound. We supplement this main result with two generalizations: 1) A generalization to patterns that are embeddable up to a single obstacle that arises from a triangle in the pattern. This completes our classification for small patterns, yielding a dichotomy theorem for all patterns of size up to eight. 2) An $H$-sensitive algorithm for embeddable patterns, which runs faster when the number of copies of $H$ is significantly smaller than the maximum possible $Ω(n^k)$. Finally, we focus on the special case of odd cycles. We present specialized Triangle Detection algorithms that are very efficient: 1) A combinatorial algorithm for $C_{2k+1}$-free graphs that runs in $\tilde O(m+n^{1+2/k})$ time for every $k\geq2$, where $m$ is the number of edges in the graph. 2) A combinatorial $C_5$-sensitive algorithm that runs in $\tilde O(n^2+n^{4/3}t^{1/3})$ time, where $t$ is the number of $5$-cycles in the graph.

</details>


### [8] [Spectral Clustering with Side Information](https://arxiv.org/abs/2511.17326)
*Hendrik Fichtenberger,Michael Kapralov,Ekaterina Kochetkova,Silvio Lattanzi,Davide Mazzali,Weronika Wrzos-Kaminska*

Main category: cs.DS

TL;DR: 该论文不直接与 DSL、图处理（特指一般的图处理，本文专注于图聚类问题）、MLIR、编译器或 HLS 相关。它属于图聚类和图算法领域。

Too long; didn't read: 这篇论文研究了带有“植入解”的图聚类问题，其中图结构信息（$\epsilon$-稀疏割和 $\Omega(1)$-扩展社区）和带有 $\delta$ 误分类的顶点标签信息同时存在。本文的关键贡献是首次证明了通过结合这两种信息，可以在 $\approx \widetilde O(εδ)$ 的近似最优误分类率下解决聚类问题，并提出了一个亚线性时间算法，该算法的关键在于对“谱模糊”顶点的观察。第二个贡献是一个多项式时间算法，它能通过重加权图的边，在保持社区扩展性的同时，将原始图转换为割更稀疏（$\approx \widetilde O(εδ)$）的新图，从而改进社区结构。


<details>
  <summary>Details</summary>
Motivation: 在带“植入解”的图聚类问题中，通常的假设是簇在图上诱导出具有良好连接性（$Ω(1)$-扩展性）的子图，并形成一个 $ε$-稀疏割。这种设置下的图聚类问题已经被充分研究，并存在达到 $\approx ε$ 误分类率的有效算法。然而，在实际应用中，图的顶点通常带有标签信息，这些标签可能被独立地以概率 $δ$ 破坏。单独使用图结构或标签信息只能达到 $\min\{ε, δ\}$ 的误分类率。因此，本文的动机是探索是否可以将这两种信息源结合起来，以实现近似最优的误分类率 $\approx εδ$，并在有效性上追求亚线性时间算法。

Method: 本文提出了一个亚线性时间算法来解决带有受损标签的植入解图聚类问题。关键算法思想是对“谱模糊”顶点的新观察。这个分类器可以达到近似最优的 $\approx \widetilde O(εδ)$ 误分类率。此外，对于常数 $k$，本文还提出了一个多项式时间算法，通过重新加权原始 $(k, ε, Ω(1))$-可聚类图的边，将其转换为一个 $(k, \widetilde O(εδ), Ω(1))$-可聚类图，从而在近似最优地提高割的稀疏性的同时，保持了社区的扩展性。

Result: 本文给出了对“是否可以结合图结构和受损标签信息来实现近似最优误分类率 $\approx εδ$”这一问题的肯定回答，并提出了一个处理此问题的亚线性时间算法，该算法的误分类率达到了近似最优的 $\approx \widetilde O(εδ)$。此外，还提出了第二个结果：一个多项式时间算法，它可以重新加权原始 $(k, ε, Ω(1))$-可聚类图的边，将其转化为一个 $(k, \widetilde O(εδ), Ω(1))$-可聚类图（对于常数 $k$），从而近似最优地提高了割的稀疏性，并保持了社区的扩展性。

Conclusion: 本文提出了一个亚线性时间分类器，首次证明了通过结合图结构和顶点标签信息，可以在近似最优的 $\approx \widetilde O(εδ)$ 误分类率下解决“植入解”的图聚类问题。此外，还提出了一个多项式时间算法，通过重新加权图的边，将初始的可聚类图转化为一个具有更高稀疏性和保持社区扩展性的可聚类图，从而改进了输入图的社区结构。这些结果为在更少的错误和亚线性时间内解决带有附加信息的图聚类问题提供了理论和算法上的进展。

Abstract: In the graph clustering problem with a planted solution, the input is a graph on $n$ vertices partitioned into $k$ clusters, and the task is to infer the clusters from graph structure. A standard assumption is that clusters induce well-connected subgraphs (i.e. $Ω(1)$-expanders), and form $ε$-sparse cuts. Such a graph defines the clustering uniquely up to $\approx ε$ misclassification rate, and efficient algorithms for achieving this rate are known. While this vanilla version of graph clustering is well studied, in practice, vertices of the graph are typically equipped with labels that provide additional information on cluster ids of the vertices. For example, each vertex could have a cluster label that is corrupted independently with probability $δ$. Using only one of the two sources of information leads to misclassification rate $\min\{ε, δ\}$, but can they be combined to achieve a rate of $\approx εδ$?
  In this paper, we give an affirmative answer to this question and present a sublinear-time algorithm in the number of vertices $n$. Our key algorithmic insight is a new observation on ``spectrally ambiguous'' vertices in a well-clusterable graph.
  While our sublinear-time classifier achieves the nearly optimal $\approx \widetilde O(εδ)$ misclassification rate, the approximate clusters that it outputs do not necessarily induce expanders in the graph $G$. In our second result, we give a polynomial-time algorithm that reweights edges of the original $(k, ε, Ω(1))$-clusterable graph to transform it into a $(k, \widetilde O(εδ), Ω(1))$-clusterable one (for constant $k$), improving sparsity of cuts nearly optimally and preserving expansion properties of the communities - an algorithm for refining community structure of the input graph.

</details>


### [9] [Relative Error Streaming Quantiles with Seamless Mergeability via Adaptive Compactors](https://arxiv.org/abs/2511.17396)
*Tomáš Domes,Pavel Veselý*

Main category: cs.DS

TL;DR: This paper is related to graph processing (data summarization/quantile summaries can be seen as a form of graph processing on data streams, though less direct than typical graph problems). A simplified and refined version of ReqSketch (a space-efficient quantile summary) is proposed, utilizing "adaptive compactors" to significantly simplify the proof of relative error guarantees in the mergeability setting, while maintaining the original space bound and algorithmic simplicity, and achieving near-optimal space bounds in specific merging scenarios.


<details>
  <summary>Details</summary>
Motivation: 现有的 ReqSketch 是目前最节省空间的 quantile 摘要算法，具有相对误差保证和可合并性，并在实践中得到了应用（例如在 Apache DataSketches 库中）。然而，ReqSketch 的可合并性证明过于复杂，需要复杂的“收费论证”（charging argument）和复杂的方差分析。因此，本文的动机是提供一个 ReqSketch 的改进版本，以简化其可合并性的证明，同时保持其核心性能和社会性。

Method: 本文通过开发“自适应压缩器”（adaptive compactors）来改进当前的 ReqSketch。这种改进使得研究人员能够以显著简化的方式证明在最通用可合并设置下的相对误差保证，同时保留了原 ReqSketch 的空间复杂度、更新时间和算法的简洁性。新的 sketch 利用其自适应性，结合新的证明技术，在特定场景（特别是合并大小相近的 sketch 时）实现了近乎最优的空间界限。

Result: 本文提出的基于自适应压缩器的改进版 ReqSketch 成功地在最一般的可合并设置下，以显著简化的方式证明了相对误差保证，同时保持了原始 ReqSketch 的空间复杂度、更新时间和算法的简洁性。此外，在合并大小相近的 sketch 等特定场景中，该改进版 sketch 实现了近乎最优的空间界限。

Conclusion: 本文提出了一种改进的 ReqSketch，称为基于自适应压缩器的 ReqSketch。这种改进的关键在于引入了自适应压缩器，它简化了在最通用可合并设置下相对误差保证的证明，同时保持了原始 ReqSketch 的空间复杂度、更新时间和算法的简洁性。此外，新 sketch 的自适应性及其证明技术，在合并大小相近的 sketch 等特定场景中，实现了近乎最优的空间界限。

Abstract: Quantile summaries provide a scalable way to estimate the distribution of individual attributes in large datasets that are often distributed across multiple machines or generated by sensor networks. ReqSketch (arXiv:2004.01668) is currently the most space-efficient summary with two key properties: relative error guarantees, offering increasingly higher accuracy towards the distribution's tails, and mergeability, allowing distributed or parallel processing of datasets. Due to these features and its simple algorithm design, ReqSketch has been adopted in practice, via implementation in the Apache DataSketches library. However, the proof of mergeability in ReqSketch is overly complicated, requiring an intricate charging argument and complex variance analysis.
  In this paper, we provide a refined version of ReqSketch, by developing so-called adaptive compactors. This enables a significantly simplified proof of relative error guarantees in the most general mergeability setting, while retaining the original space bound, update time, and algorithmic simplicity. Moreover, the adaptivity of our sketch, together with the proof technique, yields near-optimal space bounds in specific scenarios - particularly when merging sketches of comparable size.

</details>
