<div id=toc></div>

# Table of Contents

- [cs.DS](#cs.DS) [Total: 6]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [1] [Efficient Algorithms and Implementations for Extracting Maximum-Size $(k,\ell)$-Sparse Subgraphs](https://arxiv.org/abs/2511.16877)
*Péter Madarasi*

Main category: cs.DS

TL;DR: This paper is related to graph processing (finding maximum-size $(k, \ell)$-sparse subgraph; multigraphs; augmenting path method; pseudoforest; graph library). 该论文实现并高度优化了一种用于寻找最大 $(k, \ell)$-稀疏子图的增广路径算法，通过引入多种实用启发式策略（如排序和初始化），显著提高了运行效率，比现有工具快了几个数量级。同时，论文还提出了一个渐进更快的算法来解决特定的 $(k, 2k)$-稀疏子图问题，并发布了其实现。


<details>
  <summary>Details</summary>
Motivation: 寻找最大尺寸的 $(k, \ell)$-稀疏子图是刚性理论和组合优化领域的一个经典问题。尽管已有多项式时间算法，但在实际应用中，提升这些算法的效率和灵活性是其主要动机。特别地，对于 3D 刚性问题（当 $k=3$ 时）相关的 $(k, 2k)$-稀疏子图提取，仍需要更快、更工程化的解决方案。本文旨在提供一个高度高效且灵活的实现，以显著减少运行时间。

Method: 本文的核心方法是基于增广路径（augmenting path method），并辅以一系列实用的、保持最优性的启发式策略，以优化运行时间。这些启发式策略包括：边排序（edge-ordering）、节点排序（node-ordering）、两阶段策略（two-phase strategies）和基于伪森林的初始化（pseudoforest-based initialization）。这些策略的目的是让算法在执行初期更快地接受更多的边，并避免耗时的增广操作。此外，本文针对节点数至少为三的集合才要求稀疏性条件的 $(k, 2k)$-稀疏子图问题，提出了一种渐进更快的算法。

Result: 本文提出的增广路径实现及其优化的启发式策略，在对合成图和真实世界图的全面实验评估中，展现出比现有工具快几个数量级（several orders of magnitude）的性能提升。此外，还提出了针对特定 $(k, 2k)$-稀疏子图的渐进更快算法，并提供了一个精心设计的、公开可用的实现。

Conclusion: 本文实现并优化了一种基于增广路径的方法，用于求解最大尺寸的 $(k, \ell)$-稀疏子图问题。通过引入一系列高效的实用启发式策略，实验证明该方法在运行时间上比现有工具快了几个数量级。此外，本文还提出了一种渐进更快的算法，用于提取最大包含性 $(k, 2k)$-稀疏子图（稀疏性条件仅需对至少包含三个节点的集合满足），并提供了精心设计的实现，建议纳入 LEMON 图库。

Abstract: A multigraph $G = (V, E)$ is $(k, \ell)$-sparse if every subset $X \subseteq V$ induces at most $\max\{k|X| - \ell, 0\}$ edges. Finding a maximum-size $(k, \ell)$-sparse subgraph is a classical problem in rigidity theory and combinatorial optimization, with known polynomial-time algorithms. This paper presents a highly efficient and flexible implementation of an augmenting path method, enhanced with a range of powerful practical heuristics that significantly reduce running time while preserving optimality. These heuristics $\unicode{x2013}$ including edge-ordering, node-ordering, two-phase strategies, and pseudoforest-based initialization $\unicode{x2013}$ steer the algorithm toward accepting more edges early in the execution and avoiding costly augmentations. A comprehensive experimental evaluation on both synthetic and real-world graphs demonstrates that our implementation outperforms existing tools by several orders of magnitude. We also propose an asymptotically faster algorithm for extracting an inclusion-wise maximal $(k,2k)$-sparse subgraph with the sparsity condition required only for node sets of size at least three, which is particularly relevant to 3D rigidity when $k = 3$. We provide a carefully engineered implementation, which is publicly available online and is proposed for inclusion in the LEMON graph library.

</details>


### [2] [Low-Sensitivity Matching via Sampling from Gibbs Distributions](https://arxiv.org/abs/2511.16918)
*Yuichi Yoshida,Zihan Zhang*

Main category: cs.DS

TL;DR: 该论文与图处理相关。
本文从敏感性角度研究最大匹配问题，敏感性定义为删除一条边后输出分布（使用 Wasserstein 距离衡量）的最大变化。主要贡献包括：对于最大度有界的图，提出了一个多项式时间 $(1-\varepsilon)$-近似算法，敏感性为 $\Delta^{O(1/\varepsilon)}$，运行时间为 $O_{\varepsilon, \Delta}(m \log m)$，显著优于先前结果；对于平面图和二分图，设计了运行时间为 $\mathrm{poly}(n/\varepsilon)$ 的更快算法；对于最大度无界的一般图，敏感性界限改进到 $\sqrt{n} \cdot (\varepsilon^{-1} \log n)^{O(1/\varepsilon)}$。这些算法主要基于吉布斯分布采样。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是研究最大匹配问题（Maximum Matching problem）的敏感性（sensitivity），即算法对图结构微小变化（如删除一条边）的鲁棒性。敏感性是用输出分布在输入图 $G$ 和 $G-e$ 上的最大 Wasserstein 距离（以 Hamming 距离为基础）来衡量的。作者旨在改进现有最大匹配近似算法的敏感性界限和运行时间。

Method: 本文通过采样来自匹配上的吉布斯分布（Gibbs distribution）来设计近似算法。具体来说，首先通过吉布斯采样设计了一个对任何 $\varepsilon > 0$ 存在的多项式时间 $(1-\varepsilon)$-近似算法，其敏感度界限为 $\Delta^{O(1/\varepsilon)}$。对于平面图和二分图，通过设计更高效的匹配吉布斯分布采样算法，获得了显著更快的运行时间 $\mathrm{poly}(n/\varepsilon)$。最后，对于最大度无界的常规图，也提出了基于采样的改进算法。

Result: 1. **一般图（有界最大度 $\Delta$）**: 存在一个多项式时间 $(1 - \varepsilon)$-近似算法，其敏感性界限为 $\Delta^{O(1/\varepsilon)}$，相比先前结果有显著改进。该算法基于吉布斯分布采样，运行时间为 $O_{\varepsilon, \Delta}(m \log m)$。
2. **特殊图（平面图和二分图）**: 提出了运行时间为 $\mathrm{poly}(n/\varepsilon)$ 的显著更快的算法，该改进是通过设计更有效的吉布斯分布匹配采样算法实现的。
3. **一般图（无界最大度）**: 存在一个多项式时间 $(1 - \varepsilon)$-近似算法，其敏感性界限为 $\sqrt{n} \cdot (\varepsilon^{-1} \log n)^{O(1/\varepsilon)}$，改进了先前最优的 $O(n^{1/(1+\varepsilon^2)})$ 界限。

Conclusion: 本文研究了最大匹配问题的敏感性界限，改进了现有算法的敏感性和运行时间。对于一般图，提出了一个多项式时间 $(1-\varepsilon)$-近似算法，其敏感性界限显著优于先前结果。对于平面图和二分图，设计了更快的算法。这些结果为理解和设计具有稳定性的图算法提供了新的见解。

Abstract: In this work, we study the maximum matching problem from the perspective of sensitivity. The sensitivity of an algorithm $A$ on a graph $G$ is defined as the maximum Wasserstein distance between the output distributions of $A$ on $G$ and on $G - e$, where $G - e$ is the graph obtained by deleting an edge $e$ from $G$. The maximum is taken over all edges $e$, and the underlying metric for the Wasserstein distance is the Hamming distance.
  We first show that for any $\varepsilon > 0$, there exists a polynomial-time $(1 - \varepsilon)$-approximation algorithm with sensitivity $Δ^{O(1/\varepsilon)}$, where $Δ$ is the maximum degree of the input graph. The algorithm is based on sampling from the Gibbs distribution over matchings and runs in time $O_{\varepsilon, Δ}(m \log m)$, where $m$ is the number of edges in the graph. This result significantly improves the previously known sensitivity bounds.
  Next, we present significantly faster algorithms for planar and bipartite graphs as a function of $\varepsilon$ and $Δ$, which run in time $\mathrm{poly}(n/\varepsilon)$. This improvement is achieved by designing a more efficient algorithm for sampling matchings from the Gibbs distribution in these graph classes, which improves upon the previous best in terms of running time.
  Finally, for general graphs with potentially unbounded maximum degree, we show that there exists a polynomial-time $(1 - \varepsilon)$-approximation algorithm with sensitivity $\sqrt{n} \cdot (\varepsilon^{-1} \log n)^{O(1/\varepsilon)}$, improving upon the previous best bound of $O(n^{1/(1+\varepsilon^2)})$.

</details>


### [3] [Merging RLBWTs adaptively](https://arxiv.org/abs/2511.16953)
*Travis Gagie*

Main category: cs.DS

TL;DR: 该论文与 DSL、图处理、MLIR、编译器或 HLS 无关。

太长不看：本文提出了一种新的、高效且空间优化（$O(R)$ 空间）的方法来合并多个行程长度压缩的 Burrows-Wheeler 变换（RLBWTs），时间复杂度为 $\tilde{O} (L + σ+ R)$，其中 $L$ 是一个新的参数，即合并后的 eBWT 中，原始 RLBWTs 边界处的 LCP 值之和。作者推测该方法在重复但彼此不相似的字符串集合上效率高。


<details>
  <summary>Details</summary>
Motivation: RLBWTs（行程长度压缩的 Burrows-Wheeler 变换）作为一种重要的数据结构，用于压缩和索引具有重复性的文本数据。本文的动机是找到一种高效且空间优化的方法来合并多个已有的 RLBWTs，这在处理大规模、重复的文本集合或动态更新集合时非常有用。

Method: 文章提出了一种在特定参数较小时，能以 $O(R)$ 空间复杂度快速合并 RLBWTs 的方法。其时间复杂度为 $\tilde{O} (L + σ+ R)$，其中 $R$ 是总行程数，$σ$ 是字母表大小，$L$ 是在合并的扩展 Burrows-Wheeler 变换（eBWT）中，来自不同原始 RLBWT 的字符块边界处的 LCP（最长公共前缀）值的总和。

Result: 本文表明，当一个特定参数 $L$ 较小时，可以快速且在 $O(R)$ 空间内合并 RLBWTs。具体地，合并所需的时间复杂度为 $\tilde{O} (L + σ+ R)$，其中 $L$ 是一个关键参数，它定义为合并后的 eBWT 中，原始 RLBWTs 字符块边界处的 LCP 值之和。作者还推测，当原始字符串（或字符串集）具有重复性但彼此不相似时，$L$ 值会较小，从而证明该方法在这些场景下的高效性。

Conclusion: 本文提出了一种高效且空间复杂度为 $O(R)$ 的方法来合并行程长度压缩的 Burrows-Wheeler 变换（RLBWTs），其中 $R$ 是总的行程数，尤其适用于特定参数较小的情况。方法的主要时间复杂度为 $\tilde{O} (L + σ+ R)$，该上界依赖于一个参数 $L$（边界处的 LCP 值之和）。作者推测当原始字符串具有重复性但彼此不相似时，$L$ 趋向于较小，这意味着本文的合并方法在这些情况下尤其高效。

Abstract: We show how to merge run-length compressed Burrows-Wheeler Transforms (RLBWTs) quickly and in $O (R)$ space, where $R$ is the total number of runs in them, when a certain parameter is small. Specifically, we consider the boundaries in their combined extended Burrows-Wheeler Transform (eBWT) between blocks of characters from the same original RLBWT, and denote by $L$ the sum of the longest common prefix (LCP) values at those boundaries. We show how to merge the RLBWTs in $\tilde{O} (L + σ+ R)$ time, where $σ$ is the alphabet size. We conjecture that $L$ tends to be small when the strings (or sets of strings) underlying the original RLBWTs are repetitive but dissimilar.

</details>


### [4] [Triangle Detection in H-Free Graphs](https://arxiv.org/abs/2511.17224)
*Amir Abboud,Ron Safier,Nathan Wallheimer*

Main category: cs.DS

TL;DR: 这个论文是关于图处理（Graph Processing）和算法（Algorithm）的。更具体地说，它研究了在$H$-自由图上进行三角形检测（Triangle Detection）的组合算法的复杂性，旨在寻求亚立方时间的加速，并开发了基于“嵌入方法”的高效算法，最终对小尺寸模式得到了一个二分定理（Dichotomy Theorem）。
总结：本文在$H$-自由图上开创了三角形检测组合算法的研究，旨在分类哪些禁止模式$H$能实现亚立方时间加速（不使用快速矩阵乘法）。作者证明了对于非3-可着色或包含多个三角形的$H$，加速的可能性不大。同时，他们为一类“可嵌入”模式开发了亚立方的组合算法，运行时间为$\tilde O(n^{3-\frac{1}{2^{k-3}}})$。这些结果被泛化，并完成了对尺寸达八模式的分类，提出了一个二分定理。此外，作者还提供了针对奇数循环（如$C_{2k+1}$和$C_5$）的高效专门算法。


<details>
  <summary>Details</summary>
Motivation: 现有的图算法（如三角形检测）通常依赖于快速矩阵乘法，其时间复杂度难以超越立方时间。作者的动机是探索在图子结构受限（即$H$-自由图）的条件下，是否可以利用纯组合方法（排除快速矩阵乘法）实现亚立方时间的加速。他们的目标是系统性地理解和分类哪些禁止模式$H$允许这种组合加速，从而迈向一个关于$H$-自由图上三角形检测复杂度的二分定理。

Method: - **问题定义与目标**: 研究在$H$-自由图中检测三角形的组合算法，目标是绕过快速矩阵乘法，寻求亚立方时间的加速，并对可加速的模式进行分类以建立一个二分定理。
- **下界分析**: 证明如果禁止模式$H$不是3可着色的或包含多于一个三角形，则问题的复杂度保持不变，组合加速的可能性不大。
- **上界方法（嵌入方法）**: 针对“可嵌入”的模式，开发了一种嵌入方法，得到了强亚立方时间的组合算法，运行时间为$\tilde O(n^{3-\frac{1}{2^{k-3}}})$，并可扩展到三角形列表。
- **泛化与分类**:
    1. 泛化到包含单个障碍物的可嵌入模式，并完成了尺寸达八的模式的分类，得到了一个二分定理。
    2. 开发了一个$H$-敏感算法，当$H$的实例数较少时，能够更快地运行。
- **奇数循环的特殊情况**: 提供了针对$C_{2k+1}$-自由图的专门高效组合算法（$\tilde O(m+n^{1+2/k})$）和$C_5$-敏感算法（$\tilde O(n^2+n^{4/3}t^{1/3})$）。

Result: - **下界结果**: 如果$H$不是$3$-可着色的或包含多于一个三角形，则三角形检测的复杂度保持不变，组合加速不太可能。
- **上界结果（一般模式）**: 针对$\text{k}$尺寸的“可嵌入”模式，开发了一个组合算法，其运行时间为$\tilde O(n^{3-\frac{1}{2^{k-3}}})$，并可用于列举所有三角形。
- **分类结果**: 提出了两种泛化：
    1. 泛化到“可嵌入但包含单个障碍物”的模式，完成了对尺寸达八的所有模式的分类，得到了一个二分定理。
    2. 提出了$H$-敏感算法，对于可嵌入模式，当$H$的实例数$t$显著小于最大可能值$\Omega(n^k)$时，运行速度更快。
- **奇数循环结果**:
    1. 针对$C_{2k+1}$-自由图的组合算法，运行时间为$\tilde O(m+n^{1+2/k})$。
    2. 针对$C_5$-敏感的组合算法，运行时间为$\tilde O(n^2+n^{4/3}t^{1/3})$，其中$t$是图中的5-循环数量。

Conclusion: 这篇论文开创性地研究了在$H$-自由图上进行三角形检测的组合算法，并朝着一个二分定理迈进。通过上下界分析和开发嵌入方法，作者为特定类别的禁止模式找到了亚立方时间的组合算法。特别是针对奇数循环，作者还提供了非常高效的专门算法。这些结果为理解禁止子图约束对图算法复杂度的影响提供了重要见解，并为进一步研究更通用的图模式检测奠定了基础。

Abstract: We initiate the study of combinatorial algorithms for Triangle Detection in $H$-free graphs. The goal is to decide if a graph that forbids a fixed pattern $H$ as a subgraph contains a triangle, using only "combinatorial" methods that notably exclude fast matrix multiplication. Our work aims to classify which patterns admit a subcubic speedup, working towards a dichotomy theorem. On the lower bound side, we show that if $H$ is not $3$-colorable or contains more than one triangle, the complexity of the problem remains unchanged, and no combinatorial speedup is likely possible. On the upper bound side, we develop an embedding approach that results in a strongly subcubic, combinatorial algorithm for a rich class of "embeddable" patterns. Specifically, for an embeddable pattern of size $k$, our algorithm runs in $\tilde O(n^{3-\frac{1}{2^{k-3}}})$ time, where $\tilde O(\cdot)$ hides poly-logarithmic factors. This algorithm also extends to listing all the triangles within the same time bound. We supplement this main result with two generalizations: 1) A generalization to patterns that are embeddable up to a single obstacle that arises from a triangle in the pattern. This completes our classification for small patterns, yielding a dichotomy theorem for all patterns of size up to eight. 2) An $H$-sensitive algorithm for embeddable patterns, which runs faster when the number of copies of $H$ is significantly smaller than the maximum possible $Ω(n^k)$. Finally, we focus on the special case of odd cycles. We present specialized Triangle Detection algorithms that are very efficient: 1) A combinatorial algorithm for $C_{2k+1}$-free graphs that runs in $\tilde O(m+n^{1+2/k})$ time for every $k\geq2$, where $m$ is the number of edges in the graph. 2) A combinatorial $C_5$-sensitive algorithm that runs in $\tilde O(n^2+n^{4/3}t^{1/3})$ time, where $t$ is the number of $5$-cycles in the graph.

</details>


### [5] [Spectral Clustering with Side Information](https://arxiv.org/abs/2511.17326)
*Hendrik Fichtenberger,Michael Kapralov,Ekaterina Kochetkova,Silvio Lattanzi,Davide Mazzali,Weronika Wrzos-Kaminska*

Main category: cs.DS

TL;DR: 是否与DSL、图处理、MLIR、编译器或HLS相关：本论文与**图处理**相关。
太长不读：在具有先验解的图聚类问题中，将图结构信息（$ε$-稀疏割，$Ω(1)$-扩展器）和受损的顶点标签信息（$\delta$ 错误率）结合，可以实现接近最优的 $\approx \widetilde O(εδ)$ 误分类率。本文提出了一个亚线性时间分类器来达到这个目标，并通过一个多项式时间算法，重新加权图的边，将 $(k, ε, Ω(1))$-可聚类图改进为具有更优稀疏割 $(k, \widetilde O(εδ), Ω(1))$-可聚类图，同时保持了社区的扩展性。


<details>
  <summary>Details</summary>
Motivation: 在图聚类问题中，标准的做法是利用图结构（例如，期望聚类能够诱导出具有 $Ω(1)$-扩展性和 $ε$-稀疏割的良好连通子图），以推断聚类。
在实践中，图的顶点通常带有标签，这些标签提供了关于顶点聚类 ID 的额外信息，但这些标签可能被以概率 $\delta$ 独立地破坏。
单独使用图结构信息或受损标签信息，只能达到 $\min\{ε, δ\}$ 的误分类率。
本文的动机在于探索是否可以将这两种信息来源（图结构和受损标签）结合起来，以达到接近最优的误分类率 $\approx εδ$。

Method: 本文提出了一个亚线性时间算法，其核心是一个关于“光谱模糊”顶点的新观察，这个分类器能够将图结构信息和顶点受损标签信息相结合。
其次，本文提出了一个多项式时间算法，用于通过重新加权原始 $(k, ε, Ω(1))$-可聚类图的边，将其转换成一个 $(k, \widetilde O(εδ), Ω(1))$-可聚类图。

Result: 本文有两个主要结果：
1. **亚线性时间分类器：** 提出了一个亚线性时间算法，实现了接近最优的 $\approx \widetilde O(εδ)$ 误分类率，它利用了一个关于“光谱模糊”顶点的新观察，有效地结合了图结构和受损标签的信息。
2. **社区结构优化算法（多项式时间）：** 提出了一个多项式时间算法，用于优化输入图的社区结构。该算法通过重新加权原始 $(k, ε, Ω(1))$-可聚类图的边，将其转换为一个具有更优稀疏割和保持扩展性的 $(k, \widetilde O(εδ), Ω(1))$-可聚类图（对于常数 $k$），从而实现了社区结构的改进。

Conclusion: 本文提出了一个亚线性时间分类器，能够将图结构信息和顶点标签信息结合起来，以达到接近于最优误分类率 $O(εδ)$ 的目标。此外，本文还提出了一个多项式时间算法，用于优化输入图的社区结构，通过重新加权边，将 $(k, ε, Ω(1))$-可聚类图转化为 $(k, \widetilde O(εδ), Ω(1))$-可聚类图，从而在保持社区扩展性的同时，显著减少了 $\epsilon$ 稀疏割，达到了改进社区结构的目的。

Abstract: In the graph clustering problem with a planted solution, the input is a graph on $n$ vertices partitioned into $k$ clusters, and the task is to infer the clusters from graph structure. A standard assumption is that clusters induce well-connected subgraphs (i.e. $Ω(1)$-expanders), and form $ε$-sparse cuts. Such a graph defines the clustering uniquely up to $\approx ε$ misclassification rate, and efficient algorithms for achieving this rate are known. While this vanilla version of graph clustering is well studied, in practice, vertices of the graph are typically equipped with labels that provide additional information on cluster ids of the vertices. For example, each vertex could have a cluster label that is corrupted independently with probability $δ$. Using only one of the two sources of information leads to misclassification rate $\min\{ε, δ\}$, but can they be combined to achieve a rate of $\approx εδ$?
  In this paper, we give an affirmative answer to this question and present a sublinear-time algorithm in the number of vertices $n$. Our key algorithmic insight is a new observation on ``spectrally ambiguous'' vertices in a well-clusterable graph.
  While our sublinear-time classifier achieves the nearly optimal $\approx \widetilde O(εδ)$ misclassification rate, the approximate clusters that it outputs do not necessarily induce expanders in the graph $G$. In our second result, we give a polynomial-time algorithm that reweights edges of the original $(k, ε, Ω(1))$-clusterable graph to transform it into a $(k, \widetilde O(εδ), Ω(1))$-clusterable one (for constant $k$), improving sparsity of cuts nearly optimally and preserving expansion properties of the communities - an algorithm for refining community structure of the input graph.

</details>


### [6] [Relative Error Streaming Quantiles with Seamless Mergeability via Adaptive Compactors](https://arxiv.org/abs/2511.17396)
*Tomáš Domes,Pavel Veselý*

Main category: cs.DS

TL;DR: 该论文与 DSL、图处理、MLIR、编译器或 HLS 无关。

TLDR: ReqSketch 是一种用于分布式大数据集的空间高效的分位数概要算法，但其可合并性证明过于复杂。本文通过引入“自适应压缩器”提出了一种精简版的 ReqSketch，它保持了原有的空间效率和算法简洁性，同时**显著简化了在最一般可合并性设置下相对误差保证的证明**，并能在特定场景（例如合并相似规模的草图）中实现**接近最优的空间界限**。


<details>
  <summary>Details</summary>
Motivation: ReqSketch 是一种在大型分布式数据集中估计个体属性分布的有效且空间高效的方法，具有相对误差保证和可合并性，并在实践中得到了应用。然而，**ReqSketch 的可合并性证明过于复杂**，需要复杂的归因和方差分析。因此，有必要提供一个精简版本，以简化证明的复杂性，同时保持其优势。

Method: 通过开发“自适应压缩器”（adaptive compactors），提出并精简了 ReqSketch 的新版本。利用这种自适应性，实现了显著简化的相对误差保证证明，特别是在最普遍的可合并性设置中。

Result: 新版本（带自适应压缩器的精简版 ReqSketch）保留了原始 ReqSketch 的空间界限、更新时间以及算法简洁性。它在最普遍的可合并性设置下，**显著简化了相对误差保证的证明**。此外，当合并相似大小的草图时，自适应草图及其证明技术可实现**接近最优的空间界限**。

Conclusion: 本文通过引入“自适应压缩器”，提供了一个精简版的 ReqSketch (Adaptive ReqSketch)。这个新版本在保持与原版相同的空间复杂度、更新时间以及算法简洁性的同时，显著简化了在最普遍的可合并性设置下相对误差保证的证明。此外，自适应特性及其证明技术在合并相似规模的草图等特定场景下实现了接近最优的空间界限。

Abstract: Quantile summaries provide a scalable way to estimate the distribution of individual attributes in large datasets that are often distributed across multiple machines or generated by sensor networks. ReqSketch (arXiv:2004.01668) is currently the most space-efficient summary with two key properties: relative error guarantees, offering increasingly higher accuracy towards the distribution's tails, and mergeability, allowing distributed or parallel processing of datasets. Due to these features and its simple algorithm design, ReqSketch has been adopted in practice, via implementation in the Apache DataSketches library. However, the proof of mergeability in ReqSketch is overly complicated, requiring an intricate charging argument and complex variance analysis.
  In this paper, we provide a refined version of ReqSketch, by developing so-called adaptive compactors. This enables a significantly simplified proof of relative error guarantees in the most general mergeability setting, while retaining the original space bound, update time, and algorithmic simplicity. Moreover, the adaptivity of our sketch, together with the proof technique, yields near-optimal space bounds in specific scenarios - particularly when merging sketches of comparable size.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling](https://arxiv.org/abs/2511.16947)
*Chenqi Zhao,Wenfei Wu,Linhai Song,Yuchen Xu*

Main category: cs.DC

TL;DR: Paper is related to graph processing or MLIR or compiler or HLS: No.
太长不看：MoE 模型存在负载不均衡问题影响训练效率，现有解决方案不佳。本文提出了 MicroEP 这种新型并行化策略，通过高效的令牌调度在每个微批次中实现细粒度负载均衡。在此基础上，构建了分布式 MoE 训练系统 MicroMoE。实验证明，MicroMoE 能将训练吞吐量提高高达 47.6%，并持续实现接近最佳的负载均衡。


<details>
  <summary>Details</summary>
Motivation: MoE 模型由于其动态特性，导致专家之间存在负载不平衡问题，严重影响训练效率。 现有解决方案要么牺牲模型精度，要么引入额外的系统开销，未能实现对优化训练效率至关重要的细粒度负载均衡。

Method: 提出了一种名为 MicroEP 的新型并行化策略，通过跨 GPU 的高效令牌调度，在每个微批次中实现细粒度的负载均衡。 基于 MicroEP，进一步设计并实现了高效的分布式 MoE 训练系统 MicroMoE。

Result: 实验结果表明，与现有最先进的系统相比，MicroMoE 将端到端训练吞吐量提高了高达 47.6%，并且几乎持续在 GPU 之间实现了最佳的负载均衡。

Conclusion: MicroEP是一种新颖的并行化策略，它能通过跨 GPU 的高效令牌调度在每个微批次中实现更精细的负载均衡。MicroMoE 是一个基于 MicroEP 的高效分布式 MoE 训练系统，与现有系统相比，它能显著提高端到端训练的吞吐量，并持续实现 GPU 间接近最佳的负载均衡。

Abstract: Mixture-of-Experts (MoE) has emerged as a promising approach to scale up deep learning models due to its significant reduction in computational resources. However, the dynamic nature of MoE leads to load imbalance among experts, severely impacting training efficiency. While previous research has attempted to address the load balancing challenge, existing solutions either compromise model accuracy or introduce additional system overhead. As a result, they fail to achieve fine-grained load balancing, which is crucial to optimizing training efficiency.
  We propose MicroEP, a novel parallelization strategy to achieve fine-grained load balancing in MoE systems. MicroEP is capable of achieving optimal load balancing in every micro-batch through efficient token scheduling across GPUs. Furthermore, we propose MicroMoE, an efficient distributed MoE training system with MicroEP's load balancing capabilities. Our experimental results demonstrate that MicroMoE improves the end-to-end training throughput by up to 47.6% compared with the state-of-the-art system, and almost consistently achieves optimal load balance among GPUs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [NX-CGRA: A Programmable Hardware Accelerator for Core Transformer Algorithms on Edge Devices](https://arxiv.org/abs/2511.17235)
*Rohit Prasad*

Main category: cs.AR

TL;DR: 关联：这是一篇关于**硬件/体系结构**（具体是基于CGRA的硬件加速器）和**图处理/ML/Transformer**的论文，目标是加速Transformer推理。
TLDR: 针对边缘设备上日益复杂和多样化的Transformer工作负载带来的性能和能效挑战，本文提出了NX-CGRA，一个采用粗粒度可重构阵列（CGRA）架构的可编程硬件加速器。NX-CGRA通过软件驱动的可编程性，能够高效支持多种线性和非线性Transformer推理算法。基准测试结果显示，NX-CGRA具有较高的整体效率和良好的能耗-面积权衡，证明了它在受限的边缘环境中的可扩展性和适应性。


<details>
  <summary>Details</summary>
Motivation: 随着边缘设备上Transformer工作负载的多样性和复杂性日益增加，在平衡性能、能效和架构灵活性方面带来了巨大的挑战。需要一个能够有效支持多种Transformer推理算法（包括线性和非线性函数）的高效、可扩展且节能的硬件加速器。

Method: 提出并设计了NX-CGRA，这是一个采用粗粒度可重构阵列（CGRA）架构的硬件加速器。该架构通过软件驱动的可编程性，使其能够高效执行各种内核模式，支持线性和非线性函数，适用于多种Transformer推理算法。并通过代表性的基准测试进行评估。

Result: NX-CGRA在代表性的真实世界Transformer模型基准测试中展现出高整体效率和有利的能耗-面积权衡，尤其在不同类别的操作中表现良好。结果表明，NX-CGRA在受限的功率和硅预算下，是边缘Transformer部署的一个可扩展和适应性强的硬件解决方案。

Conclusion: NX-CGRA作为一种可编程的硬件加速器，在受限的功率和硅预算下，为边缘Transformer部署提供了一个可扩展和适应性强的解决方案，能够高效支持不同类型的Transformer推理算法及其混合操作。

Abstract: The increasing diversity and complexity of transformer workloads at the edge present significant challenges in balancing performance, energy efficiency, and architectural flexibility. This paper introduces NX-CGRA, a programmable hardware accelerator designed to support a range of transformer inference algorithms, including both linear and non-linear functions. Unlike fixed-function accelerators optimized for narrow use cases, NX-CGRA employs a coarse-grained reconfigurable array (CGRA) architecture with software-driven programmability, enabling efficient execution across varied kernel patterns. The architecture is evaluated using representative benchmarks derived from real-world transformer models, demonstrating high overall efficiency and favorable energy-area tradeoffs across different classes of operations. These results indicate the potential of NX-CGRA as a scalable and adaptable hardware solution for edge transformer deployment under constrained power and silicon budgets.

</details>


### [9] [DISCA: A Digital In-memory Stochastic Computing Architecture Using A Compressed Bent-Pyramid Format](https://arxiv.org/abs/2511.17265)
*Shady Agwa,Yikang Shen,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: 该论文与编译器相关部分为：该论文聚焦于 AI 硬件加速设计，尤其是内存计算架构，这与编译器优化的硬件目标紧密相关，尽管没有直接涉及编译器中间表示（如 MLIR）或 HLS。该论文与图处理无关。该论文涉及到 HLS 和 MLIR 相关部分为：无。该论文与编译器相关部分为：无。该论文与 HLS 相关部分为：无。
总结：在人工智能应用对计算能力需求日益增长的背景下，针对现有内存计算架构的能效瓶颈，本文提出了一种名为 DISCA 的新型数字内存随机计算架构。DISCA利用压缩的准随机 Bent-Pyramid 数据格式，旨在继承模拟计算的计算简易性，同时保持数字系统的可扩展性、生产力和可靠性。在 180nm CMOS 工艺下，DISCA 在 500 MHz 时达到了 3.59 TOPS/W/bit 的能效，显著提高了矩阵乘法工作负载的能效。


<details>
  <summary>Details</summary>
Motivation: 当前的 AI 革命驱使大规模 AI 模型发展迅猛，这些模型通常涉及数据密集型矩阵乘法任务。传统冯·诺依曼架构面临“内存墙”和摩尔定律终结的挑战，同时 AI 应用正快速向边缘迁移（如机器人和无人机），进一步增加了对硬件预算的限制。尽管内存计算被提出作为有希望的解决方案，但现有的模拟和数字内存计算架构都存在设计限制，导致其预期效益大打折扣。因此，需要一种新的高效架构来应对这些挑战。

Method: DISCA 是一种数字内存随机计算架构，它利用压缩版本的准随机 Bent-Pyramid 数据格式。这种方法的设计旨在继承模拟计算的计算简易性，同时保留数字系统的可扩展性、生产力和可靠性。

Result: DISCA 的版图后建模结果显示，在使用商业 180nm CMOS 技术并在 500 MHz 工作频率下，其能效为 3.59 TOPS/W/bit。这意味着 DISCA 相较于其同类架构，能将矩阵乘法工作负载的能效提高数个数量级。

Conclusion: DISCA，一种新的数字内存随机计算架构，结合了准随机 Bent-Pyramid 数据格式的压缩版本，显著提高了矩阵乘法工作负载的能效。 在商业 180nm CMOS 技术下，DISCA 在 500 MHz 时的能效为 3.59 TOPS/W/bit。这表明 DISCA 相较于同类架构在能效方面有数量级的提升。

Abstract: Nowadays, we are witnessing an Artificial Intelligence revolution that dominates the technology landscape in various application domains, such as healthcare, robotics, automotive, security, and defense. Massive-scale AI models, which mimic the human brain's functionality, typically feature millions and even billions of parameters through data-intensive matrix multiplication tasks. While conventional Von-Neumann architectures struggle with the memory wall and the end of Moore's Law, these AI applications are migrating rapidly towards the edge, such as in robotics and unmanned aerial vehicles for surveillance, thereby adding more constraints to the hardware budget of AI architectures at the edge. Although in-memory computing has been proposed as a promising solution for the memory wall, both analog and digital in-memory computing architectures suffer from substantial degradation of the proposed benefits due to various design limitations. We propose a new digital in-memory stochastic computing architecture, DISCA, utilizing a compressed version of the quasi-stochastic Bent-Pyramid data format. DISCA inherits the same computational simplicity of analog computing, while preserving the same scalability, productivity, and reliability of digital systems. Post-layout modeling results of DISCA show an energy efficiency of 3.59 TOPS/W per bit at 500 MHz using a commercial 180nm CMOS technology. Therefore, DISCA significantly improves the energy efficiency for matrix multiplication workloads by orders of magnitude if scaled and compared to its counterpart architectures.

</details>
