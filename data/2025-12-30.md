<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]
- [cs.PL](#cs.PL) [Total: 7]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.DC](#cs.DC) [Total: 18]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience](https://arxiv.org/abs/2512.22435)
*Zining Wang,Jian Gao,Weimin Fu,Xiaolong Guo,Xuan Zhang*

Main category: cs.AR

TL;DR: 否，本论文内容与DSL、图处理、MLIR、编译器或HLS不直接相关。本文关注于使用多智能体框架和LLM技术进行**模拟电路设计自动化**。
太长不看：本文提出了AnalogSAGE，一个开源的自进化多智能体框架，用于模拟电路设计自动化。它通过三阶段智能体探索和四层分层内存，并结合仿真反馈，实现了迭代优化。在运算放大器设计基准测试中，AnalogSAGE相较于现有框架将整体通过率提高了10倍，Pass@1提高了48倍，并将参数搜索空间减少了4倍，显著提升了设计的可靠性和自主性。


<details>
  <summary>Details</summary>
Motivation: 现有的模拟电路设计严重依赖人工经验和直觉，是一个知识密集型的过程。现有的基于LLM的方法受限于提示驱动的网表生成或预定义的拓扑模板，难以满足复杂的规格要求。AnalogSAGE的动机在于，通过提出一个自进化的多智能体框架和分层内存组织，显著提高模拟电路设计自动化的可靠性、通用性和自主性，并减少参数搜索空间。

Method: AnalogSAGE是一个开源的自进化多智能体框架，它通过协调三阶段的智能体探索过程和利用四个分层的内存层来进行迭代优化。该框架使用基于仿真的反馈（如通过ngspice对SKY130 PDK）来持续改进设计。具体方法包括：1. **三阶段智能体探索**：推动设计过程的迭代 refinement。2. **四层分层内存**：用于组织和管理设计过程中积累的知识和反馈。3. **基于仿真的反馈回路**：确保设计是基于实际物理性能进行验证和调整的。

Result: AnalogSAGE在包含十个不同难度的运算放大器设计问题的基准测试中，相较于现有框架，取得了显著的性能提升：1. **整体通过率**：提高了10倍。2. **Pass@1**（首次尝试的通过率）：提高了48倍。3. **参数搜索空间**：减少了4倍。这些结果基于开源的SKY130 PDK和ngspice仿真器获得，证明了分层内存和基于反馈的推理在实践中显著提高了模拟设计自动化的可靠性和自主性。

Conclusion: AnalogSAGE框架通过其自进化多智能体探索和分层内存组织，显著提高了模拟电路设计的自动化水平、可靠性和自主性。它提供了一种可扩展且通用的方法，用于解决复杂的电路设计挑战。

Abstract: Analog circuit design remains a knowledge- and experience-intensive process that relies heavily on human intuition for topology generation and device parameter tuning. Existing LLM-based approaches typically depend on prompt-driven netlist generation or predefined topology templates, limiting their ability to satisfy complex specification requirements. We propose AnalogSAGE, an open-source self-evolving multi-agent framework that coordinates three-stage agent explorations through four stratified memory layers, enabling iterative refinement with simulation-grounded feedback. To support reproducibility and generality, we release the source code. Our benchmark spans ten specification-driven operational amplifier design problems of varying difficulty, enabling quantitative and cross-task comparison under identical conditions. Evaluated under the open-source SKY130 PDK with ngspice, AnalogSAGE achieves a 10$\times$ overall pass rate, a 48$\times$ Pass@1, and a 4$\times$ reduction in parameter search space compared with existing frameworks, demonstrating that stratified memory and grounded reasoning substantially enhance the reliability and autonomy of analog design automation in practice.

</details>


### [2] [TYTAN: Taylor-series based Non-Linear Activation Engine for Deep Learning Accelerators](https://arxiv.org/abs/2512.23062)
*Soham Pramanik,Vimal William,Arnab Raha,Debayan Das,Amitava Mukherjee,Janet L. Paluh*

Main category: cs.AR

TL;DR: 本文与 DSL 无关，与图处理无关，与 MLIR 无关。它与编译器和 HLS 有间接关联，因为它涉及特定硬件加速器的设计和优化，这通常需要使用编译器后端或 HLS 工具链进行实现。
本文提出了 TYTAN，一个基于泰勒级数和通用非线性近似引擎（G-NAE）的硬件加速器，专为边缘设备上的 AI 推理加速非线性激活函数并最小化功耗。TYTAN 集成了可重构硬件和动态近似算法，旨在实现最小精度偏差。系统级仿真表明，与基线 NVDLA 相比，TYTAN 在 45nm 工艺节点上实现了约 2 倍的性能提升、约 56% 的功耗降低和约 35 倍的面积减小，展现了其在加速和节能边缘 AI 推理中的显著优势。


<details>
  <summary>Details</summary>
Motivation: 随着 AI 架构的快速发展和 AI 系统的普及，特别是在边缘设备上，对既能提高 AI 推理加速又节能的领域特定架构的需求日益增加。部署 AI 算法面临显著的资源限制，如计算成本和能耗，尤其是在涉及大量数学运算（如通用矩阵乘法 GEMM 和激活函数）的场景中。因此，需要优化这些高功耗的操作，以应对边缘部署 AI 的挑战。

Method: 本文提出了一种名为 TYTAN（基于泰勒级数的非线性激活引擎）的通用非线性近似引擎（G-NAE）。TYTAN 结合了可重构硬件设计和专用算法，该算法能够动态估计每个激活函数所需的近似值，以最小化与基线精度的偏差。该系统通过使用最先进的 AI 架构（包括 CNN 和 Transformer）进行性能评估和系统级仿真来验证，仿真基于 Silvaco 的 FreePDK45 工艺节点。

Result: 系统级仿真结果显示，基于 Silvaco 的 FreePDK45 工艺节点，TYTAN 能够以大于 950 MHz 的时钟频率运行。与基线开源 NVIDIA 深度学习加速器（NVDLA）实施相比，TYTAN 在边缘 AI 推理方面实现了约 2 倍的性能提升、约 56% 的功耗降低和约 35 倍的面积减小，同时保持了最小的精度偏差。这些结果证明了其在加速、节能的边缘 AI 推理中的有效性。

Conclusion: TYTAN 通过将可重构硬件设计与动态估计所需近似值的专用算法相结合，成功在边缘 AI 推理中实现了激活函数的加速和功耗最小化。与基线 NVDLA 实施相比，TYTAN 实现了约 2 倍的性能提升、约 56% 的功耗降低和约 35 倍的面积减小，证明了其在加速、节能的边缘 AI 推理方面的有效性和前景。

Abstract: The rapid advancement in AI architectures and the proliferation of AI-enabled systems have intensified the need for domain-specific architectures that enhance both the acceleration and energy efficiency of AI inference, particularly at the edge. This need arises from the significant resource constraints-such as computational cost and energy consumption-associated with deploying AI algorithms, which involve intensive mathematical operations across multiple layers. High-power-consuming operations, including General Matrix Multiplications (GEMMs) and activation functions, can be optimized to address these challenges. Optimization strategies for AI at the edge include algorithmic approaches like quantization and pruning, as well as hardware methodologies such as domain-specific accelerators. This paper proposes TYTAN: TaYlor-series based non-linear acTivAtion eNgine, which explores the development of a Generalized Non-linear Approximation Engine (G-NAE). TYTAN targets the acceleration of non-linear activation functions while minimizing power consumption. The TYTAN integrates a re-configurable hardware design with a specialized algorithm that dynamically estimates the necessary approximation for each activation function, aimed at achieving minimal deviation from baseline accuracy. The proposed system is validated through performance evaluations with state-of-the-art AI architectures, including Convolutional Neural Networks (CNNs) and Transformers. Results from system-level simulations using Silvaco's FreePDK45 process node demonstrate TYTAN's capability to operate at a clock frequency >950 MHz, showcasing its effectiveness in supporting accelerated, energy-efficient AI inference at the edge, which is ~2 times performance improvement, with ~56% power reduction and ~35 times lower area compared to the baseline open-source NVIDIA Deep Learning Accelerator (NVDLA) implementation.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [3] [Symbolic Specification and Reasoning for Quantum Data and Operations](https://arxiv.org/abs/2512.22383)
*Mingsheng Ying*

Main category: cs.PL

TL;DR: 该论文与编译器相关部分：理论基础、逻辑框架、形式化验证。该论文提出了一种名为符号算子逻辑$\mathbf{SOL}$的通用逻辑框架，用于量子数据和操作的符号化规范和推理。$\mathbf{SOL}$将经典一阶逻辑嵌入到形式算子语言中，允许在经典数据（如布尔代数）的理论下进行推理，从而能利用现有的经典计算自动化验证工具。该框架旨在为量子计算和信息的正式验证和自动化定理证明提供概念基础。


<details>
  <summary>Details</summary>
Motivation: 在量子信息和计算研究中，符号方法在人工规范和推理量子态和操作方面被广泛使用，对于量子算法和程序的自动化推理和验证工具的规模化和效率至关重要。然而，目前缺乏一个关于量子数据和操作的符号化规范和推理的正式理论，这极大地限制了自动化验证技术在量子计算中的实际应用。

Method: 本文提出了一种通用的逻辑框架——符号算子逻辑$\mathbf{SOL}$，以实现对量子数据和操作的符号化规范和推理。该框架的关键在于将经典一阶逻辑语言嵌入到用于规范量子数据和操作的形式算子语言中，包括它们的递归定义。这种嵌入允许在基础经典数据（如布尔代数或群论）的选定理论下进行属性推理，从而可以利用现有为经典计算开发的自动化验证工具。

Result: 本文提出了$\mathbf{SOL}$符号算子逻辑框架，它通过将经典一阶逻辑嵌入到形式算子语言中，实现了对量子数据和操作的符号化规范和推理，包括它们的递归定义。这种嵌入使得可以对量子数据的属性进行推理，并且能够利用现有针对经典计算开发的自动化验证工具，从而使得符号方法成为可能。该框架为证明助手中的量子计算和信息的正式验证和自动化定理证明提供了概念基础。

Conclusion: 本文提出了一种名为$\mathbf{SOL}$的符号算子逻辑形式化框架，旨在为量子计算和信息的理论验证和自动化定理证明提供概念基础。通过将经典一阶逻辑嵌入到形式算子语言中，$\mathbf{SOL}$使得对量子数据和操作的属性进行推理成为可能，并且可以利用现有的经典计算自动验证工具。作者设想该框架可应用于Lean、Coq等证明助手。

Abstract: In quantum information and computation research, symbolic methods have been widely used for human specification and reasoning about quantum states and operations. At the same time, they are essential for ensuring the scalability and efficiency of automated reasoning and verification tools for quantum algorithms and programs. However, a formal theory for symbolic specification and reasoning about quantum data and operations is still lacking, which significantly limits the practical applicability of automated verification techniques in quantum computing.
  In this paper, we present a general logical framework, called Symbolic Operator Logic $\mathbf{SOL}$, which enables symbolic specification and reasoning about quantum data and operations. Within this framework, a classical first-order logical language is embedded into a language of formal operators used to specify quantum data and operations, including their recursive definitions. This embedding allows reasoning about their properties modulo a chosen theory of the underlying classical data (e.g., Boolean algebra or group theory), thereby leveraging existing automated verification tools developed for classical computing. It should be emphasised that this embedding of classical first-order logic into $\mathbf{SOL}$ is precisely what makes the symbolic method possible.
  We envision that this framework can provide a conceptual foundation for the formal verification and automated theorem proving of quantum computation and information in proof assistants such as Lean, Coq, and related systems.

</details>


### [4] [Eliminate Branches by Melding IR Instructions](https://arxiv.org/abs/2512.22390)
*Yuze Li,Srinivasan Ramachandra Sharma,Charitha Saumya,Ali R. Butt,Kirshanthan Sundararajah*

Main category: cs.PL

TL;DR: 该论文与编译器相关，因为它专注于一种名为MERIT的编译器转换，旨在优化程序性能。此外，它还与LLVM（一个著名的编译器基础设施）相关。它与图处理和HLS无关。
太长不看版：分支预测错误是现代处理器的主要性能瓶颈，传统的分支消除技术如if-conversion存在限制。本文提出了一种新的编译器转换方法MERIT（融合IR指令），通过在IR级别对齐和融合相似操作来消除数据相关分支，并使用操作数级别的保护来保持语义正确性，从而避免了硬件预言。MERIT作为LLVM pass实现，并在基准测试中展现出高达32倍的峰值加速，几何平均加速比达到10.9%，效果显著。


<details>
  <summary>Details</summary>
Motivation: 现代处理器中，分支预测错误会导致性能灾难性下降。尽管存在硬件预测器和剖面指导技术，但具有不规则模式的数据相关分支仍然难以处理。传统的if-conversion通过软件预言消除分支，但在x86等架构上面临限制，特别是在包含内存指令的路径上或在完整推测大型分支体时会引入过多的指令开销。

Method: 本文提出了Melding IR Instructions (MERIT)（融合IR指令），这是一种编译器转换，通过在IR指令级别对齐和融合来自不同路径的相似操作来消除分支。它借鉴了序列比对的思想来发现融合机会，并采用安全的操作数级别保护来确保语义正确性，从而避免了对硬件预言的支持。该方法作为LLVM的一个pass实现。

Result: MERIT在LLVM中实现为一个pass，并在来自四个基准测试套件的102个程序上进行评估。与硬件分支预测器相比，MERIT实现了10.9%的几何平均加速比，峰值改进达到32倍。同时，它还减少了静态指令开销，证明了其有效性。

Conclusion: MERIT是一种编译器转换，通过在IR指令级别对齐和融合来自不同路径的相似操作来消除分支。它改进了传统if-conversion的限制，通过安全的操作数级别保护来确保语义正确性，从而避免了硬件预言。实验结果表明，MERIT比硬件分支预测器显著提高了性能，证实了其有效性。

Abstract: Branch mispredictions cause catastrophic performance penalties in modern processors, leading to performance loss. While hardware predictors and profile-guided techniques exist, data-dependent branches with irregular patterns remain challenging. Traditional if-conversion eliminates branches via software predication but faces limitations on architectures like x86. It often fails on paths containing memory instructions or incurs excessive instruction overhead by fully speculating large branch bodies.
  This paper presents Melding IR Instructions (MERIT), a compiler transformation that eliminates branches by aligning and melding similar operations from divergent paths at the IR instruction level. By observing that divergent paths often perform structurally similar operations with different operands, MERIT adapts sequence alignment to discover merging opportunities and employs safe operand-level guarding to ensure semantic correctness without hardware predication. Implemented as an LLVM pass and evaluated on 102 programs from four benchmark suites, MERIT achieves a geometric mean speedup of 10.9% with peak improvements of 32x compared to hardware branch predictor, demonstrating the effectiveness with reduced static instruction overhead.

</details>


### [5] [A Bounded Game Semantics Checker for Precise Smart Contract Analysis](https://arxiv.org/abs/2512.22417)
*Vasileios Koutavas,Yu-Yang Lin,Nikos Tzevelekos*

Main category: cs.PL

TL;DR: The paper is related to compiler and DSL. The compiler part is related to the intermediate language Yul and the tool YulToolkit which works with the intermediate language. The DSL part is related to smart contract analysis (Solidity and Yul). / This paper presents YulToolkit, a precise and bounded-complete smart contract vulnerability finder based on game semantics, which models computation as an interaction between the contract and its environment. By exploring only feasible interactions within bounds, YulToolkit avoids over-approximation and detects vulnerabilities like reentrancy on real-world incidents (The DAO, PredyPool, and Lendf.Me) effectively, demonstrating its value as a precise addition to the smart contract analysis toolbox.


<details>
  <summary>Details</summary>
Motivation: 智能合约的漏洞检测需要一种既精确（避免误报）又具有可扩展性的方法，能够应对真实世界的合约复杂性。特别是对于像重入性这样的漏洞，在实际代码中精确检测非常困难。现有的方法可能面临过度近似或缺乏精确性的问题，因此需要一种基于严格理论基础（如博弈语义）、能够实现有限完备性，同时通过精确探索和领域知识的辅助，有效检测漏洞的新方法。

Method: 本文提出了一种基于博弈语义（Game Semantics）的新方法，将计算建模为合约与其环境之间的互动，并将对未知或恶意外部合约的推理简化为踪迹枚举。该方法在名为 YulToolkit 的工具中实现，这是一个针对 Solidity 中间语言 Yul 的有限博弈语义检查器。YulToolkit 通过仅探索可行的交互，避免了过度近似，并依靠博弈语义的理论实现了有限完备性。为了使探索易于处理，YulToolkit 支持使用 Solidity 编写的且能传播到 Yul 的工具化（Instrumentation），其工作量类似于创建测试工具。然而，与测试不同的是，该技术探索了所选参数和界限内的所有可接受踪迹。

Result: YulToolkit 在三个真实世界的安全事件（The DAO、PredyPool 和 Lendf.Me）以及基准合约上进行了评估。在所有情况下，YulToolkit 都成功检测到了已知的漏洞（并生成了触发违规的踪迹），并且在应用修复后，在限定的范围内没有报告进一步的违规。这些结果表明，有限博弈语义探索是智能合约分析工具箱中一种有效且精确的补充，特别是对于在实际代码中难以精确检测的重入性等漏洞。

Conclusion: 本文提出的基于博弈语义的有限探索方法，提供了一种有效且精确的智能合约分析工具，尤其适用于精确检测诸如重入性等难以发现的漏洞。其在实际案例中的成功应用，表明了该方法作为智能合约分析工具箱的重要补充价值，具有高性能和高精确性。

Abstract: We present a new approach to finding smart contract vulnerabilities that is precise (no false positives up to our EVM-Yul interpreter), bounded-complete, and, when instrumented with domain knowledge, scales to real-world contracts. Our method is based on game semantics, modelling computation as an interaction between a contract and its environment, reducing reasoning about unknown or malicious external contracts to trace enumeration. We implement this in a tool we refer to as YulToolkit, a bounded game-semantics checker for Yul, the intermediate language of Solidity. By exploring only feasible interactions, YulToolkit avoids over-approximation, and by relying on the theory of game semantics it achieves bounded completeness. To make exploration tractable, YulToolkit supports instrumentation written in Solidity and propagated to Yul, comparable in effort to creating a test harness. Unlike tests, however, our technique explores all admissible traces within the chosen parameters and bounds. We evaluate YulToolkit on three real-world incidents: The DAO, PredyPool, and Lendf.Me, as well as benchmark contracts. In all cases, YulToolkit detects the known vulnerabilities (producing a violation-triggering trace), and after applying fixes, reports no further violations within bounds. These results show that bounded game semantics exploration is an effective and precise addition to the smart contract analysis toolbox, particularly for vulnerabilities such as reentrancy that are hard to detect precisely in real code.

</details>


### [6] [Compiling Gradual Types with Evidence](https://arxiv.org/abs/2512.22684)
*José Luis Romero,Cristóbal Isla,Matías Toro,Éric Tanter*

Main category: cs.PL

TL;DR: 与DSL、图处理、MLIR、编译器、HLS相关：编译器。这篇论文介绍了一个名为GrEv的编译器，它基于抽象渐进类型（AGT）的证据语义实现了渐进类型，并证明了这种基于证据的方法在性能上可以与现有的基于强制转换的实现（如Grift）相媲美甚至更快，且在不同配置下表现出更强的稳定性，从而为实现AGT形式化推导的复杂渐进类型规程提供了高效实现的途径。


<details>
  <summary>Details</summary>
Motivation: 现有的在具有结构化类型的语言中支持可靠渐进类型的方法，主要依赖于基于强制转换的实现（如Grift）。然而，抽象渐进类型（AGT）方法虽然在语义上具有表达性和通用性，其基于证据的运行时语义是否能实现高效的渐进类型实现尚不明确。因此，本文的动机是探索并验证基于证据的语义是否能成为实现高效渐进类型编译器的可行途径。

Method: 作者设计、实现并评估了一个名为GrEv的基于证据的渐进类型编译器。他们解释了如何弥合形式语义与编译器实现之间的差距，并提出了新的单调语义。随后，他们使用Grift基准测试套件对GrEv的性能进行了实证评估。

Result: 实证评估结果表明，基于证据的编译器GrEv在性能上可以与基于强制转换的编译器Grift相媲美，甚至更快。此外，GrEv在静态到动态谱系的配置上表现出更强的稳定性。

Conclusion: 本文介绍了GrEv编译器，它基于证据的语义实现了渐进类型，并证明了这种方法在性能上可以与传统的基于强制转换的编译器（如Grift）相媲美甚至更快，特别是在静态到动态谱系的配置上表现出更强的稳定性。这项工作不仅丰富了渐进类型编译器的领域，也为探索基于AGT形式化推导出的复杂渐进类型规程的高效实现开辟了道路。

Abstract: Efficiently supporting sound gradual typing in a language with structural types is challenging. To date, the Grift compiler is the only close-to-the-metal implementation of gradual typing in this setting, exploiting coercions for runtime checks, and further extended with monotonic references for efficient access to statically-typed data structures. On the language design and semantics side, the Abstracting Gradual Typing (AGT) methodology has proven fruitful to elucidate existing designs and to innovate by deriving gradualizations of a wide variety of typing disciplines and language features. Grounded in abstract interpretation, the Curry-Howard inspired runtime semantics of AGT is based on the notion of evidence for consistent judgments that evolve during reduction, monitoring the plausibility of well-typedness. While expressive and versatile, it is unclear whether such evidence-based semantics are a viable route to realize an efficient implementation of gradual typing.
  In this work, we explore this question by designing, implementing, and evaluating an evidence-based compiler, called GrEv. We explain how to bridge the gap between the formal semantics and the GrEv compiler implementation, and identify novel monotonic semantics. We empirically evaluate the performance of GrEv on the Grift benchmark suite. The results show that an evidence-based compiler can be competitive with, and even faster than, a coercion-based compiler, exhibiting more stability across configurations on the static-to-dynamic spectrum. In addition to enriching the space of gradual typing compilers, this work opens a direct door to exploring efficient implementations of the many advanced gradual typing disciplines formally derived with AGT in the literature.

</details>


### [7] [Adaptable TeaStore: A Choreographic Approach](https://arxiv.org/abs/2512.23497)
*Giuseppe De Palma,Saverio Giallorenzo,Ivan Lanese,Gianluigi Zavattaro*

Main category: cs.PL

TL;DR: 不是（本文不直接涉及DSL、图处理、MLIR、编译器或HLS）。
本文分析了Adaptable TeaStore微服务架构参考模型，并将其基于AIOCJ（一种编舞语言）实现。AIOCJ的编舞特性确保了系统在动态适应不同运行时配置（包括配置转换）时的通信正确性（如无死锁）。该工作评估了AIOCJ在可适应微服务中的能力和当前不足，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有的微服务架构通常需要具备可适应性（Adaptable），以应对不同的配置和运行时条件。Adaptable TeaStore 是为此提出的一个参考模型。作者的动机是利用编舞语言AIOCJ的特性，提供一个具有运行时适应能力且能保证通信正确性（如无死锁）的微服务架构实现，并评估其有效性和局限性。

Method: 本文将可适应的TeaStore微服务参考模型实现基于AIOCJ（一种编舞语言）。AIOCJ允许对多方系统进行编程，使其能够在运行时适应不同的条件。该方法遵循编舞（choreographic）传统，通过在设计阶段分析系统通信，确保了在适应（adaptation）前、中、后通信的正确性（如无死锁）。适应是动态的，适应场景只需要在运行时完全指定。

Result: 通过使用AIOCJ对Adaptable TeaStore进行建模和实现，作者展示了该方法的优势：即在支持运行时动态适应性的同时，能够确保通信的正确性。但同时也暴露了该方法的当前局限性，并根据这些局限性提出了未来改进 AIOCJ 编程范式和语言的建议，以使其更好地与真实世界的云架构保持一致。

Conclusion: 本文将可适应的TeaStore微服务参考模型实现基于AIOCJ（一种编舞语言）。该方法通过编舞传统，确保了在运行时适应不同配置的系统的通信正确性（如无死锁）。这展示了AIOCJ在处理复杂且需要运行时适应的微服务架构中的潜力，同时也指出了该方法在更好契合真实世界云架构方面的局限性，并提出了未来的改进方向。

Abstract: The Adaptable TeaStore has recently been proposed as a reference model for adaptable microservice architectures. It includes different configurations, as well as scenarios requiring to transition between them. We describe an implementation of the Adaptable TeaStore based on AIOCJ, a choreographic language that allows one to program multiparty systems that can adapt at runtime to different conditions. Following the choreographic tradition, AIOCJ ensures by-construction correctness of communications (e.g., no deadlocks) before, during, and after adaptation. Adaptation is dynamic, and the adaptation scenarios need to be fully specified only at runtime. Using AIOCJ to model the Adaptable TeaStore, we showcase the strengths of the approach and its current limitations, providing suggestions for future directions for refining the paradigm (and the AIOCJ language, in particular), to better align it with real-world Cloud architectures.

</details>


### [8] [Beyond Per-Thread Lock Sets: Multi-Thread Critical Sections and Dynamic Deadlock Prediction](https://arxiv.org/abs/2512.23552)
*Martin Sulzmann*

Main category: cs.PL

TL;DR: 该论文与编译器相关部分（死锁动态分析、锁集构造），与HLS相关部分（无）。
太长不读：现有死锁分析工具的锁集结构因忽略跨线程事件而导致误报和漏报。本文提出了一种基于跟踪和偏序关系的新临界区特征化方法，实现了跨线程的锁集构造，从而改进了锁集精度。实验证明，该方法能消除DIRK的误报并减少SPDOffline的漏报，在不影响性能的前提下提高了死锁检测的准确性和完整性。


<details>
  <summary>Details</summary>
Motivation: 现有的死锁动态分析工具（如使用锁集）依赖于标准的每线程锁集结构，该结构只考虑同一线程中获取的锁，忽略了其他线程中获取的锁。这种限制导致了死锁预测中的误报（false positives）和漏报（false negatives）。
根本原因在于，标准的临界区概念忽略了来自其他线程的事件。因此，本文的动机是提出一种更准确的、能够涵盖跨线程事件的临界区特征化方法，以改进锁集结构，从而提高死锁检测的精度。

Method: 本文通过对临界区进行基于跟踪的特征化，打破了传统锁集结构中临界区仅限于单线程的限制。新的临界区可跨越多个线程，更加符合实际情况。
随后，作者展示了如何通过偏序关系来有声地近似这种基于跟踪的特征化，从而得到更精确且仍能高效计算的改进锁集结构。
最后，作者将这些改进的锁集结构集成到SPDOffline的扩展中，并证明了它们在保证健全性（无误报）的同时，提高了完整性（减少漏报）。

Result: 本文提出了一种改进的锁集结构：
1. **减少误报（False Positives）：** 新的锁集结构能够消除DIRK死锁预测器报告的误报。
2. **减少漏报（False Negatives）并提高完整性（Completeness）：** 通过扩展SPDOffline死锁预测器，新的锁集结构减少了漏报，使得死锁预测更加完整。
3. **保持健全性（Soundness）：** 相对于SPDOffline，扩展的工具仍保持了健全性（无误报）。
4. **性能无影响：** 在广泛的标准基准测试套件上，该方法的性能没有受到影响。

Conclusion: 本文提出了一种新的基于动态分析的临界区特征化方法，以改进死锁检测中的锁集结构。通过考量跨线程事件，新的锁集结构能够减少现有死锁预测工具（如DIRK和SPDOffline）中的误报和漏报。实验证明，该方法在保持效率的同时，提升了死锁预测的完整性，并未影响性能。

Abstract: Lock sets are commonly used for dynamic analysis of deadlocks. The standard per-thread lock set construction only considers locks acquired in the same thread, but is unaware of locks acquired in another thread. This leads to false positives and false negatives. The underlying issue is that the commonly used notion of a critical section on which the lock set construction relies ignores events from other threads. We give a trace-based characterization of critical sections that drops this restriction. Critical sections are no longer restricted to a single thread and can cover multiple threads. Such forms of critical sections exist, are natural, and correct the standard formulation.
  We show how to soundly approximate the trace-based characterization via partial order relations. Thus, we obtain an improved lock set construction that can still be efficiently computed and allows us to remove false positives reported by the DIRK deadlock predictor and remove false negatives by extending the SPDOffline deadlock predictor. We integrate various lock set constructions with increased precision in an extension of SPDOffline. Our extensions remain sound (no false positives) but are more complete (fewer false negatives) w.r.t. SPDOffline. For an extensive standard benchmark suite we can also show that the performance is not affected.

</details>


### [9] [Automating the Analysis of Parsing Algorithms (and other Dynamic Programs)](https://arxiv.org/abs/2512.23665)
*Tim Vieira,Ryan Cotterell,Jason Eisner*

Main category: cs.PL

TL;DR: 该论文与编译器相关，虽然没有直接涉及DSL或图处理或MLIR或HLS，但其对算法的性能分析和代码优化（如死代码识别、复杂度推断）是编译器优化的核心课题之一。/本文开发了一个系统，旨在帮助程序员对自然语言处理（NLP）算法进行分析，包括推断类型、识别死代码和冗余代码，以及确定参数化的运行时和空间复杂度上界，并在实际的NLP算法中得到了成功的应用。


<details>
  <summary>Details</summary>
Motivation: NLP算法的研究通常涉及对复杂的正式结构进行高效操作，算法设计者需要对其提出的算法（如运行时间或空间复杂度）或衍生的数据结构进行性能保证和属性确定。本文的动机是开发一个系统来协助程序员进行这类分析。

Method: 通过开发一个系统，专注于对NLP算法进行分析，具体方法包括对算法进行分析以推断类型、识别死代码和冗余代码，以及得出参数化的运行时和空间复杂度界限。

Result: 该系统成功地应用于多个NLP算法，并成功地推断出类型、死代码、冗余代码，以及参数化的运行时和空间复杂度上界。

Conclusion: 本文介绍了开发的一个系统，该系统旨在帮助程序员对自然语言处理（NLP）算法进行类型推断、识别死代码和冗余代码，并推断参数化的运行时和空间复杂度上界。该系统的成功应用表明它可以有效地帮助算法设计者进行性能分析和代码优化。

Abstract: Much algorithmic research in NLP aims to efficiently manipulate rich formal structures. An algorithm designer typically seeks to provide guarantees about their proposed algorithm -- for example, that its running time or space complexity is upper-bounded as a certain function of its input size. They may also wish to determine the necessary properties of the quantities derived by the algorithm to synthesize efficient data structures and verify type errors. In this paper, we develop a system for helping programmers to perform these types of analyses. We apply our system to a number of NLP algorithms and find that it successfully infers types, dead and redundant code, and parametric runtime and space complexity bounds.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [10] [Half-Approximating Maximum Dicut in the Streaming Setting](https://arxiv.org/abs/2512.22729)
*Amir Azarmehr,Soheil Behnezhad,Shane Ferrante,Mohammad Saneian*

Main category: cs.DS

TL;DR: This paper is related to graph processing and algorithms. The paper studies streaming algorithms for the maximum directed cut problem. It presents a $(1/2-\varepsilon)$-approximation algorithm with $n^{1-\Omega_\varepsilon(1)}$ space for general graphs, which is a tight result against a known $0.5$ lower bound, settling the approximation complexity for this fundamental problem.


<details>
  <summary>Details</summary>
Motivation: 最大有向割问题（maximum directed cut problem）在流式算法和子线性空间约束下的近似性能是一个重要的研究方向。现有研究在 $O(n)$ 空间下可以达到 $(1-\varepsilon)$-近似，但对于真正次线性空间（$n^{1-Ω(1)}$）的最佳近似率仍不明确。Kapralov 和 Krachun 的下界表明 $0.5$ 近似是最好的期望值，但现有的最好结果是 $0.485$（一般图）和 $(1/2-\varepsilon)$（常数度图）。本文的动机是希望在一般图和真正次线性空间下，通过流式算法达到 $(1/2-\varepsilon)$ 近似，从而解决该问题的近似复杂性。

Method: 本文提出了一种流式算法，用于在一般图上获得最大有向割值的 $(1/2-\varepsilon)$-近似。该算法使用了 $n^{1-Ω_\varepsilon(1)}$ 空间，并且其关键在于对模拟的局部算法中高、低度顶点之间相关性传播的细致分析。

Result: 本文证明了对于任意 $\varepsilon > 0$，在一般图上，可以通过使用 $n^{1-Ω_\varepsilon(1)}$ 空间的流式算法，获得最大有向割值的 $(1/2-\varepsilon)$-近似。这一结果表明 Kapralov 和 Krachun 的下界是一般紧的，从而解决了这一基础问题的近似复杂性。

Conclusion: 本文研究了最大有向割问题在流式算法中的近似性能，通过提出一种具有 $n^{1-Ω_\varepsilon(1)}$ 空间复杂度的 $(1/2-\varepsilon)$-近似算法，解决了该问题的近似复杂性，证明了 Kapralov 和 Krachun 的下界是紧的。该方法的关键在于分析局部算法中高、低度顶点之间相关性传播。

Abstract: We study streaming algorithms for the maximum directed cut problem. The edges of an $n$-vertex directed graph arrive one by one in an arbitrary order, and the goal is to estimate the value of the maximum directed cut using a single pass and small space. With $O(n)$ space, a $(1-\varepsilon)$-approximation can be trivially obtained for any fixed $\varepsilon > 0$ using additive cut sparsifiers. The question that has attracted significant attention in the literature is the best approximation achievable by algorithms that use truly sublinear (i.e., $n^{1-Ω(1)}$) space.
  A lower bound of Kapralov and Krachun (STOC'20) implies .5-approximation is the best one can hope for. The current best algorithm for general graphs obtains a .485-approximation due to the work of Saxena, Singer, Sudan, and Velusamy (FOCS'23). The same authors later obtained a $(1/2-\varepsilon)$-approximation, assuming that the graph is constant-degree (SODA'25).
  In this paper, we show that for any $\varepsilon > 0$, a $(1/2-\varepsilon)$-approximation of maximum dicut value can be obtained with $n^{1-Ω_\varepsilon(1)}$ space in *general graphs*. This shows that the lower bound of Kapralov and Krachun is generally tight, settling the approximation complexity of this fundamental problem. The key to our result is a careful analysis of how correlation propagates among high- and low-degree vertices, when simulating a suitable local algorithm.

</details>


### [11] [Computing parameters that generalize interval graphs using restricted modular partitions](https://arxiv.org/abs/2512.22975)
*Flavia Bonomo-Braberman,Eric Brandwein,Ignasi Sau*

Main category: cs.DS

TL;DR: 与DSL、图处理、MLIR、编译器或HLS相关性：本文与**图处理**相关。它分析了图论中两个参数（薄度 Thinness 和同步区间数 Simultaneous Interval Number）在参数化复杂度理论下的计算复杂度，特别是利用了图的模块化分区概念。

太长不看版：本文利用新的图参数——$\mathcal{G}$-模块化基数（特别是区间模块化和聚类模块化基数），为计算图的**薄度**和**同步区间数**这两个广义区间图参数提供了**线性核**和**FPT 算法**，填补了该领域参数化算法的空白。同时证明了这些问题在许多传统的图宽度参数化下不具有多项式核。


<details>
  <summary>Details</summary>
Motivation: 近年有研究者定义了图的 $\mathcal{G}$-模块化基数，它量化了图可以被划分为属于特定图类 $\mathcal{G}$ 的模块（modules）的最小数量。本文的动机是：
1. **探索新的参数化维度：** 将 $\mathcal{G}$-模块化基数作为参数，以此来分析计算广义区间图性质（如薄度 Thinness 和同步区间数 Simultaneous Interval Number）的复杂性。
2. **解决现有空白：** 在本文工作之前，据作者所知，对于计算薄度或同步区间数的参数化算法（无论是 FPT 还是 XP）都是未知的。
3. **连接已知的参数：** $\mathcal{G}$-模块化基数可以推广邻域多样性和孪生覆盖数，因此利用新的参数可以自然地为这些已知的、更受关注的参数提供算法结果。

Method: 本文采用参数化复杂度理论的方法来分析问题复杂度。具体做法是：
1. **定义和利用新的参数：** 使用 Lafond 和 Luo 定义的 $\mathcal{G}$-模块化基数，其中 $\mathcal{G}$ 分别取区间图类（Interval-Modular Cardinality）或完全图的并集（Cluster-Modular Cardinality）作为参数。
2. **算法设计：** 针对 Thinness 问题，证明了一个以区间模块化基数为参数的线性核（Linear Kernel）。针对 Simultaneous Interval Number 问题，设计了一个以聚类模块化基数加上解集大小为参数的 FPT 算法。
3. **推导和推广：** 利用模块化基数与邻域多样性（neighborhood diversity）、孪生覆盖数（twin-cover number）和顶点覆盖数（vertex cover number）之间的关系，将主要结果推广到这些更广为人知的参数上。
4. **负面结果分析：** 证明了 Thinness 和 Simultaneous Interval Number 在许多经典图宽度参数（如 Treewidth, Pathwidth, Clique-Width 等）参数化下，在标准复杂度假设下（NP $\not\subseteq$ coNP/poly）不具有多项式核。

Result: 本文取得了以下主要结果：
1. **Thinness 线性核：** 针对以区间模块化基数参数化的 Thinness 问题，提出了一个线性核（Linear Kernel）。
2. **Thinness 推广结果：** 这一线性核结果直接导出了在以邻域多样性为参数的 Thinness 问题的线性核；同时推导出了以孪生覆盖数和顶点覆盖数为参数的 Thinness 问题的 FPT 算法。
3. **Simultaneous Interval Number FPT 算法：** 针对以聚类模块化基数加上解集大小为参数的 Simultaneous Interval Number 问题，提出了一个 FPT 算法。
4. **Simultaneous Interval Number 推广结果：** 导出了以邻域多样性加解集大小、孪生覆盖数和顶点覆盖数为参数的 Simultaneous Interval Number 问题的 FPT 算法。
5. **负面结果（非多项式核）：** 证明了 Thinness 和 Simultaneous Interval Number 在许多经典宽度参数（如 Treewidth, Pathwidth, Bandwidth, Clique-Width, Modular-Width 等）参数化下，在标准假设下不具有多项式核。

Conclusion: 本文分析了当图 $G$ 以 $\mathcal{G}$-模块化基数参数化时，计算图的“薄度”（Thinness）和“同步区间数”（Simultaneous Interval Number）这两个广义区间图参数的复杂性。主要结论是：在参数化复杂度理论的框架下，通过引入 $\mathcal{G}$-模块化基数作为参数，本文为 Thinness 和 Simultaneous Interval Number 问题提供了有效的、新的参数化算法和核心化结果，特别是利用了区间模块化基数和聚类模块化基数。此外，本文还指出了这些问题在许多经典宽度参数（如树宽、路径宽等）参数化下不具有多项式核。这些结果填补了该领域在参数化算法方面的空白。

Abstract: Recently, Lafond and Luo [MFCS 2023] defined the $\mathcal{G}$-modular cardinality of a graph $G$ as the minimum size of a partition of $V(G)$ into modules that belong to a graph class $\mathcal{G}$. We analyze the complexity of calculating parameters that generalize interval graphs when parameterized by the $\mathcal{G}$-modular cardinality, where $\mathcal{G}$ corresponds either to the class of interval graphs or to the union of complete graphs. Namely, we analyze the complexity of computing the thinness and the simultaneous interval number of a graph.
  We present a linear kernel for the Thinness problem parameterized by the interval-modular cardinality and an FPT algorithm for Simultaneous Interval Number when parameterized by the cluster-modular cardinality plus the solution size. The interval-modular cardinality of a graph is not greater than the cluster-modular cardinality, which in turn generalizes the neighborhood diversity and the twin-cover number. Thus, our results imply a linear kernel for Thinness when parameterized by the neighborhood diversity of the input graph, FPT algorithms for Thinness when parameterized by the twin-cover number and vertex cover number, and FPT algorithms for Simultaneous Interval Number when parameterized by the neighborhood diversity plus the solution size, twin-cover number, and vertex cover number. To the best of our knowledge, prior to our work no parameterized algorithms (FPT or XP) for computing the thinness or the simultaneous interval number were known.
  On the negative side, we observe that Thinness and Simultaneous Interval Number parameterized by treewidth, pathwidth, bandwidth, (linear) mim-width, clique-width, modular-width, or even the thinness or simultaneous interval number themselves, admit no polynomial kernels assuming NP $\not\subseteq$ coNP/poly.

</details>


### [12] [Practical Parallel Block Tree Construction: First Results](https://arxiv.org/abs/2512.23314)
*Robert Clausecker,Florian Kurpicz,Etienne Palanga*

Main category: cs.DS

TL;DR: 该论文与编译器、DSL、MLIR、HLS或图处理无关。
块树是一种高效的压缩文本表示结构，然而其构建速度缓慢且占用大量内存。本文提出了一种快速且占用内存小的并行算法来高效构建块树，该算法在单核上速度可与现有最快算法媲美，使用 64 核时可提速四倍，同时内存占用减少一个数量级。此外，作者还提出了一种用于 Karp-Rabin 指纹计算的数据并行算法。


<details>
  <summary>Details</summary>
Motivation: 块树（block tree）是一种压缩文本表示方法，它支持访问、秩（rank）和选择（select）查询，所需的空间与压缩文本的渐进空间需求相似（$O(z\log\frac{n}{z})$ 词，其中 $z$ 是 Lempel-Ziv 因子数）。在实际应用中，块树的查询性能与最先进的压缩秩和选择索引相当，但其构建速度明显较慢，并且最快的构建算法需要大量的内存。本文的动机是解决块树构建速度慢和对工作内存要求高的问题，以提高其实用性。

Method: 本文提出了一种快速且占用内存小的并行算法来高效构建块树。文中提到，作者提出了一个数据并行算法用于 Karp-Rabin 指纹计算，这可能是加速块树并行构建的一种技术。核心方法是通过并行化来加速块树的构建过程，同时优化内存使用，从而在保证速度的同时显著减少所需的内存。

Result: 本文提出的并行构建算法在单核上实现了与目前最快的构建算法相似的速度。在使用 64 核时，速度最高可达到当前最快算法的四倍。更重要的是，该算法在实现速度提升的同时，所需的内存比现有算法少了一个数量级。此外，作者还提出了一种独立的数据并行算法用于 Karp-Rabin 指纹计算。

Conclusion: 本文提出了快速且占用内存小的并行算法来高效构建块树。实验结果显示，所提出的算法在单核上与目前最快的构建算法速度相当，而在使用 64 核时速度最高可提升四倍，同时内存需求大幅降低。作者还提出了一种数据并行算法用于 Karp-Rabin 指纹计算，作为一项独立的成果。本文成功解决了块树构建速度慢和内存占用大的问题，有望促进其在实际应用中的推广。

Abstract: The block tree [Belazzougui et al., J. Comput. Syst. Sci. '21] is a compressed representation of a length-$n$ text that supports access, rank, and select queries while requiring only $O(z\log\frac{n}{z})$ words of space, where $z$ is the number of Lempel-Ziv factors of the text. In other words, its space-requirements are asymptotically similar to those of the compressed text. In practice, block trees offer comparable query performance to state-of-the-art compressed rank and select indices. However, their construction is significantly slower. Additionally, the fastest construction algorithms require a significant amount of working memory. To address this issue, we propose fast and lightweight parallel algorithms for the efficient construction of block trees. Our algorithm achieves similar speed than the currently fastest construction algorithm on one core and is up to four times faster using 64 cores. It achieves all that while requiring an order of magnitude less memory. As result of independent interest, we present a data parallel algorithm for Karp-Rabin fingerprint computation.

</details>


### [13] [Pseudodeterministic Algorithms for Minimum Cut Problems](https://arxiv.org/abs/2512.23468)
*Aryan Agarwala,Nithin Varma*

Main category: cs.DS

TL;DR: 该论文与图处理相关。它提出了全局最小割和最小 $s$-$t$ 割的高效伪确定性算法，其运行时间优于目前最快的顺序确定性全局最小割算法，并成功在顺序、流式、PRAM 和割查询等多种计算模型中实现。


<details>
  <summary>Details</summary>
Motivation: 全局最小割和最小 $s$-$t$ 割是图论中的基本问题。尽管已经存在确定性算法，但作者的动机在于追求更高的效率，尤其是在速度上超越现有的最好确定性算法。同时，作者也希望在缺乏高效确定性全局最小割算法的各种计算模型（如流式、PRAM等）中提供有效的解决方案。

Method: 本文的核心方法是提出了全局最小割和最小 $s$-$t$ 割的高效伪确定性算法。虽然没有详细描述算法的具体步骤，但强调了其在各种计算模型（顺序、流式、PRAM、割查询模型）中的实现和有效性，特别是其运行时间优于现有的最快顺序确定性算法。

Result: 本文提出的全局最小割伪确定性算法在运行时间上渐进地优于目前最快的顺序确定性全局最小割算法（Henzinger, Li, Rao, Wang; SODA 2024）。该算法成功地在顺序、流式、PRAM 和割查询模型中实现了高效的实现。

Conclusion: 本文成功地为全局最小割和最小 $s$-$t$ 割问题提出了高效的伪确定性算法。这些算法在运行时上优于目前最快的顺序确定性全局最小割算法，并且在顺序、流式、PRAM 和割查询模型等缺乏高效确定性算法的场景下得到了有效的实现，展示了伪确定性方法在图算法领域中的潜力。

Abstract: In this paper, we present efficient pseudodeterministic algorithms for both the global minimum cut and minimum s-t cut problems. The running time of our algorithm for the global minimum cut problem is asymptotically better than the fastest sequential deterministic global minimum cut algorithm (Henzinger, Li, Rao, Wang; SODA 2024).
  Furthermore, we implement our algorithm in sequential, streaming, PRAM, and cut-query models, where no efficient deterministic global minimum cut algorithms are known.

</details>


### [14] [Circle graphs can be recognized in linear time](https://arxiv.org/abs/2512.23492)
*Christophe Paul,Ignaz Rutter*

Main category: cs.DS

TL;DR: This paper is related to graph processing. The paper presents the first linear-time recognition algorithm for circle graphs by replacing the union-find data structure with a PC-tree in the split decomposition algorithm, thereby achieving linear-time split decomposition calculation.


<details>
  <summary>Details</summary>
Motivation: 现有的最佳圆图（circle graph）识别算法运行时间接近线性，因为它依赖于使用union-find数据结构的分割分解（split decomposition）算法。作者的动机是希望消除对union-find数据结构的依赖，从而将圆图识别算法的时间复杂度正式优化到严格的线性时间$O(n+m)$。

Method: 本文的核心方法是使用PC树（Parent-Child tree, 可能是指Prime-Component tree或相关的合成分解树）数据结构来替代现有的圆图识别算法中的union-find数据结构进行分割分解（split decomposition）的计算。具体而言，通过这种替代，实现了线性时间$O(n+m)$内完成分割分解，从而使整个圆图识别算法达到线性时间。

Result: 通过用PC树数据结构替换分割分解算法中的union-find数据结构，作者成功地在圆图的识别中实现了首次线性时间$O(n+m)$识别算法。这表明PC树能够在线性时间内完成分割分解的计算。

Conclusion: 本文成功地将圆图识别算法的时间复杂度降低到线性时间$O(n+m)$，这是通过使用PC树数据结构替代现有算法中使用的union-find数据结构来实现的，从而实现了线性时间内的分割分解计算。这使得圆图识别算法首次达到了理论上的最优时间复杂度。

Abstract: To date, the best circle graph recognition algorithm runs in almost linear time as it relies on a split decomposition algorithm that uses the union-find data-structure. We show that in the case of circle graphs, the PC-tree data-structure allows one to avoid the union-find data-structure to compute the split decomposition in linear time. As a consequence, we obtain the first linear-time recognition algorithm for circle graphs.

</details>


### [15] [A note on the depth of optimal fanout-bounded prefix circuits](https://arxiv.org/abs/2512.23657)
*Igor S. Sergeev*

Main category: cs.DS

TL;DR: 与DSL、图处理、MLIR、编译器、HLS相关。因为它讨论了电路深度和扇出限制，这直接关系到计算机体系结构、硬件设计和HLS（高层次综合）中电路优化和性能分析。
太长不看：本文证明了在扇出限制为 $k$ 的情况下，最优前缀电路的最小深度是 ${\log_{α_k} N \pm O(1)}$，其中 $α_k$ 是由 $k$ 决定的特定方程的根，从而推广了 $k=2$ 和 $k=\infty$ 时的已知结论。


<details>
  <summary>Details</summary>
Motivation: 寻找最优前缀电路（零缺陷电路）在有限扇出限制（$k$）下的最小深度，以确定电路设计的理论性能界限，并推广先前仅在 $k=2$ 和 $k=\infty$ 情况下已知的结论。

Method: 通过数学分析和推导，找到并证明了在扇出受限于 $k$ 的条件下，最优前缀电路（零缺陷电路）最小深度的精确渐近界限，该界限由涉及 $k$ 的特定多项式的根来表示。

Result: 在扇出受限于 $k$ 的 $N$ 输入上，最优前缀电路（零缺陷电路）的最小深度为 ${\log_{α_k} N \pm O(1)}$，其中 $α_k$ 是多项式 ${2+x+ x^2+\ldots + x^{k-2}-x^k}$ 的唯一正根。

Conclusion: 本文证明了在扇出受限于 $k$ 的情况下，最优前缀电路的最小深度为 ${\log_{α_k} N \pm O(1)}$，其中 $α_k$ 是特定多项式的唯一正根。这一结论推广了先前仅在 $k=2$ 和 $k=\infty$ 情况下已知的界限。

Abstract: It is shown that the minimal depth of an optimal prefix circuit (i.e., a zero-deficiency circuit) on $N$ inputs with fanout bounded by $k$ is ${\log_{α_k} N \pm O(1)}$, where $α_k$ is the unique positive root of the polynomial ${2+x+ x^2+\ldots + x^{k-2}-x^k}$. This bound was previously known in the cases $k=2$ and $k=\infty$.

</details>


### [16] [The Minimum Subgraph Complementation Problem](https://arxiv.org/abs/2512.23687)
*Juan Gutiérrez,Sagartanu Pal*

Main category: cs.DS

TL;DR: 该论文与图处理（"Subgraph complementation" on graphs, "bipartite", "chordal graphs"）相关。
最小子图补全（MSC）问题（即找到最小的顶点集$S$，通过对$S$诱导的子图进行补全操作，使图$G$转换为目标图类$\mathcal{C}$）的决策版本研究广泛，但优化变体的算法复杂性研究不足。本文从算法角度研究MSC，并在多种挑战性设置中给出了多项式时间算法，包括在二分图、互补二分图和分裂图之间的转换、将二分正则图转换为弦图、在输入为森林时转换为固定退化度图、以及转换为不连通图和2-连通图，从而显著推进了MSC优化问题的可解性研究。


<details>
  <summary>Details</summary>
Motivation: 最小子图补全（MSC）问题的决策版本已被广泛研究，但其优化变体的算法复杂性却很大程度上未被探索。本文的动机是填补这一空白，从算法角度研究MSC问题，并为如何在各种特定图类别和性质之间进行最小成本的图转换，提供高效的（多项式时间）算法。

Method: 本文通过设计和提供了多项式时间算法来解决最小子图补全（MSC）问题在多种非平凡设置中的优化变体。具体方法包括对各类目标图性质（双向图、分裂图、弦图、固定退化度、连通性）的MSC问题进行了深入的算法复杂度分析和求解。

Result: 本文在多种非平凡设置下为最小子图补全（MSC）问题提供了多项式时间算法。具体结果包括：1. 证明了在二分图、互补二分图和分裂图之间进行转换的MSC问题可以多项式时间求解。2. 证明了将二分正则图补全为弦图的MSC问题也是多项式时间可解。3. 证明了当输入图是森林时，MSC到固定退化度图类的问题可以在多项式时间求解。4. 证明了对于任意输入图，MSC到不连通图类和MSC到2-连通图类的问题都可以多项式时间求解。

Conclusion: 本文侧重于MSC问题（最小子图补全）的算法角度研究，通过提出多项式时间算法，为在特定图类之间转换以及满足特定图性质（如连通性）的MSC问题提供了有效的解决方案，填补了该领域优化变体算法研究的空白。

Abstract: Subgraph complementation is an operation that toggles all adjacencies inside a selected vertex set. Given a graph \(G\) and a target class \(\mathcal{C}\), the Minimum Subgraph Complementation problem asks for a minimum-size vertex set \(S\) such that complementing the subgraph induced by \(S\) transforms \(G\) into a graph belonging to \(\mathcal{C}\). While the decision version of Subgraph Complementation has been extensively studied and is NP-complete for many graph classes, the algorithmic complexity of its optimization variant has remained largely unexplored.
  In this paper, we study MSC from an algorithmic perspective. We present polynomial-time algorithms for MSC in several nontrivial settings. Our results include polynomial-time solvability for transforming graphs between bipartite, co-bipartite, and split graphs, as well as for complementing bipartite regular graphs into chordal graphs. We also show that MSC to the class of graphs of fixed degeneracy can be solved in polynomial time when the input graph is a forest. Moreover, we investigate MSC with respect to connectivity and prove that MSC to the class of disconnected graphs and to the class of 2-connected graphs can be solved in polynomial time for arbitrary inputs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [17] [GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems](https://arxiv.org/abs/2512.22125)
*Jithin VG,Ditto PS*

Main category: cs.DC

TL;DR: 该论文与编译器、DSL、图处理、MLIR 或 HLS **不相关**。
**太长不看 (TLDR) 摘要:** 随着 AI/LLM 工作负载的需求增长，高效的 GPU 资源共享变得至关重要，但软件 GPU 虚拟化方案缺乏标准化评估。本文提出了 GPU-Virt-Bench，一个综合基准测试框架，通过 10 个类别 56 个指标（包括 LLM 性能、隔离、调度等）系统性评估 GPU 虚拟化系统（如 HAMi-core 和 BUD-FCSP），并与 MIG 基线进行比较，为多租户环境下的部署提供关键的性能洞察和决策依据。


<details>
  <summary>Details</summary>
Motivation: 随着 GPU 加速的工作负载（尤其是 AI 和 LLM 推理）的普及，对云和容器环境中高效 GPU 资源共享的需求空前高涨。虽然 NVIDIA 的 MIG 技术提供了硬件隔离，但仅限于高端数据中心 GPU。现有的软件虚拟化解决方案（如 HAMi-core 和 BUD-FCSP）缺乏标准化的评估方法，使得无法系统地比较和评估这些解决方案的性能和适用性。

Method: 作者提出了一个名为 GPU-Virt-Bench 的综合基准测试框架。该框架通过跨越 10 个类别、共计 56 个性能指标来评估 GPU 虚拟化系统，这些指标包括开销、隔离质量、LLM 特定性能、内存带宽、缓存行为、PCIe 吞吐量、多 GPU 通信、调度效率、内存碎片和错误恢复。作者使用该框架来评估 HAMi-core、BUD-FCSP 以及模拟的 MIG 基线。

Result: 作者提出了 GPU-Virt-Bench 框架，并通过它对 HAMi-core、BUD-FCSP 和模拟的 MIG 基线进行了评估。评估结果揭示了这些虚拟化解决方案在生产部署决策中至关重要的性能特征，证实了该框架对于在多租户环境中部署 GPU 资源的从业者具有实用价值。

Conclusion: GPU-Virt-Bench框架提供了一个系统、全面的方法来评估 GPU 虚拟化解决方案（如 HAMi-core 和 BUD-FCSP），并通过将其结果与理想的 MIG 基线进行比较，为在多租户环境中部署 GPU 资源的实际决策提供了关键的性能洞察。这项工作弥补了 GPU 虚拟化系统缺乏标准化评估方法的空白。

Abstract: The proliferation of GPU-accelerated workloads, particularly in artificial intelligence and large language model (LLM) inference, has created unprecedented demand for efficient GPU resource sharing in cloud and container environments. While NVIDIA's Multi-Instance GPU (MIG) technology provides hardware-level isolation, its availability is limited to high-end datacenter GPUs. Software-based virtualization solutions such as HAMi-core and BUD-FCSP offer alternatives for broader GPU families but lack standardized evaluation methodologies. We present GPU-Virt-Bench, a comprehensive benchmarking framework that evaluates GPU virtualization systems across 56 performance metrics organized into 10 categories. Our framework measures overhead, isolation quality, LLM-specific performance, memory bandwidth, cache behavior, PCIe throughput, multi-GPU communication, scheduling efficiency, memory fragmentation, and error recovery. GPU-Virt-Bench enables systematic comparison between software virtualization approaches and ideal MIG behavior, providing actionable insights for practitioners deploying GPU resources in multi-tenant environments. We demonstrate the framework's utility through evaluation of HAMi-core, BUD-FCSP, and simulated MIG baselines, revealing performance characteristics critical for production deployment decisions.

</details>


### [18] [HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration](https://arxiv.org/abs/2512.22137)
*Jiangwen Dong,Jiayu Li,Wanyu Lin*

Main category: cs.DC

TL;DR: 相关：不涉及DSL、图处理、MLIR、编译器、HLS。它与**大型语言模型（LLMs）的部署和边缘计算**相关。摘要过长，没空看：HybridFlow是一个资源自适应框架，通过将复杂查询分解为并行子任务并根据资源和效用智能地路由到边缘或云端LLM，从而显著降低了LLM的推理延迟和代币消耗，在保持准确性的同时提高了边缘-云协同推理的效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的推理延迟和代币消耗阻碍了其在资源受限的边缘设备上的实时部署。现有的边缘-云协作方法大多采用粗粒度的任务分配策略，未能利用细粒度的推理并行性，导致冗余计算和低效的资源利用。

Method: HybridFlow分两阶段进行：1. 任务分解和并行执行：将复杂查询动态拆分为相互依赖的子任务，一旦依赖关系解决即可执行。2. 资源感知子任务路由：学习到的路由器根据预测的效用增益和实时预算状态，自适应地将每个子任务分配给边缘或云模型。

Result: 在GPQA、MMLU-Pro、AIME和LiveBench-Reasoning上的综合评估表明，HybridFlow在保持竞争性准确率的同时，有效减少了端到端推理时间（End-to-End Inference Time）和总代币使用量（Overall Token Usage）。

Conclusion: HybridFlow是一种资源自适应推理框架，通过动态任务分解和资源感知的子任务路由，实现了边缘和云端大型语言模型之间的快速、代币高效的协同推理，有效减少了端到端推理时间和代币使用量的同时保持了竞争力。

Abstract: Large language models (LLMs) exhibit impressive reasoning and problem-solving abilities, yet their substantial inference latency and token consumption pose major challenges for real-time deployment on resource-limited edge devices. Recent efforts toward edge-cloud collaboration have attempted to mitigate this issue, but most existing methods adopt coarse-grained task allocation strategies-assigning entire queries either to the edge or the cloud. Such rigid partitioning fails to exploit fine-grained reasoning parallelism and often leads to redundant computation and inefficient resource utilization. To this end, we propose HybridFlow, a resource-adaptive inference framework that enables fast and token-efficient collaborative reasoning between edge and cloud LLMs. HybridFlow operates in two stages: (1) task decomposition and parallel execution, which dynamically splits a complex query into interdependent subtasks that can execute as soon as their dependencies are resolved; and (2) resource-aware subtask routing, where a learned router adaptively assigns each subtask to the edge or cloud model according to predicted utility gains and real-time budget states. Comprehensive evaluations on GPQA, MMLU-Pro, AIME, and LiveBench-Reasoning demonstrate that HybridFlow effectively reduces end-to-end inference time and overall token usage while maintaining competitive accuracy.

</details>


### [19] [On Harnessing Idle Compute at the Edge for Foundation Model Training](https://arxiv.org/abs/2512.22142)
*Leyang Xue,Meghana Madhyastha,Myungjin Lee,Amos Storkey,Randal Burns,Mahesh K. Marina*

Main category: cs.DC

TL;DR: 涉及领域：编译器/编译技术；图处理；MLIR；HLS；DSL
这是一篇关于去中心化基础模型训练的论文，它通过引入 Cleave 范式来解决现有边缘训练方法的局限性。Cleave 采用选择性混合张量并行方法和参数服务器架构，实现了与云端相当的训练性能，并显著提高了可扩展性和容错能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基础模型开发生态系统高度集中且限于大型云数据中心运营商，训练成本高昂。去中心化基础模型训练利用边缘设备的空闲计算资源可以提供一种民主化的替代方案。然而，现有的边缘训练方法存在性能与云端训练不匹配、模型规模可扩展性有限、超出设备内存容量、通信开销大以及难以令人满意地处理设备异构性和动态性等缺点。

Method: 文章提出了一种名为 Cleave 的新范式，它通过一种新颖的选择性混合张量并行方法来精细地划分训练操作。结合一个以参数服务器为中心的训练框架，Cleave 解决了设备内存限制并避免了通信瓶颈。此外，Cleave 使用成本优化模型来指导设备选择和训练工作负载分配，以有效应对设备的异构性和动态性（流失）。

Result: 评估结果显示，Cleave 在效率上与基于云的 GPU 训练相当，能够有效地扩展到更大的模型和数千个设备，支持的设备数量是基线边缘训练方法的 8 倍。Cleave 的每批训练时间比最先进的边缘训练方法快 10 倍，并且能有效处理设备故障，恢复速度比先前的方法快至少 100 倍。

Conclusion: Cleave 通过一种新颖的选择性混合张量并行方法和以参数服务器为中心的训练框架，实现了高效的去中心化基础模型训练，能够在边缘设备上训练大型模型，性能与云端相当。它有效解决了现有边缘训练的局限性，如性能低下、可扩展性差、内存限制、通信开销大以及设备异构性和动态性问题。

Abstract: The ecosystem behind foundation model development today is highly centralized and limited to large-scale cloud data center operators: training foundation models is costly, needing immense compute resources. Decentralized foundation model training across edge devices, leveraging their spare compute, promises a democratized alternative. However, existing edge-training approaches fall short: they struggle to match cloud-based training performance, exhibit limited scalability with model size, exceed device memory capacity, and have prohibitive communication overhead. They also fail to satisfactorily handle device heterogeneity and dynamism.
  We introduce a new paradigm, Cleave, which finely partitions training operations through a novel selective hybrid tensor parallelism method. Together with a parameter server centric training framework, Cleave copes with device memory limits and avoids communication bottlenecks, thereby enabling efficient training of large models on par with the cloud. Further, with a cost optimization model to guide device selection and training workload distribution, Cleave effectively accounts for device heterogeneity and churn.
  Our evaluations show that Cleave matches cloud-based GPU training by scaling efficiently to larger models and thousands of devices, supporting up to 8x more devices than baseline edge-training approaches. It outperforms state-of-the-art edge training methods by up to a factor of 10 in per-batch training time and efficiently handles device failures, achieving at least 100x faster recovery than prior methods.

</details>


### [20] [GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs](https://arxiv.org/abs/2512.22147)
*Ruifan Chu,Anbang Wang,Xiuxiu Bai,Shuai Liu,Xiaoshe Dong*

Main category: cs.DC

TL;DR: 该论文与**编译器**和**高性能计算/GPU优化**(属于广义的编译器优化或代码生成范畴)相关。因为它专注于GPU内核的性能优化，并使用LLM框架来生成和优化代码，这涉及代码转换、性能调优和跨平台移植。

**TLDR:** 本文提出了一个端到端LLM框架，用于在不构建完整应用的情况下优化GPU热点内核。它通过将内核转化为最小可执行程序（MEP），并在应用外部进行多轮迭代优化，集成了自动错误修复和性能模式继承，以提高效率并保持正确性。该方法在NVIDIA GPU和海光DCU上对PolyBench等基准测试实现了最高7.77倍的加速，提供了一种实用且低成本的跨平台GPU内核优化方案。


<details>
  <summary>Details</summary>
Motivation: 在高性能计算中，GPU热点内核是主要的性能瓶颈。传统的专家手动调优成本高昂且难以移植。现有的基于大型语言模型（LLM）的优化方法通常假设内核可以廉价地编译和执行，但这不适用于大型应用程序，因为完整的构建和运行成本极高。因此，本文的动机是开发一个端到端的、具有性能反馈的LLM框架，该框架能够在不构建完整应用程序的情况下优化内核，以实现实用、低成本的GPU内核优化。

Method: 本文提出了一个端到端的大型语言模型（LLM）优化框架，该框架的核心步骤包括：1. **热点内核提取与独立化**：从大型应用中提取独立的热点GPU内核。2. **最小可执行程序（MEP）生成**：自动将提取的内核代码补全为最小可执行程序，使其可以在不依赖完整应用的情况下独立编译和运行。3. **多轮迭代优化与评估**：在完整应用程序外部，对MEP进行多轮迭代优化和性能评估。4. **集成优化机制**：框架集成了“自动错误修复”（Automatic Error Repair）来处理故障，以及“性能模式继承”（Performance Pattern Inheritance）来重用有效的性能策略（如平铺、内存和同步策略），以保持正确性并减少搜索成本。5. **结果回集成与验证**：将优化后的变体重新集成回原始应用程序中进行最终验证。

Result: 该方法在多个平台和基准测试上取得了显著的性能提升：1. **NVIDIA GPU (PolyBench)**：平均加速比为5.05倍。2. **海光DCU (PolyBench)**：平均加速比为7.77倍。3. **AMD APP SDK**：加速比为1.77倍。4. **大型超算应用中的三个热点内核**：加速比为1.25倍。这些结果均超过了直接使用LLM进行优化的效果。该方法无需完整源代码依赖，并具备跨平台移植性。

Conclusion: 本文提出的端到端LLM框架，通过最小可执行程序（MEP）的构建和外部应用程序的多轮迭代优化，有效地克服了传统LLM优化方法在大型应用中编译和运行成本高昂的缺点。该框架集成了自动错误修复和性能模式继承，提高了优化效率、保持了代码正确性，并在多个基准测试集和跨平台（NVIDIA GPU和海光DCU）上实现了显著的性能提升，特别是对于PolyBench和AMD APP SDK，分别实现了5.05倍到7.77倍和1.77倍的加速，证明了其在实际高性能计算环境中对GPU内核优化的实用性和低成本优势。

Abstract: In high-performance computing, hotspot GPU kernels are primary bottlenecks, and expert manual tuning is costly and hard to port. Large language model methods often assume kernels can be compiled and executed cheaply, which fails in large applications where full builds and runs are expensive. We present an end-to-end LLM framework with performance feedback that optimizes kernels without building the full application. From independently extracted hotspot kernels, it automatically completes code into a Minimal Executable Program (MEP), then performs multi-round iterative optimization and evaluation outside the full application. The framework integrates Automatic Error Repair and Performance Pattern Inheritance to fix faults, preserve correctness, reuse effective tiling/memory/synchronization strategies, and reduce search cost. Optimized variants are reintegrated into the original application for validation. We evaluate on NVIDIA GPUs and the Haiguang Deep Computing Unit (DCU) platform (AMD-licensed architecture) using PolyBench, the AMD APP SDK, and hotspot kernels from large-scale supercomputing applications. The method achieves average speedups of 5.05x (PolyBench on NVIDIA), 7.77x (PolyBench on DCU), 1.77x (AMD APP SDK), and 1.25x on three hotspot kernels, surpassing direct LLM optimization. The approach requires no full-source dependencies, offers cross-platform portability, and enables practical, low-cost GPU kernel optimization.

</details>


### [21] [Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments](https://arxiv.org/abs/2512.22149)
*Guilin Zhang,Wulan Guo,Ziqi Tan*

Main category: cs.DC

TL;DR: This paper is related to **Compiler** (as it addresses resource allocation and deployment for AI systems, which often involves compiler-related optimization in the ML/AI domain, although not explicitly mentioned).

基于LLM的多智能体系统在无服务器GPU平台上部署时，面临异构工作负载、动态计算需求和成本效益伸缩带来的资源分配挑战。本文提出了一个自适应GPU资源分配框架，通过一个复杂度为O(N)的算法，根据工作负载、智能体优先级和最小需求动态分配资源。仿真结果显示，与循环调度相比，该框架在保持吞吐量的同时，实现了85%的延迟降低，并提高了GPU利用率，为在无服务器基础设施上部署高成本效益的多智能体AI系统提供了实用方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统在无服务器GPU平台上部署时，面临着资源分配的巨大挑战。主要原因包括：智能体工作负载的异构性、计算需求的变化以及对成本效益扩展的要求。具体来说，需要解决(1)轻量级协调器和重量级专家之间计算需求的异构性，(2)需要毫秒级重分配的动态工作负载波动，和(3)无服务器环境中的容量约束。

Method: 本文提出了一种自适应GPU资源分配框架，该框架采用了一个复杂度为O(N)的算法进行实时调整。该方法根据工作负载特性、智能体优先级和最小资源需求动态分配GPU资源。它通过建模包含轻量级协调器和重量级专家的异构多智能体系统，并在容量受限的无服务器环境中进行仿真，来解决动态工作负载波动和异构计算需求带来的挑战。

Result: 与循环调度（round-robin scheduling）相比，本文提出的自适应分配策略实现了85%的延迟降低。同时，它保持了与静态分配相当的吞吐量。该方法采用了复杂度为O(N)的算法进行实时适应，并在延迟、成本和GPU利用率方面优于静态等分和循环调度策略。

Conclusion: 本文提出的自适应GPU资源分配框架有效地解决了在无服务器GPU平台上部署LLM驱动的多智能体系统所面临的资源挑战。通过动态分配，它在保持吞吐量和提高资源利用率的同时，显著降低了延迟（85%的延迟降低），从而为在无服务器基础设施上部署高成本效益的多智能体AI系统提供了一个实用的解决方案。

Abstract: Multi-agent systems powered by large language models have emerged as a promising paradigm for solving complex reasoning tasks through collaborative intelligence. However, efficiently deploying these systems on serverless GPU platforms presents significant resource allocation challenges due to heterogeneous agent workloads, varying computational demands, and the need for cost-effective scaling. This paper presents an adaptive GPU resource allocation framework that achieves 85\% latency reduction compared to round-robin scheduling while maintaining comparable throughput to static allocation, using an $O(N)$ complexity algorithm for real-time adaptation. Our approach dynamically allocates GPU resources based on workload characteristics, agent priorities, and minimum resource requirements, enabling efficient utilization while maintaining quality of service. The framework addresses three key challenges: (1) heterogeneous computational demands across lightweight coordinators and heavyweight specialists, (2) dynamic workload fluctuations requiring millisecond-scale reallocation, and (3) capacity constraints in serverless environments. Through comprehensive simulations modeling realistic multi-agent workflows with four heterogeneous agents, we demonstrate that adaptive allocation outperforms static equal and round-robin strategies across latency, cost, and GPU utilization metrics. The framework provides a practical solution for deploying cost-efficient multi-agent AI systems on serverless GPU infrastructure.

</details>


### [22] [AiiDAlab: on the route to accelerate science](https://arxiv.org/abs/2512.22173)
*Aliaksandr V. Yakutovich,Jusong Yu,Daniel Hollas,Edan Bainglass,Corsin Battaglia,Miki Bonacci,Lucas Fernandez Vilanova,Stephan Henne,Anders Kaestner,Michel Kenzelmann,Graham Kimbell,Jakob Lass,Fabio Lopes,Daniel G. Mazzone,Andres Ortega-Guerrero,Xing Wang,Nicola Marzari,Carlo A. Pignedoli,Giovanni Pizzi*

Main category: cs.DC

TL;DR: 与DSL、图处理、MLIR、编译器或HLS无关。
摘要：AiiDAlab平台已从计算材料科学工作流发展为跨学科的科学发现加速器。它通过直观的网页界面简化复杂的计算流程，自动追踪完整的模拟溯源，确保了可重复性，并已被应用于多个领域，包括量子化学、大气建模等，同时正在集成电子实验笔记本以支持开放研究数据（ORD）。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力的不断增强，需要稳健且自动化的研究工作流来在超级计算设施中实现和协调大量相互依赖的模拟。然而，执行这些工作流通常需要技术专长来设置计算输入、解释输出和处理远程机器上并行代码执行的复杂性。AiiDAlab平台旨在解决这些挑战，通过直观的网页用户界面使复杂的计算工作流变得易于访问，从而让科学家能够专注于研究而非计算细节。

Method: 文章描述的AiiDAlab平台是实现稳健和自动化的研究工作流的方法，它通过直观的网页用户界面提供对复杂计算工作流的访问和编排，并利用底层的AiiDA引擎自动跟踪完整的模拟溯源（provenance）。此外，它还采用了简化用户入门、精简计算资源访问、提供处理大型数据集的机制，并正集成电子实验笔记本（ELNs）等方法。

Result: AiiDAlab已发展成为一个强大的平台，加速了跨多个学科的科学发现，包括量子化学、大气建模、电池研究、大型设施的实验数据分析以及教育设置。它使得科学家能专注于研究，同时通过AiiDA引擎自动记录完整的模拟溯源，确保了结果的可重复性。同时简化了用户入门、优化了对计算资源的访问，并增强了处理大型数据集的能力。此外，它还通过集成电子实验笔记本（ELNs）加强了对FAIR原则的遵循，支持了可重复的开放研究数据（ORD）的生成。

Conclusion: AiiDAlab已从一个专注于计算材料科学的平台成熟为一个强大的跨学科科学发现加速器。它简化了复杂的计算工作流程，使用户能够专注于研究，同时通过AiiDA引擎自动跟踪模拟溯源，确保了可重复性，并正在集成电子实验笔记本（ELNs）以符合FAIR原则并支持生成开放研究数据（ORD）。

Abstract: With the availability of ever-increasing computational capabilities, robust and automated research workflows are essential to enable and facilitate the execution and orchestration of large numbers of interdependent simulations in supercomputer facilities. However, the execution of these workflows still typically requires technical expertise in setting up calculation inputs, interpreting outputs, and handling the complexity of parallel code execution on remote machines. To address these challenges, the AiiDAlab platform was developed, making complex computational workflows accessible through an intuitive user interface that runs in a web browser. Here, we discuss how AiiDAlab has matured over the past few years, shifting its focus from computational materials science to become a powerful platform that accelerates scientific discovery across multiple disciplines. Thanks to its design, AiiDAlab allows scientists to focus on their research rather than on computational details and challenges, while keeping automatically track of the full simulation provenance via the underlying AiiDA engine and thus ensuring reproducibility. In particular, we discuss its adoption into quantum chemistry, atmospheric modeling, battery research, and even experimental data analysis at large-scale facilities, while also being actively used in educational settings. Driven by user feedback, significant effort has been made to simplify user onboarding, streamline access to computational resources, and provide robust mechanisms to work with large datasets. Furthermore, AiiDAlab is being integrated with electronic laboratory notebooks (ELNs), reinforcing adherence to the FAIR principles and supporting researchers in data-centric scientific disciplines in easily generating reproducible Open Research Data (ORD).

</details>


### [23] [MatKV: Trading Compute for Flash Storage in LLM Inference](https://arxiv.org/abs/2512.22195)
*Kun-Woo Shin,Jay H. Park,Moonwook Oh,Yohan Jo,Jaeyoung Do,Sang-Won Lee*

Main category: cs.DC

TL;DR: 是，该论文与**编译器**相关，因为它关注于优化推理过程中的计算效率和硬件利用，特别是KV计算的优化是LLM推理性能优化的重要组成部分。论文中的优化方法，即预计算、物化和复用KVs，是一种针对特定计算模式的系统级优化，这与编译器和系统优化的目标一致。TLDR: 随着LLM生成式AI中推理成本和RAG的普及，输入文本KV向量计算（预填充阶段）变得耗时且耗能。为解决此问题，MatKV方案提出预先计算RAG对象的KV向量并将其物化到闪存（SSD）中，推理时复用而非重新计算。实验证明，MatKV将RAG工作负载的推理时间和功耗减少了一半，且不严重影响准确性。MatKV还实现了流水线加载优化和低端GPU解码优化，使大规模生成式AI更具成本效益和能效。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成式AI的发展，出现了两个主要趋势：1）推理的成本和功耗正在成为主导因素；2）检索增强生成（RAG）变得普及。在RAG中处理长输入时，计算输入文本的键值向量（KVs）的预填充阶段即使在高端GPU上也是能耗高且耗时的，因此，提高RAG推理中预填充阶段的效率至关重要。

Method: 本文提出了MatKV方案，其核心思想是预先计算RAG对象（如文档）的键值向量（KVs），将它们物化（materialize）到廉价、快速且节能的闪存存储中，并在推理时复用这些KVs，而不是在昂贵且高功耗的GPU上重新计算。此外，MatKV还通过允许GPU在解码文本的同时加载下一个实例的物化KVs，以及利用低端GPU进行解码等方式，实现了额外的优化。

Result: 实验结果表明，与在GPU上进行完整的KV计算相比，对于RAG工作负载，MatKV将推理时间和功耗都减少了一半，并且在问答任务中没有严重影响准确性。此外，MatKV使额外的优化成为可能：1）GPU可以在解码文本的同时加载下一个实例的物化KVs，减少加载延迟；2）由于解码速度对GPU性能的敏感性低于KV计算，因此在加载物化KVs到GPU内存后，可以利用低端GPU进行解码而不会显著降低速度。

Conclusion: MatKV通过将预计算的KV向量物化到闪存存储中并在推理时复用，可以显著降低RAG工作负载的推理时间和功耗，同时保持问答任务的准确性。它还能实现额外的优化，例如同时进行文本解码和KV加载，以及利用低端GPU进行解码。这些优势使MatKV成为降低大规模生成式AI应用成本和能耗，并扩大其适用性的有效方案。

Abstract: We observe two major trends in LLM-based generative AI: (1) inference is becoming the dominant factor in terms of cost and power consumption, surpassing training, and (2) retrieval augmented generation (RAG) is becoming prevalent. When processing long inputs in RAG, the prefill phase of computing the key-value vectors of input text is energy-intensive and time-consuming even with high-end GPUs. Thus, it is crucial to make the prefill phase in RAG inference efficient. To address this issue, we propose MatKV, a scheme that precomputes the key-value vectors (KVs) of RAG objects (e.g., documents), materializes them in inexpensive but fast and power-efficient flash storage, and reuses them at inference time instead of recomputing the KVs using costly and power-inefficient GPU. Experimental results using Hugging Face's Transformers library across state-of-the-art GPUs and flash memory SSDs confirm that, compared to full KV computation on GPUs, MatKV reduces both inference time and power consumption by half for RAG workloads, without severely impacting accuracy in the question-answering task. Furthermore, we demonstrate that MatKV enables additional optimizations in two ways. First, a GPU can decode text while simultaneously loading the materialized KVs for the next instance, reducing load latency. Second, since decoding speed is less sensitive to GPU performance than KV computation, low-end GPUs can be leveraged for decoding without significantly compromising speed once the materialized KVs are loaded into GPU memory. These findings underscore MatKV's potential to make large-scale generative AI applications more cost-effective, power-efficient, and accessible across a wider range of tasks and hardware environments.

</details>


### [24] [SPUMA: a minimally invasive approach to the GPU porting of OPENFOAM](https://arxiv.org/abs/2512.22215)
*Simone Bnà,Giuseppe Giaquinto,Ettore Fadiga,Tommaso Zanelli,Francesco Bottau*

Main category: cs.DC

TL;DR: This paper is related to ***Compiler*** (by discussing the impact of programming model and optimizations for parallel computing), ***Graph Processing*** (CFD simulations often involve mesh/graph-based computations), and ***HLS*** (High-Level Synthesis, although not directly mentioned, the focus on programmability and performance on heterogeneous hardware aligns with HLS goals). The paper is related to ***Compiler*** and ***Graph Processing***.

SPUMA 是一个完整的 OpenFOAM GPU 移植方案，它利用可移植编程模型和统一内存管理，首次全面支持 NVIDIA A100 和 AMD MI250X 两种主要的预百亿亿次级 GPU。在 DrivAer 工业案例中，SPUMA 展现了良好的可扩展性（强可扩展性达 65%，弱可扩展性高达 85%），并在使用高性能求解器时效率可达 90% 以上。此外，一个 A100 GPU 的性能相当于 200-300 个 Intel Sapphire Rapids 核心，并且能耗降低了高达 82%，证明了该方案在加速计算流体力学和提高能效方面的巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管 GPU 被广泛应用于高性能计算（HPC），特别是在计算流体力学（CFD）领域，但***可编程性仍是一个挑战***，特别是在***开源***环境中，如 OpenFOAM。本文的动机是***提供一个解决这一问题的完整 GPU 移植解决方案（SPUMA）***，以有效地利用现代加速器在混合集群上实现高性能 CFD 计算。

Method: 本文介绍了 SPUMA，这是将开源 CFD 软件 OpenFOAM 完全移植到 GPU 的方法。该实现基于一种***可移植编程模型***，并采用了利用现代 GPU ***统一内存***特性的***内存池管理器***。为了验证性能，作者在欧洲的两个预百亿亿次级集群 LUMI（AMD MI250X）和 Leonardo（NVIDIA A100）上进行了数值测试，特别关注了内存使用、内核运行时间、内存池的影响以及能耗。测试案例是著名的 DrivAer 工业测试案例。

Result: 1. **强可扩展性：** 在接近每个 GPU 800 万个单元的负载时，在 LUMI 和 Leonardo 上均达到了 65% 的效率。
2. **弱可扩展性：** 在 20 个 GPU 上使用 OpenFOAM 原生多重网格求解器时，效率范围从 Leonardo 上的 75% 到 LUMI 上的 85%。
3. **求解器影响：** 切换到 NVIDIA AmgX 线性代数求解器时，效率不低于 90%。
4. **性能对比：** 在足够大的负载（每个 GPU 超过 1000 万个单元）下，Leonardo 上的一个 A100 GPU 相当于 200-300 个 Intel Sapphire Rapids 核心。
5. **能耗：** 与在 CPU 上执行的类似模拟相比，能耗降低了高达 82%。
6. **关键因素：** GPU 利用率强烈影响强可扩展性结果。

Conclusion: SPUMA 成功地将 OpenFOAM 完整移植到了 NVIDIA 和 AMD GPU 上，并在 LUMI 和 Leonardo 两个预百亿亿次级集群上进行了全面测试。结果表明，SPUMA 在强可扩展性和弱可扩展性方面表现出色（分别达到 65% 和 75%-85% 的效率），特别是在使用高性能线性代数求解器如 NVIDIA AmgX 时，效率可达 90% 以上。此外，与 CPU 相比，基于 GPU 的 SPUMA 将能耗降低了高达 82%。这证明了在混合集群上使用可移植编程模型和统一内存管理加速 CFD 工作负载的可行性和显著优势。

Abstract: High Performance Computing (HPC) on hybrid clusters represents a significant opportunity for Computational Fluid Dynamics (CFD), especially when modern accelerators are utilized effectively. However, despite the widespread adoption of GPUs, programmability remains a challenge, particularly in open-source contexts. In this paper, we present SPUMA, a full GPU porting of OPENFOAM targeting NVIDIA and AMD GPUs. The implementation strategy is based on a portable programming model and the adoption of a memory pool manager that leverages the unified memory feature of modern GPUs. This approach is discussed alongside several numerical tests conducted on two pre-exascale clusters in Europe, LUMI and Leonardo, which host AMD MI250X and NVIDIA A100 GPUs, respectively. In the performance analysis section, we present results related to memory usage profiling and kernel wall-time, the impact of the memory pool, and energy consumption obtained by simulating the well-known DrivAer industrial test case. GPU utilization strongly affects strong scalability results, reaching 65% efficiency on both LUMI and Leonardo when approaching a load of 8 million cells per GPU. Weak scalability results, obtained on 20 GPUs with the OpenFOAM native multigrid solver, range from 75% on Leonardo to 85% on LUMI. Notably, efficiency is no lower than 90% when switching to the NVIDIA AmgX linear algebra solver. Our tests also reveal that one A100 GPU on Leonardo is equivalent 200-300 Intel Sapphire Rapids cores, provided the GPUs are sufficiently oversubscribed (more than 10 million of cells per GPU). Finally, energy consumption is reduced by up to 82% compared to analogous simulations executed on CPUs.

</details>


### [25] [Scalable Cloud-Native Architectures for Intelligent PMU Data Processing](https://arxiv.org/abs/2512.22231)
*Nachiappan Chockalingam,Akshay Deshpande,Lokesh Butra,Ram Sekhar Bodala,Nitin Saksena,Adithya Parthasarathy,Balakrishna Pothineni,Akash Kumar Agarwal*

Main category: cs.DC

TL;DR: This content has not passed the compliance test and has been hidden.


<details>
  <summary>Details</summary>
Motivation: 随着相量测量单元（PMU）部署规模的不断扩大，PMU 数据的高频、时间同步特性给实时电网监控带来了延迟、可扩展性和可靠性方面的严峻挑战。传统的集中式处理架构越来越难以应对 PMU 数据的体量和速度，尤其是在运行条件动态变化的现代电网中。因此，需要一个可扩展、低延迟且可靠的新型架构来处理和分析大规模 PMU 数据。

Method: 本文提出了一种可扩展的云原生架构，用于智能 PMU 数据处理。该方法整合了人工智能（AI）、边缘计算和云计算，采用分布式流处理、容器化微服务和弹性资源编排。其具体方法包括：实现低延迟数据摄取、实时异常检测和高级分析；集成用于时间序列分析的机器学习模型，以增强电网可观测性和预测能力；开发分析模型来评估系统的延迟、吞吐量和可靠性；以及嵌入安全和隐私机制。

Result: 通过提出的架构和分析模型评估，证明该架构能够实现亚秒级的响应时间，同时可以扩展到大规模的 PMU 部署。这表明该架构成功地解决了现有系统在处理大量 PMU 数据时面临的延迟、吞吐量和可扩展性问题，增强了电网的可观测性和预测能力。此外，该系统还嵌入了用于关键基础设施环境的安全和隐私机制。

Conclusion: 本文提出了一种可扩展的云原生 PMU 数据处理架构，通过整合 AI、边缘和云计算，以及采用分布式流处理、容器化微服务和弹性资源编排等技术，克服了传统集中式架构在处理大规模 PMU 数据时面临的延迟、可扩展性和可靠性挑战，为下一代智能电网提供了高性能、安全且灵活的分析基础。

Abstract: Phasor Measurement Units (PMUs) generate high-frequency, time-synchronized data essential for real-time power grid monitoring, yet the growing scale of PMU deployments creates significant challenges in latency, scalability, and reliability. Conventional centralized processing architectures are increasingly unable to handle the volume and velocity of PMU data, particularly in modern grids with dynamic operating conditions. This paper presents a scalable cloud-native architecture for intelligent PMU data processing that integrates artificial intelligence with edge and cloud computing. The proposed framework employs distributed stream processing, containerized microservices, and elastic resource orchestration to enable low-latency ingestion, real-time anomaly detection, and advanced analytics. Machine learning models for time-series analysis are incorporated to enhance grid observability and predictive capabilities. Analytical models are developed to evaluate system latency, throughput, and reliability, showing that the architecture can achieve sub-second response times while scaling to large PMU deployments. Security and privacy mechanisms are embedded to support deployment in critical infrastructure environments. The proposed approach provides a robust and flexible foundation for next-generation smart grid analytics.

</details>


### [26] [Efficient Multi-Model Orchestration for Self-Hosted Large Language Models](https://arxiv.org/abs/2512.22402)
*Bhanu Prakash Vangala,Tanu Malik*

Main category: cs.DC

TL;DR: 该论文与编译器、HLS、MLIR、图处理和DSL等技术不直接相关。

**太长不看 (TLDR):** Pick and Spin是一个基于Kubernetes的实用框架，用于扩展和优化自托管大型语言模型（LLMs）的部署和编排。它通过统一的Helm部署、自适应的规模归零（Scale-to-Zero）和混合路由（结合关键词和DistilBERT分类器）来提高GPU利用率和可靠性。与静态部署相比，该框架在四种不同规模的模型上，实现了更高的成功率（+21.6%）、更低的延迟（-30%）和更低的GPU成本（-33%）。


<details>
  <summary>Details</summary>
Motivation: 组织出于对隐私、成本控制和定制化的需求，越来越倾向于自托管大型语言模型（LLMs）。然而，在内部署和维护这些模型面临着GPU利用率、工作负载路由和可靠性方面的挑战。

Method: 采用了基于Kubernetes的框架，包含统一的Helm部署系统、自适应的Scale-to-Zero自动化，以及一个混合路由模块。该路由模块结合了关键词启发式和轻量级的DistilBERT分类器，用于平衡成本、延迟和准确性。通过评估四种不同规模的模型（Llama-3 (90B), Gemma-3 (27B), Qwen-3 (235B), DeepSeek-R1 (685B)），五种推理策略，以及两种路由变体，在八个公共基准数据集上进行了31,019个提示和163,720次推理运行的全面测试。

Result: 与相同模型的静态部署相比，Pick and Spin框架将成功率提高了21.6%，延迟降低了30%，每查询的GPU成本降低了33%。

Conclusion: Pick and Spin框架通过统一的部署、自适应的伸缩和混合路由，实现了自托管大型语言模型的高效、经济和可靠运行，显著优于静态部署。

Abstract: Self-hosting large language models (LLMs) is increasingly appealing for organizations seeking privacy, cost control, and customization. Yet deploying and maintaining in-house models poses challenges in GPU utilization, workload routing, and reliability. We introduce Pick and Spin, a practical framework that makes self-hosted LLM orchestration scalable and economical. Built on Kubernetes, it integrates a unified Helm-based deployment system, adaptive scale-to-zero automation, and a hybrid routing module that balances cost, latency, and accuracy using both keyword heuristics and a lightweight DistilBERT classifier. We evaluate four models, Llama-3 (90B), Gemma-3 (27B), Qwen-3 (235B), and DeepSeek-R1 (685B) across eight public benchmark datasets, with five inference strategies, and two routing variants encompassing 31,019 prompts and 163,720 inference runs. Pick and Spin achieves up to 21.6% higher success rates, 30% lower latency, and 33% lower GPU cost per query compared with static deployments of the same models.

</details>


### [27] [Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving](https://arxiv.org/abs/2512.22420)
*Rui Li,Zhaoning Zhang,Libo Zhang,Huaimin Wang,Xiang Fu,Zhiquan Lai*

Main category: cs.DC

TL;DR: 该论文与编译器、DSL、图处理、MLIR 或 HLS 无关。

太长不看：投机解码（SD）加速 LLM 推理，但固定长度在不同负载下性能不佳。本文提出了 Nightjar，一种基于学习的自适应 SD 算法，它能根据请求负载动态调整投机长度或禁用 SD，从而在实时服务场景中实现高达 14.8% 的吞吐量提升和 20.2% 的延迟降低。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 推理加速方法——投机解码（Speculative Decoding，SD）——在低负载、内存受限的系统中提高了吞吐量，但在高负载、计算受限的环境中由于验证开销会降低性能。当前 SD 实现中的固定推测长度无法适应动态请求速率，在实际服务场景中造成了显著的性能瓶颈。作者旨在通过提出 Nightjar 解决这一关键的权衡问题并优化性能。

Method: Nightjar 是一种新的基于学习的自适应推测推断算法。它不是使用固定的推测长度，而是动态地选择不同批次大小的最佳推测长度，甚至在投机解码没有优势时将其禁用，以适应动态的请求速率和不同的系统负载。

Result: 与标准的投机解码相比，Nightjar 实现了高达 14.8% 的吞吐量提升和 20.2% 的延迟降低。这表明 Nightjar 在实时服务中具有强大的效率和鲁棒性。

Conclusion: 我们提出了一种名为 Nightjar 的新颖的基于学习的自适应推断算法，它能够根据请求负载动态调整推测长度，并在没有益处时禁用推测解码。实验和分析表明，Nightjar 在吞吐量和延迟方面均优于传统的投机解码，展现出在实时服务场景中的强大效率，最高可实现 14.8% 的吞吐量提升和 20.2% 的延迟降低。

Abstract: Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Current SD implementations use a fixed speculative length, failing to adapt to dynamic request rates and creating a significant performance bottleneck in real-world serving scenarios. To overcome this, we propose Nightjar, a novel learning-based algorithm for adaptive speculative inference that adjusts to request load by dynamically selecting the optimal speculative length for different batch sizes and even disabling speculative decoding when it provides no benefit. Experiments show that Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.

</details>


### [28] [Role-Based Fault Tolerance System for LLM RL Post-Training](https://arxiv.org/abs/2512.22492)
*Zhenqian Chen,Baoquan Zhong,Xiang Li,Qing Dai,Xinkui Zhao,Miao Ye,Ren Cheng,Lufei Zhang,Jianwei Yin*

Main category: cs.DC

TL;DR: 相关性：该论文与**编译器**（系统优化）、**图处理**（分布式系统和通信机制）和**MLIR**（系统底层优化，虽然未直接提及但相关性较弱）和**HLS**（硬件系统设计，虽然未直接提及但相关性较弱）和**DSL**（领域特定语言，未直接提及）和**编译器**（虽然未直接提及但系统级优化相关）不直接相关。最相关的领域是**分布式系统/高性能计算**和**大语言模型（LLM）训练/RL后训练**。TLDR: 现有LLM RL后训练的故障容忍系统在处理训练和推理交错的工作负载时效率低下，经常需要重启整个任务，造成巨大开销。RobustRL是首个针对RL后训练中GPU机器错误的全面鲁棒系统，通过基于**角色的故障隔离**，将Trainer、Rollout等视为独立子任务。当故障发生时，RobustRL仅恢复失败的角色并重新连接到存活的角色，避免了全任务重启的开销（如Rollout重放）。它通过角色感知检测、非破坏性恢复和动态UCX点对点通信同步权重，实现了更高的有效训练时间比（ETTR）。实验显示，在10%故障注入下，RobustRL的ETTR超过80%（相比于ByteRobust的60%），并使端到端训练时间加快8.4%至17.4%。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM故障容忍框架大多只针对训练或推理，没有充分探索RL后训练中训练和推理工作负载交错执行所带来的异步优化潜力。RL后训练中的故障会导致系统面临来自训练和推理两方面的风险。现有的故障恢复策略（如ByteRobust）通常需要重启整个RL任务，这会导致巨大的开销，包括Rollout重放和初始化延迟，从而降低训练效率。因此，迫切需要一种新的鲁棒系统来提高RL后训练在面临GPU机器故障时的有效训练时间比（ETTR）。

Method: RobustRL通过三步法实现其鲁棒性：1. **检测（Detect）**：实现角色感知监控，区分真实故障与角色特定行为，避免误报和延迟检测。2. **重启（Restart）**：对训练器（Trainer）采用非破坏性恢复，让Rollout角色持久化状态并继续轨迹生成，并通过Rollout热备（warm standbys）快速恢复训练器。对Rollout角色则执行隔离的机器替换。3. **重新连接（Reconnect）**：用动态、基于UCX的点对点通信替代静态集体通信，实现故障恢复后角色间权重的即时同步。核心洞察是基于角色的故障隔离。

Result: 在256个GPU集群上，使用Qwen3-8B-Math工作负载和10%的故障注入频率进行RL训练任务测试时：RobustRL的有效训练时间比（ETTR）可以达到80%以上，显著高于ByteRobust的60%。RobustRL在端到端训练时间上快了8.4%至17.4%。

Conclusion: RobustRL通过引入基于角色的故障隔离机制，显著提高了LLM RL后训练的鲁棒性和训练效率。它在检测、重启和重新连接三个关键方面进行了创新，使得系统在面对GPU机器故障时，能够非破坏性地恢复失败的角色，避免了传统重启整个任务的巨大开销。实验证明，RobustRL在故障注入场景下，相较于现有系统，实现了更高的有效训练时间比和更快的端到端训练时间。

Abstract: RL post-training for LLMs has been widely scaled to enhance reasoning and tool-using capabilities. However, RL post-training interleaves training and inference workloads, exposing the system to faults from both sides. Existing fault tolerance frameworks for LLMs target either training or inference, leaving the optimization potential in the asynchronous execution unexplored for RL. Our key insight is role-based fault isolation so the failure in one machine does not affect the others. We treat trainer, rollout, and other management roles in RL training as distinct distributed sub-tasks. Instead of restarting the entire RL task in ByteRobust, we recover only the failed role and reconnect it to living ones, thereby eliminating the full-restart overhead including rollout replay and initialization delay.
  We present RobustRL, the first comprehensive robust system to handle GPU machine errors for RL post-training Effective Training Time Ratio improvement. (1) \textit{Detect}. We implement role-aware monitoring to distinguish actual failures from role-specific behaviors to avoid the false positive and delayed detection. (2) \textit{Restart}. For trainers, we implement a non-disruptive recovery where rollouts persist state and continue trajectory generation, while the trainer is rapidly restored via rollout warm standbys. For rollout, we perform isolated machine replacement without interrupting the RL task. (3) \textit{Reconnect}. We replace static collective communication with dynamic, UCX-based (Unified Communication X) point-to-point communication, enabling immediate weight synchronization between recovered roles. In an RL training task on a 256-GPU cluster with Qwen3-8B-Math workload under 10\% failure injection frequency, RobustRL can achieve an ETTR of over 80\% compared with the 60\% in ByteRobust and achieves 8.4\%-17.4\% faster in end-to-end training time.

</details>


### [29] [RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure](https://arxiv.org/abs/2512.22560)
*Wei Gao,Yuheng Zhao,Tianyuan Wu,Shaopan Xiong,Weixun Wang,Dakai An,Lunxi Cao,Dilxat Muhtar,Zichen Liu,Haizhou Zhao,Ju Huang,Siran Yang,Yongbin Li,Wenbo Su,Jiamang Wang,Lin Qu,Bo Zheng,Wei Wang*

Main category: cs.DC

TL;DR: 该论文与DSL、图处理、MLIR、编译器、HLS均不直接相关，它专注于**分布式系统**和**强化学习**（Agentic RL）工作负载的优化。
TLDR: 智能体强化学习（Agentic RL）训练具有高度异构性，在分散式基础设施上会造成同步开销和资源浪费。RollArc是一个分布式系统，通过硬件亲和性映射、细粒度异步和有状态感知计算，将异构任务分发到最合适的硬件并实现弹性扩展。它显著提高了训练吞吐量，将端到端训练时间减少了1.35-2.05倍，并在一个超过3,000个GPU的大规模MoE模型训练中展示了其鲁棒性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 智能体强化学习（Agentic RL）训练工作负载高度异构，结合了计算密集的预填充阶段、带宽限制的解码以及有状态、CPU密集型的环境模拟。这种异构性使得现有的基础设施难以有效处理，特别是在分散式基础设施上，朴素的解耦会引入大量的同步开销和资源利用率不足。因此，需要一个能够利用专业化硬件并最大化吞吐量的系统来高效地进行Agentic RL训练。

Method: 本文提出了RollArc，一个分布式系统，旨在最大化分散式基础设施上多任务智能体强化学习的吞吐量。RollArc建立在三个核心原则上：1. 硬件亲和性工作负载映射，将计算密集型和带宽密集型任务分配给最合适的GPU设备；2. 细粒度异步，在轨迹级别管理执行以缓解资源气泡；3. 有状态感知的计算，将无状态组件（如奖励模型）卸载到无服务器基础设施进行弹性扩展。

Result: RollArc显著提高了训练吞吐量，相比单体和同步基线，端到端训练时间减少了1.35-2.05倍。此外，RollArc在阿里巴巴集群上使用超过3,000个GPU训练了一个具有数千亿参数的MoE模型用于Qoder产品，进一步证明了其可扩展性和鲁棒性。

Conclusion: RollArc通过在分散式基础设施上应用硬件亲和性工作负载映射、细粒度异步以及有状态感知的计算卸载，解决了智能体强化学习（Agentic RL）训练中固有的异构性和同步开销问题。RollArc在训练吞吐量和端到端训练时间上优于单体和同步基线（1.35-2.05倍的降低），并在大规模MoE模型训练中展示了其可扩展性和鲁棒性，证明了其在高效Agentic RL训练中的有效性。

Abstract: Agentic Reinforcement Learning (RL) enables Large Language Models (LLMs) to perform autonomous decision-making and long-term planning. Unlike standard LLM post-training, agentic RL workloads are highly heterogeneous, combining compute-intensive prefill phases, bandwidth-bound decoding, and stateful, CPU-heavy environment simulations. We argue that efficient agentic RL training requires disaggregated infrastructure to leverage specialized, best-fit hardware. However, naive disaggregation introduces substantial synchronization overhead and resource underutilization due to the complex dependencies between stages.
  We present RollArc, a distributed system designed to maximize throughput for multi-task agentic RL on disaggregated infrastructure. RollArc is built on three core principles: (1) hardware-affinity workload mapping, which routes compute-bound and bandwidth-bound tasks to bestfit GPU devices, (2) fine-grained asynchrony, which manages execution at the trajectory level to mitigate resource bubbles, and (3) statefulness-aware computation, which offloads stateless components (e.g., reward models) to serverless infrastructure for elastic scaling. Our results demonstrate that RollArc effectively improves training throughput and achieves 1.35-2.05\(\times\) end-to-end training time reduction compared to monolithic and synchronous baselines. We also evaluate RollArc by training a hundreds-of-billions-parameter MoE model for Qoder product on an Alibaba cluster with more than 3,000 GPUs, further demonstrating RollArc scalability and robustness. The code is available at https://github.com/alibaba/ROLL.

</details>


### [30] [Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference](https://arxiv.org/abs/2512.22695)
*Mona Moghadampanah,Adib Rezaei Shahmirzadi,Farhana Amin,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: 该论文与编译器和高性能计算（HPC）相关。它分析了多模态大语言模型（MLLMs）在推理过程中的能耗和效率问题，特别关注“模态膨胀”带来的额外开销。研究通过阶段性分析量化了能耗开销（17% 至 94%），确定了能耗瓶颈，揭示了 GPU 利用不足，并提出阶段性动态电压和频率缩放（DVFS）作为有效的节能优化策略。


<details>
  <summary>Details</summary>
Motivation: 现有的 MLLM 通过整合额外的模态（如视觉）增强了功能，但这种新增带来了先前未被探索的跨模态能源权衡问题，并且由于大多数先前的工作只关注纯文本模型，因此对这个权衡的理解很有限。论文的动机是探讨“模态膨胀”——这是一个关键的效率低下来源，即多模态输入通过额外的编码阶段和扩展的 token 序列增加了推理工作负载。因此，需要对 MLLM 推理的能耗进行深入的、阶段性的分析。

Method: 作者通过将 MLLM 推理管线分解为视觉编码、预填充和解码三个阶段，首次进行了详细的、阶段级别的能耗分析。他们使用四种具有代表性的 MLLM 模型，在 NVIDIA A100 GPU 上进行评估，量化了相对于纯文本基线的多模态推理所需的额外能耗。通过检查 GPU 功率轨迹，作者揭示了多模态执行期间 GPU 的利用不足，并展示了输入复杂性如何导致不同模型之间能耗扩展行为的显著差异。最后，他们展示了阶段性动态电压和频率缩放（DVFS）是一种有效的优化方法。

Result: 研究量化了多模态推理与纯文本基线相比所需的额外能耗，观察到模型间的开销范围在 17% 到 94% 之间（对于相同输入）。结果表明，能耗瓶颈因模型架构而异，可能源于计算密集的视觉编码器，或源于预填充阶段中大规模视觉 token 序列的下游影响。研究还揭示了在多模态执行过程中存在显著的 GPU 利用不足。输入复杂性导致不同模型之间的能耗扩展行为明显不同。此外，作者证明了阶段性 DVFS 是一种有效的优化手段，可以在适度的性能影响下实现能源节约。

Conclusion: 这篇论文提供了一套新的见解和实际指导，旨在设计更节能的多模态 LLM 服务系统。研究表明，阶段性 DVFS 是一种有效的优化手段，可以在适度的性能影响下实现能源节约。

Abstract: Multimodal large language models (MLLMs) are built on text-only LLMs by incorporating additional modalities, enabling multimodal understanding and a broader range of applications. However, these additions introduce a previously unexplored energy trade-off across modalities that remains poorly understood, as most prior work focuses on text-only models. In this paper, we examine modality inflation, a key source of inefficiency in which multimodal inputs increase inference workloads through extra encoding stages and expanded token sequences. We provide the first detailed, stage-level analysis of energy consumption in MLLM inference by breaking the pipeline into vision encoding, prefill, and decoding stages. Using four representative MLLMs evaluated on NVIDIA A100 GPU, we quantify the additional energy required for multimodal inference compared to text-only baselines, observing overheads ranging from 17% to 94% across models for identical inputs. Our results show that energy bottlenecks differ widely across model architectures, stemming either from compute-heavy vision encoders or from the downstream impact of large visual token sequences during prefill. By examining GPU power traces, we further uncover substantial GPU underutilization during multimodal execution and show that input complexity leads to markedly different energy scaling behaviors across models. Finally, we demonstrate that stage-wise dynamic voltage and frequency scaling (DVFS) is an effective optimization, allowing energy savings with only modest performance impact. Together, these findings offer practical insights and concrete guidance for designing more energy-efficient multimodal LLM serving systems.

</details>


### [31] [Argus: Token Aware Distributed LLM Inference Optimization](https://arxiv.org/abs/2512.22925)
*Panlong Wu,Yifei Zhong,Danyang Chen,Ting Wang,Fangxin Wang*

Main category: cs.DC

TL;DR: 相关：该论文与**LLM**、**编译器**（涉及推理优化和部署）、**图处理**（如果模型内部使用了图结构优化，但摘要未明确说明）相关。具体来说，它是一个关于LLM推理优化和部署的系统工作。
总结：Argus是首个令牌感知的分布式边缘-云LLM推理框架，旨在解决异构边缘-云环境中LLM推理时间的高度变动性问题。它通过Length-Aware Semantics (LAS)模块预测精确的输出令牌长度，然后利用Lyapunov-guided Offloading Optimization (LOO)模块，考虑预填充和解码成本，将长期用户体验质量（QoE）优化公式化。最后，采用带有阻尼和拥塞控制的迭代卸载算法（IODCC）来求解优化问题。实验证明Argus在动态和异构设置中具有优越的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）推理部署方案未能充分解决在异构边缘-云系统中固有的动态、随机和异构性问题，尤其是忽略了可变的输出令牌长度和设备多样性对推理时间变动性的影响，这限制了LLMs在实际应用中的效率和性能。

Method: Argus框架包括：1. **长度感知语义（Length-Aware Semantics, LAS）模块**：使用经过微调的语言模型，通过敏感于令牌长度的特征调制来预测输入提示的输出令牌长度，实现精确估计。2. **Lyapunov指导的卸载优化（Lyapunov-guided Offloading Optimization, LOO）模块**：将长期用户体验质量（QoE）优化公式化，明确考虑了LLM的预填充和解码成本。3. **带有阻尼和拥塞控制的迭代卸载算法（Iterative Offloading Algorithm with Damping and Congestion Control, IODCC）**：用于在时变约束下有效解决由此产生的整数非线性规划问题。

Result: 广泛的理论和实证评估证明，Argus在高度动态和异构的环境中实现了鲁棒的性能和卓越的效率。它成为首个令牌感知的分布式边缘-云LLM推理框架，能够进行高效的任务卸载。

Conclusion: Argus是首个令牌感知的分布式边缘-云LLM推理框架，通过LAS实现精确的输出长度预测，并通过LOO和IODCC实现高效和鲁棒的任务卸载，显著提高了LLM在动态和异构环境中的推理效率和性能。

Abstract: Large Language Models (LLMs) are rapidly being integrated into real-world applications, yet their autoregressive architectures introduce significant inference time variability, especially when deployed across heterogeneous edge-cloud systems. Existing solutions largely neglect the dynamic, stochastic, and heterogeneous nature of such environments, often ignoring the impact of variable output token lengths and device diversity. In this work, we present Argus, the first token-aware distributed edge-cloud LLM inference framework that conducts efficient task offloading. Argus features a Length-Aware Semantics (LAS) module, which predicts output token lengths for incoming prompts using a fine-tuned language model with token-length-sensitive feature modulation, enabling precise estimation. Building on this, our Lyapunov-guided Offloading Optimization (LOO) module formulates long-term Quality-of-Experience optimization that explicitly considers both LLM prefilling and decoding costs. We introduce a novel Iterative Offloading Algorithm with Damping and Congestion Control (IODCC) to effectively solve the resulting integer nonlinear programming problem under time-varying constraints. Extensive theoretical and empirical evaluations demonstrate that Argus achieves robust performance and superior efficiency in highly dynamic, heterogeneous settings.

</details>


### [32] [Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates](https://arxiv.org/abs/2512.23434)
*Yongjie Guan*

Main category: cs.DC

TL;DR: This paper is related to graph processing and compiler, especially the hash table structure and search. The paper introduces Local Rendezvous Hashing (LRH), a new consistent hashing scheme that combines a token ring with restricted Highest Random Weight (HRW) selection within a cache-local window of C neighboring physical nodes. LRH achieves a Max/Avg load ratio of 1.0947 and a throughput of 60.05 Mkeys/s (6.8x faster than multi-probe consistent hashing) in a benchmark, with a lookup cost of $O(\log|R| + C)$ and zero excess churn under liveness changes.


<details>
  <summary>Details</summary>
Motivation: 现有的三种主要一致性哈希方案存在局限性：环形方案需要大量虚拟节点才能实现良好的负载均衡，导致高昂的内存和计算成本；多探头方案虽然改善了负载均衡，但由于分散的内存访问而导致性能下降；传统的 Rendezvous Hashing 虽然在负载均衡上表现良好，但通常需要对所有节点进行计算，效率不高。因此，需要一种既能实现良好负载均衡，又能保持高查找性能的解决方案。

Method: 本文提出了 Local Rendezvous Hashing (LRH)，它保留了令牌环结构，但将最高随机权重 (HRW) 的选择限制在 C 个不同的相邻物理节点的缓存局部窗口内。LRH 通过一次二分查找定位键，使用预先计算的下一个不同偏移量精确枚举 C 个候选节点，并从中选择 HRW 赢家（可选加权）。查找成本为 $O(\log|R| + C)$。在固定拓扑下，LRH 在节点失效时实现零额外流失。通过限制选择范围和使用预计算的偏移量，LRH 显著提高了查找速度。

Result: 在 $N=5000$ 个物理节点、$V=256$ 个虚拟节点（$|R|=1.28M$ 个环节点）、$K=50M$ 个键和窗口大小 $C=8$ 的基准测试中，LRH 将峰值/平均负载比从传统环形哈希的 $1.2785$ 降低到 $1.0947$。LRH 实现了 $60.05$ Mkeys/s 的吞吐量，比采用 $8$ 个探头的多探头一致性哈希快约 $6.8$ 倍（后者为 $8.80$ Mkeys/s），同时其负载均衡性能接近后者（峰值/平均 $1.0697$）。LRH 的查找成本是 $O(\log|R| + C)$。在节点失效时，LRH 实现零额外流失。微基准测试表明，多探头分配的性能瓶颈在于重复的环形搜索和内存流量，而不是探头生成算术。

Conclusion: Local Rendezvous Hashing (LRH) 是一种新的选择一致性哈希目标节点的算法，它通过结合环形结构和限制最高随机权重 (HRW) 选择范围来提高性能和负载均衡。LRH 在保持低查找成本和避免内存访问分散的同时，显著提高了吞吐量并提供了可接受的负载均衡，特别在大规模和高并发场景中具有优势。

Abstract: Consistent hashing is fundamental to distributed systems, but ring-based schemes can exhibit high peak-to-average load ratios unless they use many virtual nodes, while multi-probe methods improve balance at the cost of scattered memory accesses. This paper introduces Local Rendezvous Hashing (LRH), which preserves a token ring but restricts Highest Random Weight (HRW) selection to a cache-local window of C distinct neighboring physical nodes. LRH locates a key by one binary search, enumerates exactly C distinct candidates using precomputed next-distinct offsets, and chooses the HRW winner (optionally weighted). Lookup cost is O(log|R| + C). Under fixed-topology liveness changes, fixed-candidate filtering remaps only keys whose original winner is down, yielding zero excess churn. In a benchmark with N=5000, V=256 (|R|=1.28M), K=50M and C=8, LRH reduces Max/Avg load from 1.2785 to 1.0947 and achieves 60.05 Mkeys/s, about 6.8x faster than multi-probe consistent hashing with 8 probes (8.80 Mkeys/s) while approaching its balance (Max/Avg 1.0697). A microbenchmark indicates multi-probe assignment is dominated by repeated ring searches and memory traffic rather than probe-generation arithmetic.

</details>


### [33] [Optimal Configuration of API Resources in Cloud Native Computing](https://arxiv.org/abs/2512.23494)
*Eddy Truyen,Wouter Joosen*

Main category: cs.DC

TL;DR: 该论文与编译器和 DSL 或图处理或 MLIR 或 HLS 无关。

这篇论文介绍了如何将一个离线性能优化框架应用于微服务应用的 DevOps 发布阶段，以优化 CPU 和内存的资源分配配置。研究评估了不同的优化算法（包括前期因素筛选和贝叶斯优化）使用 TeaStore 微服务应用，发现前期因素筛选有助于在采样预算有限时找到最优配置，但对于寻找近最优配置，不进行筛选的贝叶斯优化可能更优。这一工作旨在解决预部署阶段资源精细调整研究不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的微服务性能优化研究大多集中在 DevOps 生命周期“运营”（Ops）阶段的智能调度和自动伸缩，而对于“发布”（Release）阶段，特别是在部署前预先精细调整 CPU 和内存等资源分配配置参数的问题，研究较少。然而，如果不预先正确配置资源，即使进行自动伸缩（如基于 CPU 使用率的水平扩展），容器的内存分配仍可能不当。本研究的动机是解决这一未被充分探索的问题，通过评估一个离线性能优化框架来优化微服务应用的预部署资源配置。

Method: 本文的方法是采用一个现有的离线性能优化框架，将其应用于微服务应用的发布阶段，专注于优化 CPU 和内存的资源配置参数。研究使用了 TeaStore 微服务应用作为评估对象，并统计性地比较了包括前期因素筛选和贝叶斯优化在内的不同优化算法，以评估它们在采样成本和接近最优配置的距离之间的性能权衡。

Result: 研究结果表明，前期因素筛选（Factor Screening）对于缩小搜索空间，在采样预算有限的情况下寻找“最优”资源配置是有帮助的。当目标是“统计比较”不同的优化算法时，也需要进行筛选以使数据收集可行。然而，如果目标是找到一个“近最优”配置，不进行筛选而直接运行贝叶斯优化（Bayesian Optimization）可能是一个更好的选择。这意味着优化策略应根据具体的优化目标（最优性、可比较性或近最优性）进行调整。

Conclusion: 本文分析了一个已有的离线性能优化框架，用于在 DevOps 生命周期中的发布（Release）阶段对微服务应用的 CPU 和内存资源配置进行优化。研究结果通过 TeaStore 微服务应用进行了评估，并统计比较了不同的优化算法，展示了在不同优化目标下，是否进行前期因素筛选和采用哪种优化算法（如贝叶斯优化）的权衡。主要的结论是，针对不同目标（寻找最优配置、比较算法或寻找近最优配置），前期因素筛选和优化算法的选择策略应有所不同，以平衡采样成本和优化效果。

Abstract: This paper presents how an existing framework for offline performance optimization can be applied to microservice applications during the Release phase of the DevOps life cycle. Optimization of resource allocation configuration parameters for CPU and memory during the Release phase remains a largely unexplored problem as most research has focused on intelligent scheduling and autoscaling of microservices during the Ops stage of the DevOps cycle. Yet horizontal auto-scaling of containers, based on CPU usage for instance, may still leave these containers with an inappropriately allocated amount of memory, if no upfront fine-tuning of both resources is applied before the Deployment phase. We evaluate the performance optimization framework using the TeaStore microservice application and statistically compare different optimization algorithms, supporting informed decisions about their trade-offs between sampling cost and distance to the optimal resource configuration. This shows that upfront factor screening, for reducing the search space, is helpful when the goal is to find the optimal resource configuration with an affordable sampling budget. When the goal is to statistically compare different algorithms, screening must also be applied to make data collection of all data points in the search space feasible.  If the goal is to find a near-optimal configuration, however, it is better to run bayesian optimization without screening.

</details>


### [34] [Decoupling Adaptive Control in TeaStore](https://arxiv.org/abs/2512.23495)
*Eddy Truyen*

Main category: cs.DC

TL;DR: 这不是一个与 DSL、图处理、MLIR、编译器或 HLS 直接相关的论文。其主要研究领域是微服务、自适应系统和软件架构。/太长不看：本文讨论了如何将自适应控制逻辑从微服务应用（TeaStore）中解耦出来。文章分析了软件架构方法、云原生 Operator 模式和传统编程语言技术，在实现系统范围一致性、规划性和模块化等自适应关键特性时的优缺点，并提出可以结合这些方法构建多层级的自适应微服务架构。


<details>
  <summary>Details</summary>
Motivation: TeaStore 规范提供了一个基于微服务的案例研究，用于通过控制循环实现自适应。然而，实现自适应时，需要考虑自适应的关键特性：系统范围的一致性（跨副本的协调适应）、规划（执行适应直到满足适当条件）和模块化（适应逻辑的清晰集成）。本文的动机在于探讨如何实现这些关键特性，特别是如何将自适应控制逻辑从 TeaStore 应用中解耦出来。

Method: 本文通过深入探讨和分析，研究了三种不同的技术方法：软件架构方法、云原生 Operator 模式以及传统编程语言技术，如何用于实现将自适应控制逻辑与微服务应用（TeaStore）解耦的目标。同时，本文对这些方法在细粒度表达性适应与系统级控制之间的权衡进行了比较分析。

Result: 研究了软件架构方法、云原生 Operator 模式和传统编程语言技术在实现自适应控制逻辑与 TeaStore 应用解耦时的有效性，并分析了它们在“细粒度表达性适应”与“系统范围控制”之间的权衡。分析表明，在某些情况下，自适应策略的重用非常有效。最终的分析结论是这些方法并非互斥，可以结合成一个多层级的自适应微服务架构。

Conclusion: 本文分析了将软件架构方法、云原生 Operator 模式以及传统编程语言技术结合起来，以实现自适应控制逻辑与 TeaStore 应用解耦的可能性。研究发现，这些方法并非互斥，而是可以结合成一个多层级架构，从而为自适应微服务提供一个灵活且分层的适应能力。

Abstract: The Adaptable TeaStore specification provides a microservice-based case study for implementing self-adaptation through a control loop.  We argue that implementations of this specification should be informed by key properties of self-adaptation: system-wide consistency (coordinated adaptations across replicas), planning (executing an adaptation until appropriate conditions are met),  and modularity (clean integration of adaptation logic).  In this implementation discussion paper, we examine how software architectural methods, the cloud-native Operator pattern, and legacy programming language techniques can decouple self-adaptive control logic from the TeaStore application. We analyze the trade-offs that these different approaches make between fine-grained expressive adaptation and system-wide control, and highlight when reuse of adaptation strategies is most effective. Our analysis suggests that these approaches are not mutually exclusive but can be combined into a multi-tiered architecture for self-adaptive microservices.

</details>
