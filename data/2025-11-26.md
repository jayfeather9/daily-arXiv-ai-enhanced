<div id=toc></div>

# Table of Contents

- [cs.DS](#cs.DS) [Total: 4]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 14]


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [1] [The Buffer Minimization Problem for Scheduling Flow Jobs with Conflicts](https://arxiv.org/abs/2511.19690)
*Niklas Haas,Sören Schmitt,Rob van Stee*

Main category: cs.DS

TL;DR: 这个摘要与图处理（冲突图）和编译器或HLS（调度/缓冲区最小化）相关。
该论文研究了流模型下多处理器系统中具有冲突的在线缓冲区最小化问题。工作负载连续到达，目标是最小化在遵守冲突限制的有效调度下，所有处理器上观察到的最大工作负载。作者为特定的四顶点图、完全图、完全二分图提供了紧密界限，为完全 k-分图提供了接近紧密的界限，并收紧了原始模型中某个特定图的界限。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于研究在引入的流模型中，多处理器系统中的在线缓冲区最小化问题。在这个模型中，工作负载连续到达，目标是找到一个有效的调度策略，最小化所有处理器上的最大工作负载，同时遵守冲突图所定义的限制。同时，本文也旨在缩短原始模型中特定图的界限差距。

Method: 研究方法是为流模型下的在线缓冲区最小化问题提供紧密或接近紧密的竞争界限。具体而言，该方法针对所有具有四个顶点（除了已被解决的路径图）、一般完全图、完全二分图以及完全 k-分图进行了分析。此外，也对原始模型中包含一个三角形和一个附加边的图的界限进行了收紧。

Result: 研究结果为流模型中的在线缓冲区最小化问题提供了以下成果：（1）为所有四顶点图（除了路径图）提供了紧密的界限。（2）为一般完全图和完全二分图提供了紧密的界限。（3）为完全 k-分图提供了接近紧密的界限。（4）将原始模型中包含一个三角形和一个附加边的图的界限差距缩小。

Conclusion: 本文分析了流模型下多处理器系统中的在线缓冲区最小化问题，特别关注了四顶点图、完全图、完全二分图以及完全 k-分图，并为这些图提供了紧密或接近紧密的界限。同时，对原始模型中某个特定图的界限也进行了收紧。研究结果提供了关于在线调度算法在不同冲突图结构下的性能保证。

Abstract: We consider the online buffer minimization in multiprocessor systems with conflicts problem (in short, the buffer minimization problem) in the recently introduced flow model. In an online fashion, workloads arrive on some of the $n$ processors and are stored in an input buffer. Processors can run and reduce these workloads, but conflicts between pairs of processors restrict simultaneous task execution. Conflicts are represented by a graph, where vertices correspond to processors and edges indicate conflicting pairs. An online algorithm must decide which processors are run at a time; so provide a valid schedule respecting the conflict constraints.
  The objective is to minimize the maximal workload observed across all processors during the schedule. Unlike the original model, where workloads arrive as discrete blocks at specific time points, the flow model assumes workloads arrive continuously over intervals or not at all. We present tight bounds for all graphs with four vertices (except the path, which has been solved previously) and for the families of general complete graphs and complete bipartite graphs. We also recover almost tight bounds for complete $k$-partite graphs.
  For the original model, we narrow the gap for the graph consisting of a triangle and an additional edge to a fourth vertex.

</details>


### [2] [Greedy Algorithms for Shortcut Sets and Hopsets](https://arxiv.org/abs/2511.20111)
*Ben Bals,Joakim Blikstad,Greg Bodwin,Daniel Dadush,Sebastian Forster,Yasamin Nazari*

Main category: cs.DS

TL;DR: 与DSL或图处理或MLIR或编译器或HLS相关吗：本文与图处理相关。
Too long; didn't read: 本文探索了用于跳跃集和快捷集的简单贪婪算法的性能。在快捷集方面，贪婪算法的规模界限与现有最佳结果相匹配；在精确跳跃集方面，其结果规模在存在性意义上是近乎最优的。尽管贪婪算法在概念上简单，但速度较慢。因此，作者转向研究基于传递闭包路径上集合覆盖近似算法的更快确定性算法，最终推出了一个能在 $O(mn^{2/3})$ 时间计算出 $O(n^{1/3})$ 跳跃界的 $\tilde{O}(n)$ 规模快捷集的确定性算法。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索贪婪算法在计算跳跃集（Hopsets）和快捷集（Shortcut Sets）方面的能力，目的是提出简单而有效的算法，并分析其在算法规模和计算速度方面的性能。

Method: 本文提出了简单的贪婪算法来计算跳跃集和快捷集。对于快捷集，贪婪算法得到的规模上界为 $|H| \le \tilde{O}(n^2/\beta^3 + n^{3/2}/\beta^{3/2})$。对于精确跳跃集，输出集合的大小在技术假设下是存在性最优的。同时，本文还提出了一种基于传递闭包路径的集合覆盖近似算法的更快确定性算法。

Result: 本文有两个主要结果：
1. **贪婪算法的性能**：
    * **快捷集**：算法得到的规模上界 $|H| \le \tilde{O}(n^2/\beta^3 + n^{3/2}/\beta^{3/2})$，与现有最佳上界相匹配。
    * **精确跳跃集**：在特定技术假设下，输出集合的大小是存在性最优的（最多相差亚多项式因子）。
2. **更快确定性算法**：一个基于贪婪集合覆盖近似的确定性算法，能在 $O(mn^{2/3})$ 时间内计算出大小为 $\tilde{O}(n)$、跳跃界为 $O(n^{1/3})$ 的快捷集。

Conclusion: 本文探索了用于跳跃集和快捷集的贪婪算法的性能，并提出了一种基于传递闭包路径上的集合覆盖近似算法的更快确定性方法。最后一部分结果是基于更快确定性算法，且该算法采用了传递闭包路径上的某个贪心集合覆盖近似算法。一种推论是得到一个确定性算法，能在 $O(mn^{2/3})$ 时间内计算出大小为 $\tilde{O}(n)$、跳跃界为 $O(n^{1/3})$ 的快捷集。

Abstract: We explore the power of greedy algorithms for hopsets and shortcut sets. In particular, we propose simple greedy algorithms that, given an input graph $G$ and a parameter $β$, compute a shortcut set or an exact hopset $H$ of hopbound at most $β$, and we prove the following guarantees about the size $|H|$ of the output:
  For shortcut sets, we prove the bound $$|H| \le \tilde{O}\left( \frac{n^2}{β^3} + \frac{n^{3/2}}{β^{3/2}} \right).$$ This matches the current state-of-the-art upper bound by Kogan and Parter [SODA '22].
  For exact hopsets of $n$-node, $m$-edge weighted graphs, the size of the output hopset is existentially optimal up to subpolynomial factors, under some technical assumptions.
  Despite their simplicity and conceptual implications, these greedy algorithms are slower than existing sampling-based approaches. Our second set of results focus on faster deterministic algorithms that are based on a certain greedy set cover approximation algorithm on paths in the transitive closure. One consequence is a deterministic algorithm that takes $O(mn^{2/3})$ time to compute a shortcut set of size $\tilde{O}(n)$ and hopbound $O(n^{1/3})$.

</details>


### [3] [Robust Algorithms for Finding Cliques in Random Intersection Graphs via Sum-of-Squares](https://arxiv.org/abs/2511.20376)
*Andreas Göbel,Janosch Ruff,Leon Schiller*

Main category: cs.DS

TL;DR: 部分相关（Graph Processing）：本文研究的是稠密随机交集图（RIGs）上的团恢复算法，这属于图处理和社区发现的范畴。

太长不看（TLDR）：本文针对一个拓扑结构更复杂、现有简单方法失效的领域——稠密随机交集图（RIGs）中重叠团的恢复问题，首次提出了高效的算法。该算法能够进行精确和近似恢复，对各种噪声和损坏具有鲁棒性，并在团大小 $k \gg \sqrt{n \log(n)}$ 时有效。提出的方法基于平方和（SoS）层次的“证明到算法”框架。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中在恢复随机图中的单个或多个不相交的植入团，而恢复重叠植入团这一自然扩展问题却在很大程度上未被探索。在稠密随机交集图（RIGs）模型中，每个顶点可以属于多项式数量的团，这使得任务比不相交团恢复困难得多，并导致简单的组合和谱算法失效。因此，本文的动机是为稠密随机交集图中的重叠团恢复设计首批高效算法。

Method: 本文提出了针对稠密随机交集图（RIGs）中重叠团恢复问题的首批高效算法。这些算法利用了平方和（Sum-of-Squares, SoS）层次的“证明到算法”（proofs-to-algorithms）框架来解决团恢复问题，实现了精确恢复和近似恢复。算法设计同时考虑了对噪声、单调对抗者以及一定数量的边损坏的鲁棒性。

Result: 本文获得了首批针对稠密随机交集图（RIGs）社区结构恢复的高效算法，囊括了精确恢复和近似恢复。这些算法在理论上对噪声、单调对抗者以及一定数量的边损坏具有鲁棒性。算法在团大小 $k \gg \sqrt{n \log(n)}$ 的条件下有效。

Conclusion: 本文首次提出了针对稠密随机交集图（RIGs）中重叠团恢复的高效算法，该算法在理论上被证明在精确和近似恢复方面均有效，且对噪声、对抗、边损坏等具有鲁棒性。本文的方法基于利用平方和（Sum-of-Squares, SoS）层次的“证明到算法”框架，有望推广到其他重叠社区植入模型，进一步推动对重叠、非高斯社区发现问题的研究。

Abstract: We study efficient algorithms for recovering cliques in dense random intersection graphs (RIGs). In this model, $d = n^{Ω(1)}$ cliques of size approximately $k$ are randomly planted by choosing the vertices to participate in each clique independently with probability $δ$. While there has been extensive work on recovering one, or multiple disjointly planted cliques in random graphs, the natural extension of this question to recovering overlapping cliques has been, surprisingly, largely unexplored. Moreover, because every vertex can be part of polynomially many cliques, this task is significantly harder than in case of disjointly planted cliques (as recently studied by Kothari, Vempala, Wein and Xu [COLT'23]) and manifests in the failure of simple combinatorial and even spectral algorithms.
  In this work we obtain the first efficient algorithms for recovering the community structure of RIGs both from the perspective of exact and approximate recovery. Our algorithms are further robust to noise, monotone adversaries, a certain, optimal number of edge corruptions, and work whenever $k \gg \sqrt{n \log(n)}$. Our techniques follow the proofs-to-algorithms framework utilizing the sum-of-squares hierarchy.

</details>


### [4] [Counting large patterns in degenerate graphs](https://arxiv.org/abs/2511.20385)
*Christine Awofeso,Patrick Greaves,Oded Lachish,Felix Reidl*

Main category: cs.DS

TL;DR: This paper is related to **graph processing**. The paper addresses the problem of subgraph counting, which involves finding the number of occurrences of a pattern graph $H$ in a host graph $G$. This is a core problem in graph processing. **tldr:** Subgraph counting is $\#\text{W}[1]$-hard. Curticapean and Marx gave an FPT upper bound based on the vertex cover number $\tau$ of the pattern graph $H$. This paper improves this bound for $d$-degenerate host graphs $G$ and a family of pattern graphs $H$ (including all bicliques) that are $(c,d)$-locatable. The key improvement is that the running time's dependency on the size of $H$ becomes polynomial, rather than exponential. For the special case of $(1,d)$-locatable graphs $H$, the algorithm achieves linear running time dependency on $|G|$. Furthermore, a lower bound is established, showing that counting slightly non-$(1,d)$-locatable graphs is already $\#\text{W}[1]$-hard. The paper highlights that, despite progress in counting substructures in degenerate graphs, existing general results still have an exponential dependency on the pattern graph size.


<details>
  <summary>Details</summary>
Motivation: 子图计数问题是计算领域的经典挑战，它在模式图 $H$ 具有简单结构时，仍然是 $\#\text{W}[1]$ 难的。Curticapean 和 Marx 的工作给出了基于 $H$ 的点覆盖数 $\tau$ 的上界 $O(|H|^{2^{O(τ)}} |G|^{τ+ O(1)})$。先前的工作表明，如果将宿主图 $G$ 限制在特定的图族中（例如 $d$-退化图），这个上界可能会得到改进。特别地，Eppstein 的工作证实了这一点。本文的动机在于，探究是否可以将 Curticapean 和 Marx 的上界推广到更广泛的模式图 $H$（包括所有双重团），并在 $G$ 限于 $d$-退化图时，将运行时间对 $|H|$ 的依赖性降至多项式级别，从而在退化图上的子结构计数方面寻找更精细的界限。

Method: 作者通过引入 $(c,d)$-可定位图 $H$ 的概念，在输入图 $G$ 受限于 $d$-退化图时，改进了 Curticapean 和 Marx 关于子图计数问题的 FPT 算法的运行时间。该方法的核心在于利用 $d$-退化图的特性和可定位图的结构，将运行时间对 $|H|$ 的依赖性从指数级降低到多项式级。对于 $(1,d)$-可定位图，算法达到了对 $|G|$ 的线性运行时间。此外，作者还通过证明下界，来刻画这一改进的界限。

Result: 当输入图 $G$ 限制为 $d$-退化图时，本文提出改进的子图计数算法，适用于一类满足 $(c,d)$-可定位属性的模式图 $H$，该图族包含了所有的双重团。与 Curticapean 和 Marx 的上界相比，新算法的关键在于运行时间对模式图 $|H|$ 的大小仅具有多项式依赖性。本文进一步刻画了 $(1,d)$-可定位图，对于这些图，算法实现了对 $|G|$ 的线性运行时间。此外，本文还建立了一个下界，证明了对于略微不满足 $(1,d)$-可定位性质的图，计数问题已经是 $\#\text{W}[1]$ 难的。

Conclusion: 本文将 Curticapean 和 Marx 的上界推广到了更广的模式图 $H$ 族（包括所有的双重团，满足 $(c,d)$-可定位属性），并在 $G$ 限制为 $d$-退化图的情况下，将算法的运行时间对 $|H|$ 的依赖性降低到多项式级别。此外，本文还给出了 $(1,d)$-可定位图 $H$ 的更精确的线性时间算法，并证明了对于略微不满足 $(1,d)$-可定位性质的图，子图计数问题仍然是 $\#\text{W}[1]$ 难的，从而确立了新的复杂性界限。文章还指出，尽管在退化图中子结构计数的研究取得了显著进展，但现有的通用结果在模式图大小上仍存在指数依赖性，这为未来的研究留下了挑战。

Abstract: The problem of subgraph counting asks for the number of occurrences of a pattern graph $H$ as a subgraph of a host graph $G$ and is known to be computationally challenging: it is $\#W[1]$-hard even when $H$ is restricted to simple structures such as cliques or paths. Curticapean and Marx (FOCS'14) show that if the graph $H$ has vertex cover number $τ$, subgraph counting has time complexity $O(|H|^{2^{O(τ)}} |G|^{τ+ O(1)})$. This raises the question of whether this upper bound can be improved for input graphs $G$ from a restricted family of graphs. Earlier work by Eppstein~(IPL'94) shows that this is indeed possible, by proving that when $G$ is a $d$-degenerate graph and $H$ is a biclique of arbitrary size, subgraph counting has time complexity $O(d 3^{d/3} |G|)$. We show that if the input is restricted to $d$-degenerate graphs, the upper bound of Curticapean and Marx can be improved for a family of graphs $H$ that includes all bicliques and satisfies a property we call $(c,d)$-locatable. Importantly, our algorithm's running time only has a polynomial dependence on the size of~$H$. A key feature of $(c,d)$-locatable graphs $H$ is that they admit a vertex cover of size at most $cd$. We further characterize $(1,d)$-locatable graphs, for which our algorithms achieve a linear running time dependence on $|G|$, and we establish a lower bound showing that counting graphs which are barely not $(1,d)$-locatable is already $\#\text{W}[1]$-hard. We note that the restriction to $d$-degenerate graphs has been a fruitful line of research leading to two very general results (FOCS'21, SODA'25) and this creates the impression that we largely understand the complexity of counting substructures in degenerate graphs. However, all aforementioned results have an exponential dependency on the size of the pattern graph $H$.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [5] [Mechanizing a Proof-Relevant Logical Relation for Timed Message-Passing Protocols](https://arxiv.org/abs/2511.19521)
*Tesla Zhang,Asher Kornfeld,Rui Li,Sonya Simkin,Yue Yao,Stephanie Balzer*

Main category: cs.PL

TL;DR: 关联领域：编译器领域（Specifically, theorem prover, which is a tool for formal verification）。
Too long; didn't read: 语义类型被用于验证 timed message-passing 协议，但 Yao 等人的形式化工作缺乏机械化。本文在 Rocq 定理证明器中成功地机械化了该系统，包括逻辑关系和可计算轨迹的代数，克服了内涵式类型理论中轨迹等式支持有限的挑战，为该验证工具提供了机器证明和可扩展的基础。


<details>
  <summary>Details</summary>
Motivation: 语义类型（Semantic typing）是程序验证的有力工具，尤其是在 Yao 等人将其应用于验证 timed message-passing 协议（常见于 IoT 和实时系统）后。这些应用具有异构性和对时间约束（Timing）的固有需求。尽管 Yao 等人的工作提供了带有时序谓词和可计算轨迹的逻辑关系形式化，但缺乏机械化验证。机械化不仅能提供机器证明，还能促进未来扩展和应用的可扩展性。因此，本文的动机是解决该系统的机械化挑战。

Method: 本文通过在 Rocq 定理证明器中对姚等人提出的 timed message-passing 协议的语义类型系统进行机械化，解决了缺乏机械化的问题。机械化的内容包括逻辑关系、可计算轨迹的代数及其支撑引理，以及逻辑关系的基本定理。它处理了轨迹的交错、划分和连接，克服了内涵式类型理论中对于轨迹等式支持不足的挑战。

Result: 本文成功地在 Rocq 定理证明器中对 Yao 等人提出的 timed message-passing 协议的语义类型系统进行了机械化：
1. 实现了逻辑关系、可计算轨迹的代数及其支撑引理，以及逻辑关系的基本定理。
2. 克服了基于内涵式类型理论的证明助手对轨迹等式的支持有限的挑战。
3. 为原系统的形式化提供了机器证明，并为未来的扩展和应用奠定了可扩展的基础。

Conclusion: 本文成功地在 Rocq 定理证明器中对 Yao 等人提出的 timed message-passing 协议的语义类型系统进行了机械化。这包括了逻辑关系、可计算轨迹的代数及其支撑引理，以及逻辑关系的基本定理。机械化克服了内涵式类型理论中对轨迹等式支持有限的挑战，通过机械证明验证了原系统的正确性，为系统未来的扩展和应用提供了可扩展的基础。

Abstract: Semantic typing has become a powerful tool for program verification, applying the technique of logical relations as not only a proof method, but also a device for prescribing program behavior. In recent work, Yao et al. scaled semantic typing to the verification of timed message-passing protocols, which are prevalent in, e.g., IoT and real-time systems applications. The appeal of semantic typing in this context is precisely because of its ability to support typed and untyped program components alike -- including physical objects -- which caters to the heterogeneity of these applications. Another demand inherent to these applications is timing: constraining the time or time window within which a message exchange must happen. Yao et al. equipped their logical relation not only with temporal predicates, but also with computable trajectories, to supply the evidence that an inhabitant can step from one time point to another one. While Yao et al. provide the formalization for such a verification tool, it lacks a mechanization. Mechanizing the system would not only provide a machine proof for it, but also facilitate scalability for future extensions and applications.
  This paper tackles the challenge of mechanizing the resulting proof-relevant logical relation in a proof assistant. allowing trajectories to be interleaved, partitioned, and concatenated, while the intended equality on trajectories is the equality of their graphs when seen as processes indexed by time. Unfortunately, proof assistants based on intensional type theory only have modest support for such equations, forcing a prolific use of transports. This paper reports on the process of mechanizing Yao et al.'s results, comprising the logical relation, the algebra of computable trajectories with supporting lemmas, and the fundamental theorem of the logical relation, in the Rocq theorem prover.

</details>


### [6] [Understanding Accelerator Compilers via Performance Profiling](https://arxiv.org/abs/2511.19764)
*Ayaka Yorihiro,Griffin Berlstein,Pedro Pontes García,Kevin Laeufer,Adrian Sampson*

Main category: cs.PL

TL;DR: 是的，这篇论文与 DSL (加速器设计语言 ADL)、编译器、HLS (通过 ADL 编译到硬件)、和图处理（未直接提及，但加速器设计通常涉及数据流图或控制流图，虽然重点在控制流分析）相关。加速器设计语言（ADL）编译器由于复杂的调度和优化，存在固有的性能不可预测性。本文介绍了 Petal，一个针对 Calyx 中间语言的周期级性能分析工具，它通过植入探针、分析 RTL 仿真轨迹并将其映射回高级控制结构，帮助 ADL 程序员理解编译器决策对性能的影响。Petal 能够识别性能瓶颈并指导手动优化，在一个案例中实现了 46.9% 的总周期数减少。


<details>
  <summary>Details</summary>
Motivation: 加速器设计语言（ADL）编译器虽然有助于快速设计高效的专用硬件，但它们在高级语义和周期级调度之间存在巨大的语义鸿沟，并且通常依赖复杂的启发式算法进行电路优化，导致性能难以控制和预测。开发人员需要进行猜测才能找到并解决生成硬件中的性能问题。本文认为 ADL 编译器永远无法完美，一些性能不可预测性是固有的。因此，开发人员需要工具来理解编译器决策对性能的影响。

Method: Petal 的方法如下：1. 它针对 Calyx 中间语言（IL）设计。2. 它通过在 Calyx 代码中植入探针（probes）来对代码进行检测（instruments）。3. 它分析寄存器传输级（RTL）仿真产生的轨迹（trace）。4. 它将轨迹中的事件映射回 Calyx 代码中的高级控制结构，以跟踪每个构造活跃的周期数，从而生成周期级性能分析（cycle-level profiles）。

Result: 本文提出了 Petal，一个针对 Calyx 中间语言（IL）的周期级性能分析工具（cycle-level profiler）。通过案例研究，证明了 Petal 的周期级性能分析能够识别现有加速器设计中的性能瓶颈（performance problems）。这些洞察可以指导开发人员进行编译器无法自动执行的优化，例如在一个应用中将总周期数减少了 46.9%。

Conclusion: 本文认为，ADL 编译器存在固有的性能不可预测性。作为替代方案，本文提出了 Petal 工具，该工具通过跟踪 RTL 仿真并映射回 Calyx 程序的控制结构，为 ADL 程序员提供对编译器决策如何影响性能的深入理解。通过案例研究，Petal 能够识别现有加速器设计中的性能问题，并指导开发人员进行优化，从而显著提高性能，例如在一个应用中将总周期数减少了 46.9%。

Abstract: Accelerator design languages (ADLs), high-level languages that compile to hardware units, help domain experts quickly design efficient application-specific hardware. ADL compilers optimize datapaths and convert software-like control flow constructs into control paths. Such compilers are necessarily complex and often unpredictable: they must bridge the wide semantic gap between high-level semantics and cycle-level schedules, and they typically rely on advanced heuristics to optimize circuits. The resulting performance can be difficult to control, requiring guesswork to find and resolve performance problems in the generated hardware. We conjecture that ADL compilers will never be perfect: some performance unpredictability is endemic to the problem they solve.
  In lieu of compiler perfection, we argue for compiler understanding tools that give ADL programmers insight into how the compiler's decisions affect performance. We introduce Petal, a cycle-level Petal for the Calyx intermediate language (IL). Petal instruments the Calyx code with probes and then analyzes the trace from a register-transfer-level simulation. It maps the events in the trace back to high-level control constructs in the Calyx code to track the clock cycles when each construct was active. Using case studies, we demonstrate that Petal's cycle-level profiles can identify performance problems in existing accelerator designs. We show that these insights can also guide developers toward optimizations that the compiler was unable to perform automatically, including a reduction by 46.9\% of total cycles for one application.

</details>


### [7] [The Ghosts of Empires: Extracting Modularity from Interleaving-Based Proofs (Extended Version)](https://arxiv.org/abs/2511.20369)
*Frank Schüssele,Matthias Zumkeller,Miriam Lagunes-Rochin,Dominik Klumpp*

Main category: cs.PL

TL;DR: 该论文与 DSL、图处理、MLIR、编译器或 HLS **均不相关**。这篇论文提出了一种将并发程序验证器生成的基于交错的正确性证明转换为 Owicki-Gries 风格的线程模块化证明的方法。通过自动合成幽灵变量来捕获关键的交错信息并抽象不相关细节，该方法旨在生成紧凑的正确性证书，以便独立高效地验证验证器的结果，从而帮助揭示软件验证器中的实现错误。评估显示该方法高效且能生成紧凑的证明。


<details>
  <summary>Details</summary>
Motivation: 现有的算法软件验证器可能存在实现错误，威胁其正确性（soundness）。生成正确程序（correct programs）的正确性证书（correctness certificates）可以实现对验证结果的独立高效验证，这有助于发现验证器中的错误。然而，自动为并发程序生成小巧且紧凑的正确性证明是一项挑战，因为正确性论证可能依赖于特定的线程交错（interleaving），这可能导致指数级爆炸。

Method: 提出了一种将基于交错的正确性证明转换为 Owicki-Gries 风格线程模块化正确性证明的方法。该方法涉及自动合成幽灵变量（ghost variables）来捕获相关的交错信息，并抽象化不相关的细节。

Result: 评估结果表明，与基线相比，该方法在实践中是高效的，并且能够生成紧凑的证明。

Conclusion: 本方法将基于交错的正确性证明转换为 Owicki-Gries 风格的线程模块化证明，证明中包含了自动合成的幽灵变量，这些变量捕获了相关的交错信息并抽象了不相关的细节。这一转换方法能够有效生成紧凑的正确性证书，用于独立验证并发程序软件验证器的结果，从而提高验证器的可靠性和有效性。

Abstract: Implementation bugs threaten the soundness of algorithmic software verifiers. Generating correctness certificates for correct programs allows for efficient independent validation of verification results, and thus helps to reveal such bugs. Automatic generation of small, compact correctness proofs for concurrent programs is challenging, as the correctness arguments may depend on the particular interleaving, which can lead to exponential explosion. We present an approach that converts an interleaving-based correctness proof, as generated by many algorithmic verifiers, into a thread-modular correctness proof in the style of Owicki and Gries. We automatically synthesize ghost variables that capture the relevant interleaving information, and abstract away irrelevant details. Our evaluation shows that the approach is efficient in practice and generates compact proofs, compared to a baseline.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [CAMformer: Associative Memory is All You Need](https://arxiv.org/abs/2511.19740)
*Tergel Molom-Ochir,Benjamin F. Morris,Mark Horton,Chiyue Wei,Cong Guo,Brady Taylor,Peter Liu,Shan X. Wang,Deliang Fan,Hai Helen Li,Yiran Chen*

Main category: cs.AR

TL;DR: 与DSL或图处理或MLIR或编译器或HLS无关。
CAMformer是一种新型的Transformer加速器，通过将注意力机制重新解释为一种关联存储器操作，并在一种称为电压域二元注意力内容可寻址存储器（BA-CAM）的硬件中计算注意力分数。这种方法通过模拟电荷共享实现恒定时间（Constant-time）的相似性搜索，取代了传统的数字算术运算。CAMformer还结合了分层Top-k过滤和流水线执行等优化。实验证明，CAMformer在BERT和Vision Transformer上实现了超过10倍的能效提升、高达4倍的吞吐量，以及6-8倍的面积降低，同时保持了接近无损的准确性。


<details>
  <summary>Details</summary>
Motivation: Transformer模型由于其注意力机制中的二次方计算成本（Quadratic cost），特别是查询（Queries）和键（Keys）之间密集的相似性计算，在可扩展性方面面临着严峻的挑战。作者旨在设计一个新型加速器，以解决这一效率瓶颈，并实现高能效、高吞吐量和低面积占用。

Method: CAMformer的关键方法是将注意力机制重新解释为关联存储器（Associative Memory）操作，并使用电压域二元注意力内容可寻址存储器（BA-CAM）来计算注意力分数。
主要的创新点包括：
1. **模拟计算（Analog Computation）**：利用模拟电荷共享实现恒定时间（Constant-time）的相似性搜索，从而取代了传统的数字算术运算。
2. **硬件架构优化**：集成了分层两阶段（Hierarchical Two-stage）Top-k 过滤、流水线式执行（Pipelined execution）和高精度上下文处理（High-precision contextualization）。

Result: CAMformer在BERT和Vision Transformer工作负载上进行了评估，结果表明：
1. **能效（Energy Efficiency）**：相比于现有的先进加速器，能效提升超过10倍。
2. **吞吐量（Throughput）**：吞吐量提升高达4倍。
3. **面积（Area）**：面积降低6-8倍。
4. **准确性（Accuracy）**：在实现上述效率提升的同时，保持了接近无损的准确性（near-lossless accuracy）。

Conclusion: CAMformer通过将注意力机制重新解释为关联存储器操作，并在电压域二元注意力内容可寻址存储器（BA-CAM）中计算注意力分数，实现了恒定时间相似性搜索。这种方法取代了数字算术运算，通过物理相似性感应进行计算，带来了显著的能效、吞吐量和面积优势，同时保持了接近无损的准确性。CAMformer是为解决Transformer模型中注意力机制的二次方计算成本和可扩展性挑战而设计的高效、高精度加速器。

Abstract: Transformers face scalability challenges due to the quadratic cost of attention, which involves dense similarity computations between queries and keys. We propose CAMformer, a novel accelerator that reinterprets attention as an associative memory operation and computes attention scores using a voltage-domain Binary Attention Content Addressable Memory (BA-CAM). This enables constant-time similarity search through analog charge sharing, replacing digital arithmetic with physical similarity sensing. CAMformer integrates hierarchical two-stage top-k filtering, pipelined execution, and high-precision contextualization to achieve both algorithmic accuracy and architectural efficiency. Evaluated on BERT and Vision Transformer workloads, CAMformer achieves over 10x energy efficiency, up to 4x higher throughput, and 6-8x lower area compared to state-of-the-art accelerators--while maintaining near-lossless accuracy.

</details>


### [9] [Pickle Prefetcher: Programmable and Scalable Last-Level Cache Prefetcher](https://arxiv.org/abs/2511.19973)
*Hoa Nguyen,Pongstorn Maidee,Jason Lowe-Power,Alireza Kaviani*

Main category: cs.AR

TL;DR: 该论文与图处理（在评估中使用图应用，特别是BFS）、编译器（通过软件可编程接口定义预取策略，隐含有编译或运行时支持）相关。
现代大型 LLC 对于不规则内存访问模式效率低下，而现有预取器难以应对。Pickle Prefetcher 是一种可编程、可扩展的 LLC 预取器，它允许软件定义预取策略，避免了复杂的硬件预测逻辑，专注于调度和请求。通过在 gem5 全系统模拟中的广泛评估，Pickle Prefetcher 在图应用（如 BFS）上，相比基线系统实现了高达 1.74 倍的加速比，并与其他预取器结合时也带来了显著的性能提升，证明了软件指导预取在处理不规则内存访问模式上的优越性。


<details>
  <summary>Details</summary>
Motivation: 现代高性能架构中的大型末级缓存（LLC）虽然有助于具有高局部性的工作负载，但对于具有不规则内存访问模式的工作负载，可能会增加延迟。现有的基于预测的预取器在处理不规则内存访问模式时效率低下，而这种模式在现代应用中尤为普遍。因此，需要一种能够有效处理独立不规则内存访问模式的预取器。本文的动机是设计一个能够将预取策略的定义权交给软件，从而更好地适应这些复杂访问模式的LLC预取器。

Method: 本文介绍了一种名为Pickle Prefetcher的可编程且可扩展的末级缓存（LLC）预取器。该预取器放弃了传统的静态启发式或复杂的预测算法，而是允许软件通过一个简单的编程接口来定义预取策略，而无需扩展指令集架构（ISA）。这种方法将硬件预测的逻辑复杂性转化为软件的可编程性，使预取器能够适应广泛的内存访问模式，并将硬件资源集中于调度和及时发出预取请求。作者通过在 gem5 全系统模拟中对 Pickle Prefetcher 进行广泛评估，并将其性能与传统预取技术进行比较，特别关注在图应用等具有不规则访问模式的工作负载上的表现。

Result: 通过在 gem5 全系统模拟中进行的广泛评估，Pickle Prefetcher 相比传统预取技术展现出显著的性能优势。在 GAPBS 广度优先搜索（BFS）实现上，Pickle Prefetcher 实现了高达 1.74 倍的加速比。当与私有缓存预取器结合使用时，相比于仅使用私有缓存预取器的系统，Pickle Prefetcher 带来了高达 1.40 倍的加速比。这些结果证明了 Pickle Prefetcher 在处理不规则内存访问模式方面的有效性。

Conclusion: Pickle Prefetcher通过交易硬件预测的逻辑复杂性以换取软件可编程性，从而能够有效地适应各种内存访问模式。它在全系统模拟评估中表现出显著的性能提升，尤其是在图应用等具有不规则但可预测访问模式的工作负载上，证明了软件指导的预取策略在处理不规则内存访问模式方面的优越性和潜力。

Abstract: Modern high-performance architectures employ large last-level caches (LLCs). While large LLCs can reduce average memory access latency for workloads with a high degree of locality, they can also increase latency for workloads with irregular memory access patterns. Prefetchers are widely used to reduce memory latency by prefetching data into the cache hierarchy before it is accessed by the core. However, existing prediction-based prefetchers often struggle with irregular memory access patterns, which are especially prevalent in modern applications. This paper introduces the Pickle Prefetcher, a programmable and scalable LLC prefetcher designed to handle independent irregular memory access patterns effectively. Instead of relying on static heuristics or complex prediction algorithms, Pickle Prefetcher allows software to define its own prefetching strategies using a simple programming interface without expanding the instruction set architecture (ISA). By trading the logic complexity of hardware prediction for software programmability, Pickle Prefetcher can adapt to a wide range of access patterns without requiring extensive hardware resources for prediction. This allows the prefetcher to dedicate its resources to scheduling and issuing timely prefetch requests. Graph applications are an example where the memory access pattern is irregular but easily predictable by software. Through extensive evaluations of the Pickle Prefetcher on gem5 full-system simulations, we demonstrate tha Pickle Prefetcher significantly outperforms traditional prefetching techniques. Our results show that Pickle Prefetcher achieves speedups of up to 1.74x on the GAPBS breadth-first search (BFS) implementation over a baseline system. When combined with private cache prefetchers, Pickle Prefetcher provides up to a 1.40x speedup over systems using only private cache prefetchers.

</details>


### [10] [R3A: Reliable RTL Repair Framework with Multi-Agent Fault Localization and Stochastic Tree-of-Thoughts Patch Generation](https://arxiv.org/abs/2511.20090)
*Zizhang Luo,Fan Cui,Kexing Zhou,Runlin Guo,Mile Xia,Hongyuan Hou,Yun Lian*

Main category: cs.AR

TL;DR: 相关领域：编译器、DSL。
太长不看版：RTL 错误修复对于硬件设计和验证至关重要。传统的自动程序修复（APR）方法依赖固定的模板，效果有限。尽管大型语言模型（LLM）在理解代码语义方面具有潜力，但其固有随机性和长上下文导致结果不可靠。本文提出了 R3A，一个基于 LLM 的自动 RTL 程序修复框架，以提高可靠性。R3A 引入了随机思想树（Stochastic Tree-Of-Thoughts）方法，通过启发式函数采样来平衡探索和利用，并提出了多智能体故障定位方法来确定补丁生成的起点。实验证明，R3A 能修复 90.6% 的 RTL-repair 错误，比传统方法提升 45%，并实现了 86.7% 的平均 pass@5 成功率，展示了高可靠性和优越性。


<details>
  <summary>Details</summary>
Motivation: 传统的自动程序修复（APR）方法依赖固定的模板，只能处理有限的错误。探索使用理解代码语义的大型语言模型（LLM）进行 RTL 修复，但它们又受限于固有随机性以及 RTL 代码和波形的长输入上下文，导致结果不可靠。因此，迫切需要一种提高 LLM 在 RTL 修复中可靠性的方法。

Method: R3A 提出了一种随机思想树（Stochastic Tree-Of-Thoughts）方法，用于控制补丁生成智能体，以探索经验证的解决方案。该算法根据启发式函数对搜索状态进行采样，以平衡探索与利用，从而获得可靠的结果。此外，R3A 还提出了一种多智能体故障定位方法，以找到故障候选点作为补丁生成智能体的起点，进一步提高了可靠性。

Result: R3A 可以在给定的时间限制内修复 RTL-repair 数据集中 90.6% 的错误，比传统方法和其他基于 LLM 的方法多覆盖 45% 的错误，同时平均实现 86.7% 的 pass@5 成功率，显示出高可靠性。

Conclusion: R3A 通过提出随机思想树（Stochastic Tree-Of-Thoughts）方法和多智能体故障定位方法，显著提高了基于 LLM 的 RTL 错误自动修复的可靠性和修复率。实验证明，R3A 在 RTL-repair 数据集上的修复率（90.6%）和 pass@5 成功率（86.7%）均优于传统方法和其他基于 LLM 的方法。

Abstract: Repairing RTL bugs is crucial for hardware design and verification. Traditional automatic program repair (APR) methods define dedicated search spaces to locate and fix bugs with program synthesis. However, they heavily rely on fixed templates and can only deal with limited bugs. As an alternative, Large Language Models with the ability to understand code semantics can be explored for RTL repair. However, they suffer from unreliable outcomes due to inherent randomness and long input contexts of RTL code and waveform. To address these challenges, we propose R3A, an LLM-based automatic RTL program repair framework upon the basic model to improve reliability. R3A proposes the stochastic Tree-Of-Thoughts method to control a patch generation agent to explore a validated solution for the bug. The algorithm samples search states according to a heuristic function to balance between exploration and exploitation for a reliable outcome. Besides, R3A proposes a multi-agent fault localization method to find fault candidates as the starting points for the patch generation agent, further increasing the reliability. Experiments show R3A can fix 90.6% of bugs in the RTL-repair dataset within a given time limit, which covers 45% more bugs than traditional methods and other LLM-based approaches, while achieving an 86.7% pass@5 rate on average, showing a high reliability.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [11] [Asynchronous Cooperative Optimization of a Capacitated Vehicle Routing Problem Solution](https://arxiv.org/abs/2511.19445)
*Luca Accorsi,Demetrio Laganà,Federico Michelotto,Roberto Musmanno,Daniele Vigo*

Main category: cs.DC

TL;DR: This paper is not directly related to DSL, graph processing, MLIR, compiler, or HLS. It focuses on the parallel optimization of a classic operations research problem (Capacitated Vehicle Routing Problem, CVRP). The paper proposes FILO2$^x$, a parallel, shared-memory adaptation of the existing FILO2 algorithm, designed to minimize synchronization effort and avoid explicit decomposition. It achieves iteration-based parallelism by having multiple solvers concurrently and asynchronously optimize potentially unrelated local solution areas of the same underlying solution, resulting in a significant reduction in computation time while maintaining similar solution quality across a wide range of instance sizes.


<details>
  <summary>Details</summary>
Motivation: 解决容量限制车辆路径问题 (Capacitated Vehicle Routing Problem, CVRP)，旨在在最小化同步开销且无需显式分解的情况下，合作优化大实例的解。尽管单线程 FILO2 算法效率高，但希望通过更好地利用可用计算资源（并行化）来进一步缩短求解时间。

Method: 提出 FILO2$^x$，这是 FILO2 算法的单轨迹并行化版本，专为共享内存系统设计。它利用 FILO2 优化应用的局部性，允许多个可能不相关的解区域同时异步优化。通过多个求解器对同一底层解的同时优化，实现了基于迭代的并行性。

Result: 计算结果表明，与原方法相比，FILO2$^x$ 在保持相似的最终解质量的同时，可以大大缩短求解时间，适用于从数百到数十万客户规模的实例。这证明了它在提高计算资源利用率方面的巨大潜力。

Conclusion: FILO2$^x$ 通过并行化 FILO2 算法，在共享内存架构下实现了对 CVRP 问题的加速求解，显著缩短了求解时间，而最终解的质量与原算法相近，证明了其在高效利用计算资源方面的有效性。

Abstract: We propose a parallel shared-memory schema to cooperatively optimize the solution of a Capacitated Vehicle Routing Problem instance with minimal synchronization effort and without the need for an explicit decomposition. To this end, we design FILO2$^x$ as a single-trajectory parallel adaptation of the FILO2 algorithm originally proposed for extremely large-scale instances and described in Accorsi and Vigo (2024). Using the locality of the FILO2 optimization applications, in FILO2$^x$ several possibly unrelated solution areas are concurrently asynchronously optimized. The overall search trajectory emerges as an iteration-based parallelism obtained by the simultaneous optimization of the same underlying solution performed by several solvers. Despite the high efficiency exhibited by the single-threaded FILO2 algorithm, the computational results show that, by better exploiting the available computing resources, FILO2$^x$ can greatly enhance the resolution time compared to the original approach, still maintaining a similar final solution quality for instances ranging from hundreds to hundreds of thousands customers.

</details>


### [12] [AI-driven Predictive Shard Allocation for Scalable Next Generation Blockchains](https://arxiv.org/abs/2511.19450)
*M. Zeeshan Haider,Tayyaba Noreen,M. D. Assuncao,Kaiwen Zhang*

Main category: cs.DC

TL;DR: 本论文与领域相关性：图处理（通过地址聚类启发式映射，可以联想到图数据结构和处理）；编译器/HLS/MLIR/DSL（不相关）；或（不相关）。
太长不看：PSAP是一种用于区块链的动态智能分片分配协议，它结合了**时间工作负载预测**（TWF）模型和**安全约束的强化学习**（Safe-PPO）控制器，通过**预测性地**分配账户和交易**，实现**多区块提前预测和自适应分片重配置，旨在**解决现有分片分配中的工作负载倾斜和高跨分片通信问题**。实验结果表明，PSAP在吞吐量、延迟和跨分片开销上均优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有的分片技术（如静态或启发式分片分配）虽然提高了吞吐量，但常因工作负载倾斜、拥塞和过多的跨分片通信而导致可扩展性效益下降。作者旨在解决这些挑战，实现稳定且高效的负载均衡，同时维护拜占庭安全。

Method: 提出了一种动态且智能的分片分配框架——预测性分片分配协议（PSAP）。PSAP整合了时间工作量预测模型（TWF）和安全约束的强化学习（Safe-PPO）控制器。具体来说，TWF模型实现多区块提前预测，而Safe-PPO控制器则负责根据预测结果自适应地重新配置分片，以实现负载均衡。为了保证确定性推断和安全性，PSAP采用了同步量化运行时和安全门机制，限制了质押集中度、迁移消耗和利用率阈值。协议通过执行有界限的原子迁移来应对热点形成。

Result: 在包括以太坊、NEAR和Hyperledger Fabric在内的异构数据集上进行实验评估（通过地址聚类启发式映射），结果显示，PSAP相比现有动态分片基线，吞吐量提升高达2倍，延迟降低35\%，跨分片开销减少20\%。这些结果证明了PSAP在实现稳定负载均衡方面的有效性。

Conclusion: PSAP通过整合时间工作量预测模型（TWF）和安全约束的强化学习控制器（Safe-PPO），实现了前瞻性、确定性和安全感知的分片分配。实验结果证明，PSAP在吞吐量、延迟和跨分片开销方面均优于现有的动态分片基线，这表明预测性、确定性且注重安全性的分片分配是下一代可扩展区块链系统的一个有前途的方向。

Abstract: Sharding has emerged as a key technique to address blockchain scalability by partitioning the ledger into multiple shards that process transactions in parallel. Although this approach improves throughput, static or heuristic shard allocation often leads to workload skew, congestion, and excessive cross-shard communication diminishing the scalability benefits of sharding. To overcome these challenges, we propose the Predictive Shard Allocation Protocol (PSAP), a dynamic and intelligent allocation framework that proactively assigns accounts and transactions to shards based on workload forecasts. PSAP integrates a Temporal Workload Forecasting (TWF) model with a safety-constrained reinforcement learning (Safe-PPO) controller, jointly enabling multi-block-ahead prediction and adaptive shard reconfiguration. The protocol enforces deterministic inference across validators through a synchronized quantized runtime and a safety gate that limits stake concentration, migration gas, and utilization thresholds. By anticipating hotspot formation and executing bounded, atomic migrations, PSAP achieves stable load balance while preserving Byzantine safety. Experimental evaluation on heterogeneous datasets, including Ethereum, NEAR, and Hyperledger Fabric mapped via address-clustering heuristics, demonstrates up to 2x throughput improvement, 35\% lower latency, and 20\% reduced cross-shard overhead compared to existing dynamic sharding baselines. These results confirm that predictive, deterministic, and security-aware shard allocation is a promising direction for next-generation scalable blockchain systems.

</details>


### [13] [AVS: A Computational and Hierarchical Storage System for Autonomous Vehicles](https://arxiv.org/abs/2511.19453)
*Yuxin Wang,Yuankai He,Weisong Shi*

Main category: cs.DC

TL;DR: 该论文与以下领域相关：编译器（系统栈、存储栈）、图处理（可能处理图结构数据，但未明确提及）、MLIR（未提及）、HLS（未提及）、DSL（未提及）。
总结：自动驾驶汽车（AVs）每天产生海量数据，现有车载存储系统效率低下。本文提出了AVS车载存储系统，通过模态感知缩减/压缩、冷热分层和轻量级元数据索引的分层共设计计算与存储，在嵌入式硬件上验证了其能实现实时摄取、快速检索和显著的存储空间缩减。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶汽车发展成为移动计算平台，其强大的处理器和多样化的传感器每天生成海量异构数据（例如每天14 TB）。现有的车载数据记录器和存储栈无法提供高效的数据存储和检索能力，而支持新兴的第三方应用需要一个通用的、可查询的车载存储系统。因此，本文旨在设计一个能够高效管理和检索自动驾驶（AV）数据的存储系统。

Method: 本文提出了AVS（Autonomous Vehicle Storage）系统，该系统通过计算与分层布局的协同设计来解决车载存储的挑战。其具体方法包括：模态感知的数据缩减和压缩、具有日常归档功能的冷热分层、以及用于索引的轻量级元数据层。该设计通过在AV数据上进行系统级基准测试（包括SSD和HDD文件系统和嵌入式索引），并在嵌入式硬件上使用真实的L4自动驾驶跟踪数据进行了验证。

Result: AVS原型系统在嵌入式硬件上实现了可预测的实时数据摄取、快速的选择性检索，并在适度的资源预算下大幅减少了存储占用空间（即“占地面积”）。研究结果表明了将存储视为AV堆栈中一等公民组件的重要性，并提出了进一步实现更具可扩展性和更长久部署的见解和后续步骤。

Conclusion: AVS系统通过分层布局和计算与存储的协同设计，实现了对自动驾驶数据的高效存储和检索，并在嵌入式硬件上验证了其在实时摄取、快速选择性检索和大幅减少存储占用方面的有效性。未来的工作将侧重于实现更具可扩展性和更长久部署的存储解决方案。

Abstract: Autonomous vehicles (AVs) are evolving into mobile computing platforms, equipped with powerful processors and diverse sensors that generate massive heterogeneous data, for example 14 TB per day. Supporting emerging third-party applications calls for a general-purpose, queryable onboard storage system. Yet today's data loggers and storage stacks in vehicles fail to deliver efficient data storage and retrieval. This paper presents AVS, an Autonomous Vehicle Storage system that co-designs computation with a hierarchical layout: modality-aware reduction and compression, hot-cold tiering with daily archival, and a lightweight metadata layer for indexing. The design is grounded with system-level benchmarks on AV data that cover SSD and HDD filesystems and embedded indexing, and is validated on embedded hardware with real L4 autonomous driving traces. The prototype delivers predictable real-time ingest, fast selective retrieval, and substantial footprint reduction under modest resource budgets. The work also outlines observations and next steps toward more scalable and longer deployments to motivate storage as a first-class component in AV stacks.

</details>


### [14] [Optimizations on Graph-Level for Domain Specific Computations in Julia and Application to QED](https://arxiv.org/abs/2511.19456)
*Anton Reinhard,Simeon Ehrig,René Widera,Michael Bussmann,Uwe Hernandez Acosta*

Main category: cs.DC

TL;DR: 该论文与 DSL、图处理（有向无环图 DAG）、编译器（静态调度和编译代码）和 HLS（硬件调度优化）相关：1. DSL：虽然没有直接说明是 DSL，但“域特定信息”和“域特定计算”暗示了对特定问题领域的关注，可能涉及领域特定语言或概念。2. 图处理：论文明确使用了有向无环图（DAG）来表示和调度计算问题。3. 编译器：该框架能够生成“静态调度和编译的代码”，这直接涉及编译器的功能。4. HLS：描述中提到的“调度它在最适合的硬件上”、“最大限度地利用硬件”和“硬件分析”与高性能计算和硬件级优化有关。

**太长不读：** 这篇论文提出了一个基于 Julia, 使用有向无环图（DAG）来表示复杂科学计算问题的软件框架。它能自动、动态地生成静态调度和编译的代码。通过将领域特定的计算信息融入 DAG 调度中，实现了优化，从而提升了多类型硬件上的计算效率。以量子电动力学的矩阵元素计算作为案例进行了演示。


<details>
  <summary>Details</summary>
Motivation: 复杂科学计算问题通常由计算需求不同的较小子任务组成。为了实现最优效率，需要对每个子任务进行分析，并将其调度到最合适的硬件上。同时，还需要考虑并行性、子任务之间的依赖关系以及设备间的数据传输速度等因素。因此，需要一个能够使用有向无环图（DAG）来表示这些问题，并实现尽可能多地利用给定机器上硬件的软件框架。

Method: 本文提出了一种基于 Julia 的软件框架，该框架能够自动且动态地生成静态调度和编译的代码。它将特定领域的计算信息融入到 DAG 调度的现有概念中，从而实现额外的优化。该方法通过一个具体的量子电动力学散射过程矩阵元素计算的例子来阐述和实现。

Result: 该框架能够自动和动态地生成静态调度和编译的代码。通过将关于计算的特定领域信息添加到现有的 DAG 调度概念中，实现了在其他情况下不可能实现的优化。用量子电动力学中涉及多个外部粒子的散射过程的矩阵元素计算的案例证明了该理论与实现的有效性。

Conclusion: 本文提出了一个基于 Julia 的软件框架，可以自动、动态地生成静态调度和编译的代码。通过将特定领域的计算信息和现有的 DAG 调度概念相结合，实现了更优的调度和优化，为复杂科学计算提供了高效的解决方案。

Abstract: Complex computational problems in science often consist of smaller parts that can have largely distinct compute requirements from one another. For optimal efficiency, analyzing each subtask and scheduling it on the best-suited hardware would be necessary. Other considerations must be taken into account, too, such as parallelism, dependencies between different subtasks, and data transfer speeds between devices. To achieve this, directed acyclic graphs are often employed to represent these problems and enable utilizing as much hardware as possible on a given machine. In this paper, we present a software framework written in Julia capable of automatically and dynamically producing statically scheduled and compiled code. We lay theoretical foundations and add domain-specific information about the computation to the existing concepts of DAG scheduling, enabling optimizations that would otherwise be impossible. To illustrate the theory we implement an example application: the computation of matrix elements for scattering processes with many external particles in quantum electrodynamics.

</details>


### [15] [Systemic approach for modeling a generic smart grid](https://arxiv.org/abs/2511.19460)
*Sofiane Ben Amor,Guillaume Guerard,Loup-Noé Levy*

Main category: cs.DC

TL;DR: 否。论文内容涉及智能电网建模与仿真和分布式优化。这是一个关于智能电网建模与仿真、分布式优化的研究。智能电网的技术进步带来了复杂的跨学科建模和仿真难题，传统的计算方法难以解决。本文提出了一个智能电网的主干模型，通过集成建模电力系统、能源市场、需求侧管理等不同系统，并利用子系统的分布式优化来实现生产和消费调度，从而在保持灵活性和可伸缩性的同时，实现电网替代场景的测试和假设验证。


<details>
  <summary>Details</summary>
Motivation: 智能电网的技术进步带来了复杂的跨学科建模和日益困难的仿真问题，传统的计算方法难以解决。为了模拟智能电网，需要一种系统性的方法来集成建模电力系统、能源市场、需求侧管理以及其他资源和资产。其动机正是为了解决这一挑战。

Method: 本文提出了一种智能电网的主干模型，该模型通过对电力系统、能源市场、需求侧管理等不同系统进行集成建模和仿真。其核心方法是通过对子系统进行分布式优化，以实现生产和消费调度。

Result: 本文提出的智能电网主干模型能够仿真不同的系统，用以在引入大规模实物模型进行测试之前验证假设。通过对子系统进行分布式优化，该模型实现了生产和消费调度，并保持了系统的灵活性和可伸缩性。

Conclusion: 本文提出了一个智能电网主干模型，用以测试电网的替代场景。该模型通过分布式优化子系统，实现了生产和消费调度，同时保持了灵活性和可伸缩性。这一工具能够帮助在引入实物模型进行测试之前验证假设。

Abstract: Smart grid technological advances present a recent class of complex interdisciplinary modeling and increasingly difficult simulation problems to solve using traditional computational methods. To simulate a smart grid requires a systemic approach to integrated modeling of power systems, energy markets, demand-side management, and much other resources and assets that are becoming part of the current paradigm of the power grid. This paper presents a backbone model of a smart grid to test alternative scenarios for the grid. This tool simulates disparate systems to validate assumptions before the human scale model. Thanks to a distributed optimization of subsystems, the production and consumption scheduling is achieved while maintaining flexibility and scalability.

</details>


### [16] [Urban Buildings Energy Consumption Estimation Using HPC: A Case Study of Bologna](https://arxiv.org/abs/2511.19463)
*Aldo Canfora,Eleonora Bergamaschi,Riccardo Mioli,Federico Battini,Mirko Degli Esposti,Giorgio Pedrazzi,Chiara Dellacasa*

Main category: cs.DC

TL;DR: 该论文与DSL、图处理、MLIR、编译器或HLS不相关。

TLDR: 本文提出了一个城市建筑能耗建模（UBEM）流程，该流程整合了EnergyPlus能耗模拟工具、高性能计算（HPC，使用Cineca的Leonardo超级计算机）和开放地理空间数据集。研究利用Bologna的开放数据和LiDAR获取建筑几何数据，并从区域法规和TABULA数据库获取非几何属性。结果显示，该系统成功在不到30分钟内完成了对Bologna市约25,000栋建筑的能耗估算。


<details>
  <summary>Details</summary>
Motivation: 城市建筑能耗建模（UBEM）对于理解和预测城市尺度的能耗至关重要。本文的动机是建立一个高效、可靠的UBEM流程，能够整合现有的开放地理空间数据、高性能计算资源和专业的能耗模拟工具（EnergyPlus），以快速估算像Bologna这样整个城市的建筑能耗。

Method: 研究团队设计了一个完整的UBEM流程：首先，整合来自Bologna开放数据门户的建筑足迹和高度等几何信息，并利用LiDAR测量数据进行增强。其次，从区域建筑法规和欧洲TABULA数据库获取建筑材料、隔热特性和窗户性能等非几何属性。最后，利用Cineca的Leonardo超级计算机执行EnergyPlus模拟，以实现大规模、高效率的能耗估算。

Result: 研究团队成功地利用Leonardo超级计算机，在不到30分钟内完成了对Bologna市约25,000栋建筑的能耗模拟。这证明了所提出的UBEM流程在处理大规模城市数据和实现高效计算方面的有效性。

Conclusion: 本文成功展示了一个整合EnergyPlus、HPC（超级计算机）和开放地理空间数据集的城市建筑能耗建模（UBEM）流程，并在Bologna市得到了快速且大规模的应用。该流程为城市尺度的能耗分析和预测提供了高效的工具，其计算效率已在实际测试中得到验证。

Abstract: Urban Building Energy Modeling (UBEM) plays a central role in understanding and forecasting energy consumption at the city scale. In this work, we present a UBEM pipeline that integrates EnergyPlus simulations, high-performance computing (HPC), and open geospatial datasets to estimate the energy demand of buildings in Bologna, Italy. Geometric information including building footprints and heights was obtained from the Bologna Open Data portal and enhanced with aerial LiDAR measurements. Non-geometric attributes such as construction materials, insulation characteristics, and window performance were derived from regional building regulations and the European TABULA database. The computation was carried out on Leonardo, the Cineca-hosted supercomputer, enabling the simulation of approximately 25,000 buildings in under 30 minutes.

</details>


### [17] [Temperature in SLMs: Impact on Incident Categorization in On-Premises Environments](https://arxiv.org/abs/2511.19464)
*Marcio Pohlmann,Alex Severo,Gefté Almeida,Diego Kreutz,Tiago Heinrich,Lourenço Pereira*

Main category: cs.DC

TL;DR: 该论文与 DSL、图处理、MLIR、编译器、HLS 均不相关。
本文旨在探究在本地执行小型语言模型（SLMs）用于自动化安全事件分类的可行性，以避免使用基于云的 LLMs 所带来的成本、延迟和保密性风险。研究评估了 21 个参数量在 1B 到 20B 之间、运行在两种不同架构上的 SLMs，并测量了它们在不同温度设置下的执行时间与精度。结果表明，温度对性能影响甚微，而模型的参数量和 GPU 性能是关键决定性因素。研究证实，在现代硬件上使用小于 7B 参数的 SLM 进行低延迟、高准确率的事件分类是可行的。


<details>
  <summary>Details</summary>
Motivation: 面对 SOCs 和 CSIRTs 自动化事件分类的日益增长的压力，研究人员旨在探讨本地执行的小型语言模型（SLMs）是否可以应对这一挑战。因为使用基于云的大型语言模型（LLMs）存在成本、延迟和保密性风险。

Method: 研究人员评估了 21 个参数量在 1B 到 20B 之间的本地执行 SLM，并改变温度超参数。他们测量了在两种不同架构上的执行时间和精度，以评估 SLM 在事件分类中的性能。

Result: 实验结果显示，温度超参数对性能影响很小。而模型的参数量和 GPU 容量是决定性因素。结果证实了在现代硬件上使用小于 7B 参数的 SLM 进行本地事件分类是可行的，同时能提供低延迟和高准确率。

Conclusion: 在本地执行 SLM 进行事件分类是可行的，特别是对于小于 7B 参数的模型，在现代硬件上可以实现低延迟和高准确率。然而，模型的参数量和 GPU 性能是决定性因素，而温度超参数的影响微乎其微。

Abstract: SOCs and CSIRTs face increasing pressure to automate incident categorization, yet the use of cloud-based LLMs introduces costs, latency, and confidentiality risks. We investigate whether locally executed SLMs can meet this challenge. We evaluated 21 models ranging from 1B to 20B parameters, varying the temperature hyperparameter and measuring execution time and precision across two distinct architectures. The results indicate that temperature has little influence on performance, whereas the number of parameters and GPU capacity are decisive factors.

</details>


### [18] [Improved Linear-Time Construction of Minimal Dominating Set via Mobile Agents](https://arxiv.org/abs/2511.19880)
*Prabhat Kumar Chand,Anisur Rahaman Molla*

Main category: cs.DC

TL;DR: DSL or graph processing or MLIR or compiler or HLS: graph processing.
太长不看：本文在同步移动智能体模型下，针对匿名图的最小支配集（mDS）问题，提出了两种新的线性时间$O(n)$算法，每个智能体仅需$O(\log n)$位内存，优于现有最佳结果。同时，这些算法还能在$O(n)$轮内实现生成树构建和领导者选举。


<details>
  <summary>Details</summary>
Motivation: 移动智能体作为一种强大的框架，能够有效解决分布式环境下的图问题，并且其具备局部计算能力、有限内存以及在图上遍历的能力，能够提供高效的经典问题解决方案。本文的动机是聚焦于利用移动智能体在匿名图上计算最小支配集（mDS）这一核心问题，并旨在设计出比现有技术复杂度更优的算法。

Method: 本文设计了两种新的算法，用于在同步移动智能体模型下计算匿名图的最小支配集（mDS）。这些算法基于最近提出的最优分散算法，旨在实现线性时间$O(n)$的解决方案。具体来说，给定一个连通的$n$节点图，初始配置为有根或任意配置，每个智能体仅使用$O(\log n)$位的内存，并实现了$O(n)$轮内的mDS计算。该方法还能在$O(n)$轮内构建生成树并选举唯一的领导者。

Result: 本文提出的两种新算法在同步移动智能体模型下，实现了匿名图最小支配集（mDS）计算的线性时间复杂度$O(n)$解决方案，每个智能体仅需要$O(\log n)$位的内存。这比同模型下现有最佳结果有所改进。此外，作为自然副产品，这些算法还能在$O(n)$轮内构建一个生成树并选举出唯一的领导者。

Conclusion: 本文设计了两种新的算法，在同步移动智能体模型下，以线性时间复杂度$O(n)$解决了匿名图中的最小支配集（mDS）计算问题。这一结果在时间和空间复杂度上均优于现有最佳结果，并且作为副产品，这两种算法还能在$O(n)$轮内构建一个生成树并选举出唯一的领导者。这证明了移动智能体框架在解决分布式图问题上的高效性。

Abstract: Mobile agents have emerged as a powerful framework for solving fundamental graph problems in distributed settings in recent times. These agents, modelled as autonomous physical or software entities, possess local computation power, finite memory and have the ability to traverse a graph, offering efficient solutions to a range of classical problems. In this work, we focus on the problem of computing a \emph{minimal dominating set} (mDS) in anonymous graphs using mobile agents. Building on the recently proposed optimal dispersion algorithm on the synchronous mobile agent model, we design two new algorithms that achieve a \emph{linear-time} solution for this problem in the synchronous setting. Specifically, given a connected $n$-node graph with $n$ agents initially placed in either rooted or arbitrary configurations, we show that an mDS can be computed in $O(n)$ rounds using only $O(\log n)$ bits of memory per agent, without using any prior knowledge of any global parameters. This improves upon the best-known complexity results in the literature over the same model. In addition, as natural by-products of our methodology, our algorithms also construct a spanning tree and elect a unique leader in $O(n)$ rounds, which are also important results of independent interest in the mobile-agent framework.

</details>


### [19] [Towards a future space-based, highly scalable AI infrastructure system design](https://arxiv.org/abs/2511.19468)
*Blaise Agüera y Arcas,Travis Beals,Maria Biggs,Jessica V. Bloom,Thomas Fischbacher,Konstantin Gromov,Urs Köster,Rishiraj Pravahan,James Manyika*

Main category: cs.DC

TL;DR: 该论文与编译器、DSL、MLIR、图处理和HLS等领域均无关。
摘要：由于AI计算对能源的需求持续增长，本文提出并分析了一种利用太阳能在太空部署AI计算基础设施的方案。该方案使用配备太阳能电池阵列、自由空间光学链路和Google TPU（Trillium型）芯片的卫星集群。为实现高带宽、低延迟通信，卫星将采用近距离编队飞行，并提出用基于ML的模型来控制大型星座。辐射测试显示Trillium TPU能承受5年任务寿命的辐射剂量。发射成本分析表明，未来发射成本有望显著降低，使该方案在经济上具有可行性。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于应对AI计算对能源需求的持续增长问题，并探索如何最有效地利用最大的能源来源——太阳能。考虑到AI作为一种基础性通用技术，其计算和能源需求预计将继续增长，因此探索利用太空和太阳能的AI基础设施成为一个具有潜力的解决方案。

Method: 本文提出了一种在太空部署可扩展机器学习计算系统的方法，主要包括以下几个关键方面：利用太阳能作为能源；使用配备太阳能电池阵列、自由空间光学（FSO）卫星间链路和Google TPU加速器的卫星舰队；为了实现高带宽、低延迟的卫星间通信，卫星将近距离编队飞行；并提出使用基于ML的高精度模型来控制大规模星座。同时，验证了Trillium TPU的抗辐射性能和分析了发射成本的未来趋势。

Result: 研究结果表明，部署在太空中的AI计算系统是可行的：Trillium TPU芯片已经通过辐射测试，可以承受相当于5年任务寿命的总电离剂量，并表征了位翻转错误；提出了一个半径1公里的81颗卫星组成的集群编队飞行的基本方法；学习曲线分析表明，到2030年代中期，低地球轨道（LEO）的发射成本可能降至$\lesssim$\$200/kg，这将有助于降低系统的总体成本。

Conclusion: 本文分析了一个在太空部署AI计算系统的可能性和前景，该系统利用太阳能、卫星舰队和TPU芯片。虽然面临工程挑战和成本问题，但通过技术发展（如降低发射成本、提高硬件可靠性和研发AI驱动的星座控制）可以使这一愿景成为可能，并为未来AI可持续计算提供一个潜在的解决方案。

Abstract: If AI is a foundational general-purpose technology, we should anticipate that demand for AI compute -- and energy -- will continue to grow. The Sun is by far the largest energy source in our solar system, and thus it warrants consideration how future AI infrastructure could most efficiently tap into that power. This work explores a scalable compute system for machine learning in space, using fleets of satellites equipped with solar arrays, inter-satellite links using free-space optics, and Google tensor processing unit (TPU) accelerator chips. To facilitate high-bandwidth, low-latency inter-satellite communication, the satellites would be flown in close proximity. We illustrate the basic approach to formation flight via a 81-satellite cluster of 1 km radius, and describe an approach for using high-precision ML-based models to control large-scale constellations. Trillium TPUs are radiation tested. They survive a total ionizing dose equivalent to a 5 year mission life without permanent failures, and are characterized for bit-flip errors. Launch costs are a critical part of overall system cost; a learning curve analysis suggests launch to low-Earth orbit (LEO) may reach $\lesssim$\$200/kg by the mid-2030s.

</details>


### [20] [Federated Learning Framework for Scalable AI in Heterogeneous HPC and Cloud Environments](https://arxiv.org/abs/2511.19479)
*Sangam Ghimire,Paribartan Timalsina,Nirjal Bhurtel,Bishal Neupane,Bigyan Byanju Shrestha,Subarna Bhattarai,Prajwal Gaire,Jessica Thapa,Sudan Jha*

Main category: cs.DC

TL;DR: 该论文不涉及 DSL、图处理、MLIR、编译器或 HLS。
Too Long; Didn't Read 摘要: 本文提出了一个联邦学习框架，旨在高效地在混合 HPC 和云环境中运行，它解决了系统异构性、高通信开销和资源调度等挑战，同时保持了模型精度和数据隐私。实验证明，该框架在可扩展性、容错性和收敛性方面表现出色，即使在非 IID 数据和不同硬件条件下也是如此，凸显了联邦学习在构建可扩展分布式 AI 系统中的实用性。


<details>
  <summary>Details</summary>
Motivation: 随着对可扩展和注重隐私的 AI 系统的需求增长，联邦学习（FL）成为一种有前景的去中心化模型训练解决方案。然而，将高性能计算（HPC）与云基础设施相结合虽然提供了强大的计算能力，但在处理异构硬件、通信限制和非均匀数据时引入了新的复杂性。本文的动机是构建一个能够高效应对这些挑战并在混合 HPC 和云环境中运行的联邦学习框架。

Method: 作者提出了一个用于在混合 HPC 和云环境中高效运行的联邦学习框架（Federated Learning Framework）。该框架旨在解决系统异构性、通信开销高和资源调度复杂等关键挑战。论文通过在一个混合测试平台上进行了实验验证。

Result: 在混合测试平台上的实验表明，本文提出的联邦学习框架在可扩展性、容错性和收敛性方面表现出强大的性能。即使在非独立同分布（non-IID）数据分布和不同硬件条件下，系统也能保持良好的性能。实验结果凸显了联邦学习作为在现代分布式计算环境中构建可扩展人工智能系统的一种实用方法的潜力。

Conclusion: 本文提出的联邦学习框架能够有效地在混合的 HPC 和云环境中运行，解决了系统异构性、通信开销和资源调度等关键挑战，同时保持了模型精度和数据隐私。实验证明了该系统在可扩展性、容错性和收敛性方面表现出色，即使在非独立同分布（non-IID）数据分布和不同硬件条件下也能保持良好性能，突出了联邦学习在现代分布式计算环境中构建可扩展 AI 系统的潜力。

Abstract: As the demand grows for scalable and privacy-aware AI systems, Federated Learning (FL) has emerged as a promising solution, allowing decentralized model training without moving raw data. At the same time, the combination of high- performance computing (HPC) and cloud infrastructure offers vast computing power but introduces new complexities, especially when dealing with heteroge- neous hardware, communication limits, and non-uniform data. In this work, we present a federated learning framework built to run efficiently across mixed HPC and cloud environments. Our system addresses key challenges such as system het- erogeneity, communication overhead, and resource scheduling, while maintaining model accuracy and data privacy. Through experiments on a hybrid testbed, we demonstrate strong performance in terms of scalability, fault tolerance, and convergence, even under non-Independent and Identically Distributed (non-IID) data distributions and varied hardware. These results highlight the potential of federated learning as a practical approach to building scalable Artificial Intelligence (AI) systems in modern, distributed computing settings.

</details>


### [21] [Enabling Scientific Workflow Scheduling Research in Non-Uniform Memory Access Architectures](https://arxiv.org/abs/2511.19832)
*Aurelio Vivas,Harold Castro*

Main category: cs.DC

TL;DR: 关联：编译器（涉及运行时系统、任务/数据放置、性能优化）。TLDR：数据密集型科学工作流在基于NUMA的HPC系统上面临调度挑战，因为大多数现有调度策略缺乏NUMA感知。本文提出了nFlows，一个NUMA感知的工作流执行运行时系统，它允许对数据密集型工作流进行建模、裸机执行、仿真和调度算法验证，以研究NUMA效应、设计新的调度算法并解决HPC环境中的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 数据密集型科学工作流在HPC系统上的调度面临巨大挑战，主要源于HPC系统中常见的非统一内存访问（NUMA）架构。现代HPC节点集成多NUMA域、异构内存（如HBM和DRAM）以及加速器（GPU/FPGA）和网卡（NIC），导致数据访问延迟变异性增加，任务和数据放置复杂化。当前大多数工作流调度策略源自Grid或Cloud环境，很少考虑NUMA的影响，从而无法充分利用HPC系统的性能。

Method: 本文设计并实现了一个名为nFlows的NUMA感知工作流执行运行时系统（NUMA-aware Workflow Execution Runtime System）。nFlows不仅提供了一个统一的平台，允许对NUMA感知的数据密集型工作流进行建模、调度算法的裸机执行、仿真和验证，还支持将模拟模型直接部署到物理系统上进行执行和研究。

Result: nFlows系统的设计、实现和验证方法已经提供。该系统支持构建模拟模型，并能直接在物理系统上执行这些模型。这使得研究者能够研究NUMA对调度的影响、设计NUMA感知算法、分析数据移动行为、识别性能瓶颈以及探索内存内工作流执行。

Conclusion: nFlows填补了现有工作流调度策略（多源于Grid/Cloud环境）缺乏对NUMA架构考虑的空白，为数据密集型工作流在基于NUMA的HPC系统上的建模、裸机执行、仿真和调度算法设计与验证提供了一个统一的平台，有效解决了NUMA架构带来的访存复杂性和调度挑战。

Abstract: Data-intensive scientific workflows increasingly rely on high-performance computing (HPC) systems, complementing traditional Grid and Cloud platforms. However, workflow scheduling on HPC infrastructures remains challenging due to the prevalence of non-uniform memory access (NUMA) architectures. These systems require schedulers to account for data locality not only across distributed environments but also within each node. Modern HPC nodes integrate multiple NUMA domains and heterogeneous memory regions, such as high-bandwidth memory (HBM) and DRAM, and frequently attach accelerators (GPUs or FPGAs) and network interface cards (NICs) to specific NUMA nodes. This design increases the variability of data-access latency and complicates the placement of both tasks and data. Despite these constraints, most workflow scheduling strategies were originally developed for Grid or Cloud environments and rarely incorporate NUMA-aware considerations. To address this gap, this work introduces nFlows, a NUMA-aware Workflow Execution Runtime System that enables the modeling, bare-metal execution, simulation, and validation of scheduling algorithms for data-intensive workflows on NUMA-based HPC systems. The system's design, implementation, and validation methodology are presented. nFlows supports the construction of simulation models and their direct execution on physical systems, enabling studies of NUMA effects on scheduling, the design of NUMA-aware algorithms, the analysis of data-movement behavior, the identification of performance bottlenecks, and the exploration of in-memory workflow execution.

</details>


### [22] [PolarStore: High-Performance Data Compression for Large-Scale Cloud-Native Databases](https://arxiv.org/abs/2511.19949)
*Qingda Hu,Xinjun Yang,Feifei Li,Junru Li,Ya Lin,Yuqi Zhou,Yicong Zhu,Junwei Zhang,Rongbiao Xie,Ling Zhou,Bin Wu,Wenchao Zhou*

Main category: cs.DC

TL;DR: This paper is related to **compiler** (in a broad sense as the system involves low-level optimization for a database system) and **graph processing** (not directly, but RDBMS and data management are related to large-scale data structures which can be represented as graphs). The paper presents PolarStore, a compressed shared storage system for cloud-native RDBMSs, which uses a dual-layer compression mechanism with dedicated CSD hardware and software-based compression, along with database-oriented optimizations and compression-aware scheduling. Deployed in PolarDB, it achieves a 3.55 compression ratio and 60% storage cost reduction while maintaining uncompressed performance.


<details>
  <summary>Details</summary>
Motivation: 云原生RDBMS通过计算和存储分离实现计算资源的弹性，但存储成本仍是用户关注的焦点。数据压缩是降低存储成本的有效策略。然而，现有的RDBMS压缩方法存在弊端：软件方法带来显著的性能开销，而硬件方法缺乏应对多样化数据库工作负载所需的灵活性。因此，需要一种既能有效压缩数据，又能保持高性能和灵活性的解决方案。

Method: 本文介绍了PolarStore，一个为云原生RDBMS设计的压缩共享存储系统。方法的核心是：1. 采用双层压缩机制，结合PolarCSD硬件的存储内压缩和软件中的轻量级压缩。2. 整合面向数据库的优化，以在关键I/O路径上保持高性能。3. 引入PolarCSD的硬件改进，以确保主机级稳定性。4. 提出压缩感知调度方案，以提高集群级空间效率。

Result: PolarStore在PolarDB内部署于数千个存储服务器上，管理着超过100 PB的数据。它在保持与未压缩集群相当的性能的同时，实现了3.55的压缩比，并将存储成本降低了大约60%。

Conclusion: PolarStore在阿里云的PolarDB中得到了大规模部署，管理着超过100PB的数据。它实现了3.55的压缩比，降低了大约60%的存储成本，同时性能与未压缩集群相当。这证明了PolarStore作为一种有效降低云原生RDBMS存储成本的解决方案的实用性和优越性。

Abstract: In recent years, resource elasticity and cost optimization have become essential for RDBMSs. While cloud-native RDBMSs provide elastic computing resources via disaggregated computing and storage, storage costs remain a critical user concern. Consequently, data compression emerges as an effective strategy to reduce storage costs. However, existing compression approaches in RDBMSs present a stark trade-off: software-based approaches incur significant performance overheads, while hardware-based alternatives lack the flexibility required for diverse database workloads. In this paper, we present PolarStore, a compressed shared storage system for cloud-native RDBMSs. PolarStore employs a dual-layer compression mechanism that combines in-storage compression in PolarCSD hardware with lightweight compression in software. This design leverages the strengths of both approaches. PolarStore also incorporates database-oriented optimizations to maintain high performance on critical I/O paths. Drawing from large-scale deployment experiences, we also introduce hardware improvements for PolarCSD to ensure host-level stability and propose a compression-aware scheduling scheme to improve cluster-level space efficiency. PolarStore is currently deployed on thousands of storage servers within PolarDB, managing over 100 PB of data. It achieves a compression ratio of 3.55 and reduces storage costs by approximately 60%. Remarkably, these savings are achieved while maintaining performance comparable to uncompressed clusters.

</details>


### [23] [QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation](https://arxiv.org/abs/2511.20100)
*Xinguo Zhu,Shaohui Peng,Jiaming Guo,Yunji Chen,Qi Guo,Yuanbo Wen,Hang Qin,Ruizhi Chen,Qirui Zhou,Ke Gao,Yanjun Wu,Chen Zhao,Ling Li*

Main category: cs.DC

TL;DR: 该论文与编译器、HLS、图处理、DSL 和 MLIR 相关。它涉及高性能 GPU 内核生成，属于编译器优化和 HLS 的范畴。

**TLDR 摘要:** 针对现有 LLM 直接生成高性能 GPU 内核时在正确性和效率方面表现出的局限性，本文提出了一种名为 Macro Thinking Micro Coding (MTMC) 的分层框架。该框架将优化策略与实现细节解耦，其中“宏观思考”使用强化学习指导的轻量级 LLM 学习高效的优化策略，“微观编码”利用通用 LLM 增量实现这些策略以确保正确性。MTMC 有效地导航了巨大的优化空间，并在 KernelBench 和 TritonBench 上的实验结果表明，它在准确性和运行时间上均超越了现有的 SOTA LLM 和专家优化内核。


<details>
  <summary>Details</summary>
Motivation: 开发高性能 GPU 内核对于 AI 和科学计算至关重要，但当前的挑战在于这通常依赖于专家手工制作且可移植性差。尽管 LLM 在自动化方面具有潜力，但现有的通用和微调 LLM 在内核生成中存在局限性，即难以同时保证**正确性**和**效率**。根本原因在于现有方法直接生成整个优化的低级程序，需要在包含优化策略和实现代码的巨大空间中进行探索。因此，需要一种方法来应对探索这一棘手空间的挑战。

Method: 本文提出了 Macro Thinking Micro Coding (MTMC) 分层框架。它将优化策略与实现细节解耦。**宏观思考（Macro Thinking）**利用强化学习（RL）来指导轻量级 LLM 有效探索和学习语义优化策略，以最大化硬件利用率。**微观编码（Micro Coding）**利用通用 LLM 0增量地实现宏观思考提出的逐步优化建议，避免了全内核生成错误。这种分级方法旨在解决现有 LLM 直接生成低级优化程序时面临的正确性和效率问题。

Result: MTMC 在广泛采用的基准测试中表现出超越现有技术的性能，尤其是在准确性和运行时间方面。
*   在 KernelBench 上，MTMC 在 Level 1-2 和 Level 3 上的准确率接近 100% 和 70%，比 SOTA 的通用和领域微调 LLM 高出 50% 以上。
*   运行时间方面，MTMC 比 LLM 快达 7.3 倍，比专家优化的 PyTorch Eager 内核快达 2.2 倍。
*   在更具挑战性的 TritonBench 上，MTMC 的准确率高达 59.64%，速度提高 34 倍。
这些结果证明 MTMC 成功地解决了现有 LLM 方法在高性能 GPU 内核生成中的局限性。

Conclusion: MTMC 是一种受人类专家分阶段优化策略启发的分层框架，它将优化策略与实现细节解耦。通过宏观思考（RL 指导的轻量级 LLM）和微观编码（通用 LLM 增量实现）的结合，MTMC 能够高效地探索优化空间并确保实现的正确性。实验结果验证了 MTMC 在 GPU 内核生成方面的优越性，显著提高了准确性和运行时间，性能优于现有的 SOTA 通用和领域微调 LLM，甚至超过了专家优化的 PyTorch Eager 内核。

Abstract: Developing high-performance GPU kernels is critical for AI and scientific computing, but remains challenging due to its reliance on expert crafting and poor portability. While LLMs offer promise for automation, both general-purpose and finetuned LLMs suffer from two fundamental and conflicting limitations: correctness and efficiency. The key reason is that existing LLM-based approaches directly generate the entire optimized low-level programs, requiring exploration of an extremely vast space encompassing both optimization policies and implementation codes. To address the challenge of exploring an intractable space, we propose Macro Thinking Micro Coding (MTMC), a hierarchical framework inspired by the staged optimization strategy of human experts. It decouples optimization strategy from implementation details, ensuring efficiency through high-level strategy and correctness through low-level implementation. Specifically, Macro Thinking employs reinforcement learning to guide lightweight LLMs in efficiently exploring and learning semantic optimization strategies that maximize hardware utilization. Micro Coding leverages general-purpose LLMs to incrementally implement the stepwise optimization proposals from Macro Thinking, avoiding full-kernel generation errors. Together, they effectively navigate the vast optimization space and intricate implementation details, enabling LLMs for high-performance GPU kernel generation. Comprehensive results on widely adopted benchmarks demonstrate the superior performance of MTMC on GPU kernel generation in both accuracy and running time. On KernelBench, MTMC achieves near 100% and 70% accuracy at Levels 1-2 and 3, over 50% than SOTA general-purpose and domain-finetuned LLMs, with up to 7.3x speedup over LLMs, and 2.2x over expert-optimized PyTorch Eager kernels. On the more challenging TritonBench, MTMC attains up to 59.64% accuracy and 34x speedup.

</details>


### [24] [Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management](https://arxiv.org/abs/2511.20172)
*Xinjun Yang,Qingda Hu,Junru Li,Feifei Li,Yuqi Zhou,Yicong Zhu,Qiuru Lin,Jian Dai,Yang Kong,Jiayu Zhang,Guoqiang Xu,Qiang Liu*

Main category: cs.DC

TL;DR: 该论文涉及图处理/DSL/MLIR/编译器/HLS吗？不相关。
太长不看摘要：这篇论文提出了Beluga，一个利用CXL交换机为GPU和CPU提供共享、大规模内存池的新型内存架构，旨在解决LLM大模型推理中KVCache导致的内存瓶颈。Beluga支持原生的load/store语义，提供接近本地内存的低延迟访问，并减少了编程和同步复杂性。基于Beluga，作者实现了Beluga-KVCache系统，该系统在vLLM推理中，相较于基于RDMA的方案，将首个Token生成时间（TTFT）降低了89.6%，吞吐量提高了7.35倍。


<details>
  <summary>Details</summary>
Motivation: 随着LLM模型规模的不断增加和对长上下文推理需求的增长，内存已成为GPU加速服务系统中的关键瓶颈。GPU上的HBM容量有限，导致KVCache等大型工作集必须依赖主机内存（CPU DRAM）。然而，CPU DRAM的容量受到每个CPU插槽内存通道数量的限制。现有的解决方案，如基于RDMA的内存池解耦，引入了高访问延迟、复杂的通信协议和同步开销。新兴的CXL技术为解决这些挑战提供了新的机会。

Method: 本文提出了Beluga内存架构，它允许GPU和CPU通过CXL交换机访问一个共享的、大规模内存池。Beluga支持原生的load/store访问语义，实现了接近本地内存的延迟，并简化了编程和同步开销。作者对一个商用CXL交换机内存池进行了系统性的特性分析，并提出了一套设计指南。基于Beluga，他们设计并实现了Beluga-KVCache系统，用于管理LLM推理中的大规模KVCache。

Result: Beluga-KVCache（基于所提出的Beluga内存架构）在vLLM推理引擎中的表现显著优于基于RDMA的解决方案。它实现了首个Token生成时间（TTFT）降低89.6%，吞吐量提高了7.35倍。

Conclusion: Beluga是首个实现GPU通过CXL交换机直接访问大规模内存池的系统，它提供了一种低延迟、共享访问大容量内存资源的新范式。实验结果显示，相对于基于RDMA的解决方案，Beluga-KVCache在vLLM推理引擎中将首个Token生成时间（TTFT）降低了89.6%，吞吐量提高了7.35倍。

Abstract: The rapid increase in LLM model sizes and the growing demand for long-context inference have made memory a critical bottleneck in GPU-accelerated serving systems. Although high-bandwidth memory (HBM) on GPUs offers fast access, its limited capacity necessitates reliance on host memory (CPU DRAM) to support larger working sets such as the KVCache. However, the maximum DRAM capacity is constrained by the limited number of memory channels per CPU socket. To overcome this limitation, current systems often adopt RDMA-based disaggregated memory pools, which introduce significant challenges including high access latency, complex communication protocols, and synchronization overhead. Fortunately, the emerging CXL technology introduces new opportunities in KVCache design. In this paper, we propose Beluga, a novel memory architecture that enables GPUs and CPUs to access a shared, large-scale memory pool through CXL switches. By supporting native load/store access semantics over the CXL fabric, our design delivers near-local memory latency, while reducing programming complexity and minimizing synchronization overhead. We conduct a systematic characterization of a commercial CXL switch-based memory pool and propose a set of design guidelines. Based on Beluga, we design and implement Beluga-KVCache, a system tailored for managing the large-scale KVCache in LLM inference. Beluga-KVCache achieves an 89.6% reduction in Time-To-First-Token (TTFT) and 7.35x throughput improvement in the vLLM inference engine compared to RDMA-based solutions. To the best of our knowledge, Beluga is the first system that enables GPUs to directly access large-scale memory pools through CXL switches, marking a significant step toward low-latency, shared access to vast memory resources by GPUs.

</details>
