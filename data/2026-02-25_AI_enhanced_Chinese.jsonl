{"id": "2602.20376", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.20376", "abs": "https://arxiv.org/abs/2602.20376", "authors": ["Ria Stevens", "Fangshuo Liao", "Barbara Su", "Jianqiang Li", "Anastasios Kyrillidis"], "title": "Exploiting Low-Rank Structure in Max-K-Cut Problems", "comment": null, "summary": "We approach the Max-3-Cut problem through the lens of maximizing complex-valued quadratic forms and demonstrate that low-rank structure in the objective matrix can be exploited, leading to alternative algorithms to classical semidefinite programming (SDP) relaxations and heuristic techniques. We propose an algorithm for maximizing these quadratic forms over a domain of size $K$ that enumerates and evaluates a set of $O\\left(n^{2r-1}\\right)$ candidate solutions, where $n$ is the dimension of the matrix and $r$ represents the rank of an approximation of the objective. We prove that this candidate set is guaranteed to include the exact maximizer when $K=3$ (corresponding to Max-3-Cut) and the objective is low-rank, and provide approximation guarantees when the objective is a perturbation of a low-rank matrix. This construction results in a family of novel, inherently parallelizable and theoretically-motivated algorithms for Max-3-Cut. Extensive experimental results demonstrate that our approach achieves performance comparable to existing algorithms across a wide range of graphs, while being highly scalable.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20762", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.20762", "abs": "https://arxiv.org/abs/2602.20762", "authors": ["Keigo Oka"], "title": "Turing Completeness of GNU find: From mkdir-assisted Loops to Standalone Computation", "comment": null, "summary": "The Unix command \\texttt{find} is among the first commands taught to beginners, yet remains indispensable for experienced engineers. In this paper, we demonstrate that \\texttt{find} possesses unexpected computational power, establishing three Turing completeness results using the GNU implementation (a standard in Linux distributions). (1) \\texttt{find} + \\texttt{mkdir} (a system that has only \\texttt{find} and \\texttt{mkdir}) is Turing complete: by encoding computational states as directory paths and using regex back-references to copy substrings, we simulate 2-tag systems. (2) GNU \\texttt{find} 4.9.0+ alone is Turing complete: by reading and writing to files during traversal, we simulate a two-counter machine without \\texttt{mkdir}. (3) \\texttt{find} + \\texttt{mkdir} without regex back-references is still Turing complete: by a trick of encoding regex patterns directly into directory names, we achieve the same power.\n  These results place \\texttt{find} among the ``surprisingly Turing-complete'' systems, highlighting the hidden complexity within seemingly simple standard utilities.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20833", "categories": ["cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20833", "abs": "https://arxiv.org/abs/2602.20833", "authors": ["Eduar Castrillo Velilla"], "title": "DRESS: A Continuous Framework for Structural Graph Refinement", "comment": null, "summary": "The Weisfeiler-Lehman (WL) hierarchy is a cornerstone framework for graph isomorphism testing and structural analysis. However, scaling beyond 1-WL to 3-WL and higher requires tensor-based operations that scale as O(n^3) or O(n^4), making them computationally prohibitive for large graphs. In this paper, we start from the Original-DRESS equation (Castrillo, Leon, and Gomez, 2018)--a parameter-free, continuous dynamical system on edges--and show that it distinguishes the prism graph from K_{3,3}, a pair that 1-WL provably cannot separate. We then generalize it to Motif-DRESS, which replaces triangle neighborhoods with arbitrary structural motifs and converges to a unique fixed point under three sufficient conditions, and further to Generalized-DRESS, an abstract template parameterized by the choice of neighborhood operator, aggregation function and norm. Finally, we introduce Delta-DRESS, which runs DRESS on each node-deleted subgraph G\\{v}, connecting the framework to the Kelly-Ulam reconstruction conjecture. Both Motif-DRESS and Delta-DRESS empirically distinguish Strongly Regular Graphs (SRGs)--such as the Rook and Shrikhande graphs--that confound 3-WL. Our results establish the DRESS family as a highly scalable framework that empirically surpasses both 1-WL and 3-WL on well-known benchmark graphs, without the prohibitive O(n^4) computational cost.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20854", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.20854", "abs": "https://arxiv.org/abs/2602.20854", "authors": ["Elena Gribelyuk", "Honghao Lin", "David P. Woodruff", "Huacheng Yu", "Samson Zhou"], "title": "Adversarial Robustness on Insertion-Deletion Streams", "comment": null, "summary": "We study adversarially robust algorithms for insertion-deletion (turnstile) streams, where future updates may depend on past algorithm outputs. While robust algorithms exist for insertion-only streams with only a polylogarithmic overhead in memory over non-robust algorithms, it was widely conjectured that turnstile streams of length polynomial in the universe size $n$ require space linear in $n$. We refute this conjecture, showing that robustness can be achieved using space which is significantly sublinear in $n$. Our framework combines multiple linear sketches in a novel estimator-corrector-learner framework, yielding the first insertion-deletion algorithms that approximate: (1) the second moment $F_2$ up to a $(1+\\varepsilon)$-factor in polylogarithmic space, (2) any symmetric function $\\cal{F}$ with an $\\mathcal{O}(1)$-approximate triangle inequality up to a $2^{\\mathcal{O}(C)}$ factor in $\\tilde{\\mathcal{O}}(n^{1/C}) \\cdot S(n)$ bits of space, where $S$ is the space required to approximate $\\cal{F}$ non-robustly; this includes a broad class of functions such as the $L_1$-norm, the support size $F_0$, and non-normed losses such as the $M$-estimators, and (3) $L_2$ heavy hitters. For the $F_2$ moment, our algorithm is optimal up to $\\textrm{poly}((\\log n)/\\varepsilon)$ factors. Given the recent results of Gribelyuk et al. (STOC, 2025), this shows an exponential separation between linear sketches and non-linear sketches for achieving adversarial robustness in turnstile streams.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20204", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20204", "abs": "https://arxiv.org/abs/2602.20204", "authors": ["Javed Absar", "Samarth Narang", "Muthu Baskaran"], "title": "Analyzing Latency Hiding and Parallelism in an MLIR-based AI Kernel Compiler", "comment": "Accepted at MLBench workshop as part of ASPLOS'26", "summary": "AI kernel compilation for edge devices depends on the compiler's ability to exploit parallelism and hide memory latency in the presence of hierarchical memory and explicit data movement. This paper reports a benchmark methodology and corresponding results for three compiler-controlled mechanisms in an MLIR-based compilation pipeline: vectorization (Vec), multi-threading (MT) across hardware contexts, and double buffering (DB) using ping--pong scratchpad buffers to overlap DMA transfers with compute. Using Triton/Inductor-generated kernels, we present an ablation ladder that separates the contribution of Vec, MT, and DB, and we quantify how MT speedup scales with problem size using GELU as a representative activation kernel. The results show that vectorization provides the primary gain for bandwidth-sensitive kernels, MT delivers substantial improvements once scheduling overhead is amortized, and DB provides additional benefit when transfers and compute can be overlapped (i.e., outside the extremes of purely memory-bound or purely compute-bound behavior).", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20471", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20471", "abs": "https://arxiv.org/abs/2602.20471", "authors": ["Da Chen", "Guangyu Hu", "Kaihong Xu", "Kaichao Liang", "Songjiang Li", "Wei Yang", "XiangYu Wen", "Mingxuan Yuan"], "title": "SegSEM: Enabling and Enhancing SAM2 for SEM Contour Extraction", "comment": "4 pages, 6 figures, accpeted by ISCAS 2026", "summary": "Extracting high-fidelity 2D contours from Scanning Electron Microscope (SEM) images is critical for calibrating Optical Proximity Correction (OPC) models. While foundation models like Segment Anything 2 (SAM2) are promising, adapting them to specialized domains with scarce annotated data is a major challenge. This paper presents a case study on adapting SAM2 for SEM contour extraction in a few-shot setting. We propose SegSEM, a framework built on two principles: a data-efficient fine-tuning strategy that adapts by selectively training only the model's encoders, and a robust hybrid architecture integrating a traditional algorithm as a confidence-aware fallback. Using a small dataset of 60 production images, our experiments demonstrate this methodology's viability. The primary contribution is a methodology for leveraging foundation models in data-constrained industrial applications.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20341", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20341", "abs": "https://arxiv.org/abs/2602.20341", "authors": ["Ignacio Amores-Sesar", "Mirza Ahad Baig", "Seth Gilbert", "Ray Neiheiser", "Michelle X. Yeo"], "title": "The Tragedy of Chain Commons", "comment": null, "summary": "Byzantine Fault Tolerant (BFT) consensus forms the foundation of many modern blockchains striving for both high throughput and low latency. A growing bottleneck is transaction execution and validation on the critical path of consensus, which has led to modular decoupled designs that separate ordering from execution: Consensus orders only metadata, while transactions are executed and validated concurrently. While this approach improves performance, it can leave invalid transactions in the ledger, increasing storage costs and enabling new forms of strategic behavior. We present the first systematic study of this setting, providing a formal framework to reason about the interaction between consensus and execution. Using this framework, we show that the decoupled design enables a previously unidentified attack, which we term gaslighting. We prove a fundamental trade-off between resilience to this attack and resource capacity utilization, where both are impossible to achieve deterministically in the decoupled model. To address this trade-off, we discuss an intermediate model for leader-based protocols that is robust to gaslighting attacks while achieving high throughput and low latency.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20949", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.20949", "abs": "https://arxiv.org/abs/2602.20949", "authors": ["Vinicius Tikara Venturi Date", "Leandro Miranda Zatesko"], "title": "A $2$-branching construction for the $\u03c7\\leq 2r$ bound", "comment": "12 pages, 4 tables, 1 figure. Submitted to CPM 2026", "summary": "The string repetitiveness measures $\u03c7$ (the size of a smallest suffixient set of a string) and $r$ (the number of runs in the Burrows--Wheeler Transform) are related. Recently, we have shown that the bound $\u03c7\\leq 2r$, proved by Navarro et al., is asymptotically tight as the size $\u03c3$ of the alphabet increases, but achieving near-tight ratios for fixed $\u03c3> 2$ remained open. We introduce a \\emph{2-branching property}: a cyclic string is 2-branching at order~$k$ if every $(k{-}1)$-length substring admits exactly two $k$-length extensions. We show that 2-branching strings of order~$k$ yield closed-form ratios $\u03c7/r = (2\u03c3^{k-1}+1)/(\u03c3^{k-1}+4)$. For order~$3$, we give an explicit construction for every $\u03c3\\geq 2$, narrowing the gap to~$2$ from $O(1/\u03c3)$ to $O(1/\u03c3^2)$. For $\u03c3\\in \\{3,4\\}$, we additionally present order-$5$ instances with ratios exceeding~$1.91$.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20866", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2602.20866", "abs": "https://arxiv.org/abs/2602.20866", "authors": ["Timon B\u00f6hler", "Tobias Reinhard", "David Richter", "Mira Mezini"], "title": "DeCo: A Core Calculus for Incremental Functional Programming with Generic Data Types", "comment": "Accepted at OOPSLA'26", "summary": "Incrementalization speeds up computations by avoiding unnecessary recomputations and by efficiently reusing previous results. While domain-specific techniques achieve impressive speedups, e.g., in the context of database queries, they are difficult to generalize. Meanwhile, general approaches offer little support for incrementalizing domain-specific operations. In this work, we present DeCo, a novel core calculus for incremental functional programming with support for a wide range of user-defined data types. Despite its generic nature, our approach statically incrementalizes domain-specific operations on user-defined data types. It is, hence, more fine-grained than other generic techniques which resort to treating domain-specific operations as black boxes. We mechanized our work in Lean and proved it sound, meaning incrementalized execution computes the same result as full reevaluation. We also provide an executable implementation with case studies featuring examples from linear algebra, relational algebra, dictionaries, trees, and conflict-free replicated data types, plus a brief performance evaluation on linear and relational algebra and on trees.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20515", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20515", "abs": "https://arxiv.org/abs/2602.20515", "authors": ["Rakshith Jayanth", "Viktor Prasanna"], "title": "FAST-Prefill: FPGA Accelerated Sparse Attention for Long Context LLM Prefill", "comment": null, "summary": "In long-context large language model (LLM) inference, the prefill stage dominates computation due to self-attention over the complete input context. Sparse attention significantly reduces self-attention computation by limiting each token's interactions to a subset of tokens. The attention sparsity pattern varies across input prompts, and within a prompt, each attention head can follow a distinct pattern. This makes attention sparsity dynamic. The requirement of generating the sparsity pattern, combined with limited data reuse in attention, shifts the prefill compute to being memory-bound. This, in addition to the huge energy requirements for long-context inference on GPU, motivates FPGAs as good candidates for accelerating dynamic long-context inference.\n  To tackle these challenges, we propose FAST-Prefill, the first FPGA accelerator for long-context prefill-stage inference with dynamic sparse attention. To efficiently generate sparse indices, we propose a \\textit{fused pipeline unit with a memory-aware execution order} to reduce large tensors and irregular memory accesses. To reduce off-chip memory traffic for accessing the KV cache, we utilize the memory hierarchy to design a \\textit{liveness-driven, dual-tier cache}. For high-throughput matrix multiplication, we design a \\textit{hybrid Matrix Processing Unit (MPU)} with DSPs and bit-plane decomposition using LUTs. We implement FAST-Prefill on Alveo U280 and evaluate it on the Llama and Qwen models (batch size = 1) for context lengths ranging from 4K to 128K tokens. We demonstrate an average speedup of up to 2.5$\\times$ in TTFT and 4.5$\\times$ improvement in energy efficiency over GPU implementation on Nvidia A5000 GPU.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20444", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20444", "abs": "https://arxiv.org/abs/2602.20444", "authors": ["Paul Borrill"], "title": "Circumventing the FLP Impossibility Result with Open Atomic Ethernet", "comment": "12 pages, 3 figures, 1 table", "summary": "The Fischer--Lynch--Paterson (FLP) impossibility result is widely regarded as one of the most fundamental negative results in distributed computing: no deterministic protocol can guarantee consensus in an asynchronous system with even one faulty process. For forty years, the field has treated this as an immovable constraint, designing around it with randomized protocols, failure detectors, and weakened consistency models. This essay argues that FLP is not a law of physics but a theorem about a particular system model -- and that Open Atomic Ethernet (OAE) circumvents it by rejecting the asynchronous model at its foundation. We introduce the term bisynchronous to describe OAE's key property: bounded-time bilateral resolution in which both parties reach common knowledge of outcome at every round boundary -- a strictly stronger guarantee than synchrony alone. By constructing a bisynchronous, swap-based protocol at Layer 2, OAE sidesteps the load-bearing assumptions of FLP's asynchronous model, achieving deterministic atomic coordination without violating any impossibility result.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.21088", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.21088", "abs": "https://arxiv.org/abs/2602.21088", "authors": ["Roman Edenhofer"], "title": "A Space-space Trade-off for Directed st-Connectivity", "comment": null, "summary": "We prove a space-space trade-off for directed $st$-connectivity in the catalytic space model. For any integer $k \\leq n$, we give an algorithm that decides directed $st$-connectivity using $O(\\log n \\cdot \\log k+\\log n)$ regular workspace and $O\\left(\\frac{n}{k} \\cdot \\log^2 n\\right)$ bits of catalytic memory. This interpolates between the classical $O(\\log^2 n)$-space bound from Savitch's algorithm and a catalytic endpoint with $O(\\log n)$ workspace and $O(n\\cdot \\log^2 n)$ catalytic memory.\n  As a warm-up, we present a catalytic variant of Savitch's algorithm achieving the endpoint above. Up to logarithmic factors, this matches the smallest catalyst size currently known for catalytic logspace algorithms, due to Cook and Pyne (ITCS 2026). Our techniques also extend to counting the number of walks from $s$ to $t$ of a given length $\\ell\\leq n$.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20662", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20662", "abs": "https://arxiv.org/abs/2602.20662", "authors": ["Hongyi Guan", "Yijia Zhang", "Wenqiang Wang", "Yizhao Gao", "Shijie Cao", "Chen Zhang", "Ningyi Xu"], "title": "TOM: A Ternary Read-only Memory Accelerator for LLM-powered Edge Intelligence", "comment": "13 pages", "summary": "The deployment of Large Language Models (LLMs) for real-time intelligence on edge devices is rapidly growing. However, conventional hardware architectures face a fundamental memory wall challenge, where limited on-device memory capacity and bandwidth severely constrain the size of deployable models and their inference speed, while also limiting on-device adaptation. To address this challenge, we propose TOM, a hybrid ROM-SRAM accelerator co-designed with ternary quantization, which balances extreme density with on-device tunability. TOM exploits the synergy between ternary quantization and ROM to achieve extreme memory density and bandwidth, while preserving flexibility through a hybrid ROM-SRAM architecture designed for QLoRA-based tunability. Specifically, we introduce: (1) a sparsity-aware ROM architecture that synthesizes ternary weights as standard-cell logic, eliminating area overhead from zero-valued bits; (2) a distributed processing architecture that co-locates high-density ROM banks with flexible SRAM-based QLoRA adapters and compute units; and (3) a workload-aware dynamic power gating scheme that exploits the logic-based nature of ROM to power down inactive banks, minimizing dynamic energy consumption. TOM achieves an inference throughput of 3,306 TPS using BitNet-2B model, demonstrating its effectiveness in delivering real-time, energy-efficient edge intelligence.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20450", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20450", "abs": "https://arxiv.org/abs/2602.20450", "authors": ["Nihal Balivada", "Shrey Gupta", "Shashank Shreedhar Bhatt", "Suyash Gupta"], "title": "Heterogeneity-Aware Client Selection Methodology For Efficient Federated Learning", "comment": null, "summary": "Federated Learning (FL) enables a distributed client-server architecture where multiple clients collaboratively train a global Machine Learning (ML) model without sharing sensitive local data. However, FL often results in lower accuracy than traditional ML algorithms due to statistical heterogeneity across clients. Prior works attempt to address this by using model updates, such as loss and bias, from client models to select participants that can improve the global model's accuracy. However, these updates neither accurately represent a client's heterogeneity nor are their selection methods deterministic. We mitigate these limitations by introducing Terraform, a novel client selection methodology that uses gradient updates and a deterministic selection algorithm to select heterogeneous clients for retraining. This bi-pronged approach allows Terraform to achieve up to 47 percent higher accuracy over prior works. We further demonstrate its efficiency through comprehensive ablation studies and training time analyses, providing strong justification for the robustness of Terraform.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.21089", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.21089", "abs": "https://arxiv.org/abs/2602.21089", "authors": ["Petr Chmel", "Aditi Dudeja", "Michal Kouck\u00fd", "Ian Mertz", "Ninad Rajgopal"], "title": "Frontier Space-Time Algorithms Using Only Full Memory", "comment": null, "summary": "We develop catalytic algorithms for fundamental problems in algorithm design that run in polynomial time, use only $\\mathcal{O}(\\log(n))$ workspace, and use sublinear catalytic space matching the best-known space bounds of non-catalytic algorithms running in polynomial time.\n  First, we design a polynomial time algorithm for directed $s$-$t$ connectivity using $n \\big/ 2^{\u0398(\\sqrt{\\log n})}$ catalytic space, which matches the state-of-the-art time-space bounds in the non-catalytic setting [Barnes et al., 1998], and improves the catalytic space usage of the best known algorithm [Cook and Pyne, 2026]. Furthermore, using only $\\mathcal{O}(\\log(n))$ random bits we get a randomized algorithm whose running time nearly matches the fastest time bounds known for space-unrestricted algorithms.\n  Second, we design polynomial time algorithms for the problems of computing Edit Distance, Longest Common Subsequence, and the Discrete Fr\u00e9chet Distance, again using $n \\big/ 2^{\u0398(\\sqrt{\\log n})}$ catalytic space. This again matches non-catalytic time-space frontier for Edit Distance and Least Common Subsequence [Kiyomi et al., 2021].", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20802", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20802", "abs": "https://arxiv.org/abs/2602.20802", "authors": ["Philippos Papaphilippou"], "title": "LUTstructions: Self-loading FPGA-based Reconfigurable Instructions", "comment": null, "summary": "General-purpose processors feature a limited number of instructions based on an instruction set. They can be numerous, such as with vector extensions that include hundreds or thousands of instructions, but this comes at a cost; they are often unable to express arbitrary tasks efficiently. This paper explores the concept of having reconfigurable instructions by incorporating reconfigurable areas in a softcore. It follows a relatively-recently proposed computer architecture concept for seamlessly loading instruction implementation-carrying bitstreams from main memory. The resulting softcore is entirely evaluated on an FPGA, essentially having an FPGA-on-an-FPGA for the instruction implementations, with no notable operating frequency overhead. This is achieved with a custom FPGA architecture called LUTstruction, which is tailored towards low-latency for custom instructions and wide reconfiguration, as well as a soft implementation for the purposes of architectural exploration.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20561", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20561", "abs": "https://arxiv.org/abs/2602.20561", "authors": ["Sana Taghipour Anvar", "David Kaeli"], "title": "A Granularity Characterization of Task Scheduling Effectiveness", "comment": null, "summary": "Task-based runtime systems provide flexible load balancing and portability for parallel scientific applications, but their strong scaling is highly sensitive to task granularity. As parallelism increases, scheduling overhead may transition from negligible to dominant, leading to rapid drops in performance for some algorithms, while remaining negligible for others. Although such effects are widely observed empirically, there is a general lack of understanding how algorithmic structure impacts whether dynamic scheduling is always beneficial. In this work, we introduce a granularity characterization framework that directly links scheduling overhead growth to task-graph dependency topology. We show that dependency structure, rather than problem size alone, governs how overhead scales with parallelism. Based on this observation, we characterize execution behavior using a simple granularity measure that indicates when scheduling overhead can be amortized by parallel computation and when scheduling overhead dominates performance. Through experimental evaluation on representative parallel workloads with diverse dependency patterns, we demonstrate that the proposed characterization explains both gradual and abrupt strong-scaling breakdowns observed in practice. We further show that overhead models derived from dependency topology accurately predict strong-scaling limits and enable a practical runtime decision rule for selecting dynamic or static execution without requiring exhaustive strong-scaling studies or extensive offline tuning.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20656", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20656", "abs": "https://arxiv.org/abs/2602.20656", "authors": ["Guanbin Xu", "ZhenGuo Xu", "Yuzhe Li", "Youhui Bai", "Ping Gong", "Chaoyi Ruan", "Cheng Li"], "title": "Lagom: Unleashing the Power of Communication and Computation Overlapping for Distributed LLM Training", "comment": "6 pages, 8 figures", "summary": "Overlapping communication with computation is crucial for distributed large-model training, yet optimizing it - especially when computation becomes the bottleneck-remains challenging. We present Lagom, a system that co-tunes communication parameters to balance resource usage between computation and communication. By introducing a unified cost model and a priority-based search algorithm, Lagom reduces optimization complexity from exponential to linear. Evaluations on high- and low-bandwidth GPU clusters show that Lagom achieves 1.07-1.33x and 1.03-1.27x speedup over NCCL and AutoCCL across diverse models and parallelizations.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20887", "categories": ["cs.DC", "cs.CG"], "pdf": "https://arxiv.org/pdf/2602.20887", "abs": "https://arxiv.org/abs/2602.20887", "authors": ["David Knapp", "Johannes Albrecht Holke", "Thomas Spenke", "Carsten Burstedde"], "title": "A Morton-Type Space-Filling Curve for Pyramid Subdivision and Hybrid Adaptive Mesh Refinement", "comment": null, "summary": "The forest-of-refinement-trees approach allows for dynamic adaptive mesh refinement (AMR) at negligible cost. While originally developed for quadrilateral and hexahedral elements, previous work established the theory and algorithms for unstructured meshes of simplicial and prismatic elements. To harness the full potential of tree-based AMR for three-dimensional mixed-element meshes, this paper introduces the pyramid as a new functional element type; its primary purpose is to connect tetrahedral and hexahedral elements without hanging edges.We present a well-defined space-filling curve (SFC) for the pyramid and detail how the unique challenges on the element and forest level associated with the pyramidal refinement are resolved. We propose the necessary functional design and generalize the fundamental global parallel algorithms for refinement, coarsening, partitioning, and face ghost exchange to fully support this new element. Our demonstrations confirm the efficiency and scalability of this complete, hybrid-element dynamic AMR framework.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.21022", "categories": ["cs.DC", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.21022", "abs": "https://arxiv.org/abs/2602.21022", "authors": ["Antonio Cruciani", "Avinandan Das", "Massimo Equi", "Henrik Lievonen", "Diep Luong-Le", "Augusto Modanese", "Jukka Suomela"], "title": "Is a LOCAL algorithm computable?", "comment": "33 pages, 1 figure", "summary": "Common definitions of the \"standard\" LOCAL model tend to be sloppy and even self-contradictory on one point: do the nodes update their state using an arbitrary function or a computable function? So far, this distinction has been safe to neglect, since problems where it matters seem contrived and quite different from e.g. typical local graph problems studied in this context.\n  We show that this question matters even for locally checkable labeling problems (LCLs), perhaps the most widely studied family of problems in the context of the LOCAL model. Furthermore, we show that assumptions about computability are directly connected to another aspect already recognized as highly relevant: whether we have any knowledge of $n$, the size of the graph. Concretely, we show that there is an LCL problem $\u03a0$ with the following properties:\n  1. $\u03a0$ can be solved in $O(\\log n)$ rounds if the \\textsf{LOCAL} model is uncomputable.\n  2. $\u03a0$ can be solved in $O(\\log n)$ rounds in the computable model if we know any upper bound on $n$.\n  3. $\u03a0$ requires $\u03a9(\\sqrt{n})$ rounds in the computable model if we do not know anything about $n$.\n  We also show that the connection between computability and knowledge of $n$ holds in general: for any LCL problem $\u03a0$, if you have any bound on $n$, then $\u03a0$ has the same round complexity in the computable and uncomputable models.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.21140", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21140", "abs": "https://arxiv.org/abs/2602.21140", "authors": ["Haley Li", "Xinglu Wang", "Cong Feng", "Chunxu Zuo", "Yanan Wang", "Hei Lo", "Yufei Cui", "Bingji Wang", "Duo Cui", "Shuming Jing", "Yizhou Shan", "Ying Xiong", "Jiannan Wang", "Yong Zhang", "Zhenan Fan"], "title": "ReviveMoE: Fast Recovery for Hardware Failures in Large-Scale MoE LLM Inference Deployments", "comment": "21 pages, 6 figures", "summary": "As LLM deployments scale over more hardware, the probability of a single failure in a system increases significantly, and cloud operators must consider robust countermeasures to handle these inevitable failures. A common recovery approach is to simply restart the LLM serving instance; however, this is costly in model-as-a-service (MaaS) inference settings, where reloading model weights and recompiling computation graphs can introduce significant delays to incoming requests. We propose ReviveMoE, a method for rapid failure recovery in large-scale LLM deployments without restarting the serving instance. ReviveMoE is designed to support both the traditional LLM architecture, which collocates MoE and attention on the same hardware, and the disaggregated architectures, which separate MoE from attention. Integrated into Huawei Cloud's MaaS, ReviveMoE is built on top of Huawei's xDeepServe serving platform and the XCCL communications library.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.21182", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21182", "abs": "https://arxiv.org/abs/2602.21182", "authors": ["Paul Borrill"], "title": "Circumventing the CAP Theorem with Open Atomic Ethernet", "comment": "23 pages, 14 figures", "summary": "The CAP theorem is routinely treated as a systems law: under network partition, a replicated service must sacrifice either consistency or availability. The theorem is correct within its standard asynchronous network model, but operational practice depends on where partition-like phenomena become observable and on how lower layers discard or preserve semantic information about message fate. This paper argues that Open Atomic Ethernet (OAE) shifts the engineering regime in which CAP tradeoffs become application-visible by (i) replacing fire-and-forget link semantics with bounded-time bilateral reconciliation of endpoint state -- the property we call bisynchrony -- and (ii) avoiding Clos funnel points via an octavalent mesh in which each node can act as the root of a locally repaired spanning tree. The result is not the elimination of hard graph cuts, but a drastic reduction in the frequency and duration of application-visible \"soft partitions\" by detecting and healing dominant fabric faults within hundreds of nanoseconds. We connect this view to Brewer's original CAP framing, the formalization by Gilbert and Lynch, the CAL theorem of Lee et al., which replaces binary partition tolerance with a quantitative measure of apparent latency, and Abadi's PACELC extension.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
