<div id=toc></div>

# Table of Contents

- [cs.DS](#cs.DS) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [1] [Economic Warehouse Lot Scheduling: Approximation Schemes via Efficiently-Representable DP-Encoded Policies](https://arxiv.org/abs/2601.14993)
*Danny Segev*

Main category: cs.DS

TL;DR: 该论文与DSL或图处理或MLIR或编译器或HLS不相关。
本文介绍了经济仓库批次调度问题（EWLSP）准最优补货策略构建的算法进展。针对动态策略的内在复杂性，本文开发了新的分析基础和算法，最终为常数数量的商品提供了一个多项式时间近似方案（PTAS），解决了 $\epsilon$-最优动态策略的有效设计和表示问题，此前这些问题一直被认为是难以解决的。


<details>
  <summary>Details</summary>
Motivation: 经济仓库批次调度问题（EWLSP）在库存管理中是一个经典问题，但自其诞生以来，在计算方面仍存在许多基本问题尚未解决，尤其是在动态策略的内在复杂性方面。早期的尝试要么局限于高结构化的策略类别，要么没有提供可证明的良好性能保证，缺乏严格可分析的结果。本文的动机是为EW L SP的准最优补货策略的有效构建提供期待已久的算法进展。

Method: 本文开发了新的分析基础，可以直接与动态策略进行竞争。结合进一步的算法进步和新获得的见解，最终形成了一个针对常数数量商品的多项式时间近似方案（PTAS）。这使得能够有效地设计准最优的动态补货策略。

Result: 本文的主要成果是为常数数量的商品提供了一个多项式时间近似方案（PTAS），用于构建经济仓库批次调度问题（EWLSP）的 $\epsilon$-最优动态策略。这标志着在直接与动态策略竞争的分析基础、算法进步以及解决动态策略的多项式空间表示这一基本开放问题方面取得了重要突破。

Conclusion: 本文为经济仓库批次调度问题（Economic Warehouse Lot Scheduling Problem, EWLSP）的准最优补货策略的有效构建提供了新的算法进展。通过发展新的分析基础，直接与动态策略竞争，并结合进一步的算法进步和新见解，最终为常数数量的商品提供了一个多项式时间近似方案（PTAS）。这克服了此前对于设计 $\epsilon$-最优动态策略的困难，解决了甚至连动态策略的多项式空间表示都是一个基本悬而未决的问题。

Abstract: In this focused technical paper, we present long-awaited algorithmic advances toward the efficient construction of near-optimal replenishment policies for a true inventory management classic, the economic warehouse lot scheduling problem. While this paradigm has accumulated a massive body of surrounding literature since its inception in the late '50s, we are still very much in the dark as far as basic computational questions are concerned, perhaps due to the intrinsic complexity of dynamic policies in this context. The latter feature forced earlier attempts to either study highly-structured classes of policies or to forgo provably-good performance guarantees altogether; to this day, rigorously analyzable results have been few and far between.
  The current paper develops novel analytical foundations for directly competing against dynamic policies. Combined with further algorithmic progress and newly-gained insights, these ideas culminate in a polynomial-time approximation scheme for constantly-many commodities. In this regard, the efficient design of $ε$-optimal dynamic policies appeared to have been out of reach, since beyond their inherent algorithmic challenges, even the polynomial-space representation of such policies has been a fundamental open question.

</details>


### [2] [Economic Warehouse Lot Scheduling: Breaking the 2-Approximation Barrier](https://arxiv.org/abs/2601.15068)
*Danny Segev*

Main category: cs.DS

TL;DR: 该论文与 DSL、图处理、MLIR、编译器或 HLS 无关。

太长不看：经济仓库批量调度问题是一个自 20 世纪 90 年代中期以来最佳近似因子一直停留在 $2$ 的基础库存理论模型。本文开发了新的分析基础和算法技术，首次实现对 $2$-近似界限的改进，设计了一个多项式时间的随机动态策略，将预期长期平均成本逼近最优解的 $2-\frac{17}{5000} + \epsilon$ 倍，打破了持续数十年的算法瓶颈。


<details>
  <summary>Details</summary>
Motivation: 经济仓库批量调度问题是一个基础性的库存理论模型，它需要在共享能力限制下动态协调多种商品的补货决策。尽管该问题已有六十年的研究历史，但其算法理解仍然受限。自 20 世纪 90 年代中期以来，对于一般问题实例，最好的近似保证一直停留在 $2$ 的因子上，这是通过高度结构化的“平稳订单规模和平稳间隔”（SOSI）策略实现的，其回避了与完全动态策略的直接竞争。作者的动机是打破 $2$-近似的瓶颈，并实现与完全动态策略的直接比较，以获得更好的近似保证。

Method: 本文的主要方法是开发新的分析基础和算法技术，以实现对完全动态策略的直接比较。具体而言，它设计了一种建设性的方法，允许以比以前基于 SOSI（平稳订单规模和平稳间隔）的方法更细的粒度平衡成本和容量。最终，它提出了一种多项式时间构建的随机、容量可行动态策略。

Result: 本文设计了一种多项式时间构建的随机、容量可行的动态策略，其预期长期平均成本接近最优解的 $2-\frac{17}{5000} + \epsilon$ 倍。这是对 $2$-近似界限的首次可证明的改进。

Conclusion: 本文通过开发新的分析基础和算法技术，首次在经济仓库批量调度问题上实现了对 $2$-近似界限的改进。通过设计一种随机的、容量可行的动态策略，将预期的长期平均成本逼近最优解的 $2-\frac{17}{5000} + \epsilon$ 倍，这标志着该领域的一个重大进展，打破了数十年来 $2$-近似的瓶颈。

Abstract: The economic warehouse lot scheduling problem is a foundational inventory-theory model, capturing computational challenges in dynamically coordinating replenishment decisions for multiple commodities subject to a shared capacity constraint. Even though this model has generated a vast body of literature over the last six decades, our algorithmic understanding has remained surprisingly limited. Indeed, for general problem instances, the best-known approximation guarantees have remained at a factor of $2$ since the mid-1990s. These guarantees were attained by the now-classic work of Anily [Operations Research, 1991] and Gallego, Queyranne, and Simchi-Levi [Operations Research, 1996] via the highly-structured class of "stationary order sizes and stationary intervals" (SOSI) policies, thereby avoiding direct competition against fully dynamic policies.
  The main contribution of this paper resides in developing new analytical foundations and algorithmic techniques that enable such direct comparisons, leading to the first provable improvement over the $2$-approximation barrier. Leveraging these ideas, we design a constructive approach that allows us to balance cost and capacity at a finer granularity than previously possible via SOSI-based methods. Consequently, given any economic warehouse lot scheduling instance, we present a polynomial-time construction of a random capacity-feasible dynamic policy whose expected long-run average cost is within factor $2-\frac{17}{5000} + ε$ of optimal.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [JAXMg: A multi-GPU linear solver in JAX](https://arxiv.org/abs/2601.14466)
*Roeland Wiersema*

Main category: cs.DC

TL;DR: 该论文与编译器（JIT 编译、XLA）方面相关。
该论文介绍了 JAXMg，这是一个为 JAX 设计的多 GPU 密集线性代数库，它通过 XLA 外部函数接口集成了 NVIDIA 的 cuSOLVERMg，从而在 JAX 工作流中实现了对超出单 GPU 内存限制的大型密集线性系统和特征值问题的可扩展求解。


<details>
  <summary>Details</summary>
Motivation: 科学计算的许多领域需要解决大型密集线性系统和特征值问题。然而，在现代编程框架中，将这些操作扩展到单个 GPU 之外仍然具有挑战性。虽然存在高度优化的多 GPU 求解器库，但它们难以集成到可组合的、即时编译（JIT）的 Python 工作流中。

Method: 通过设计 JAXMg 库，利用 XLA 外部函数接口（Foreign Function Interface, FFI）将 JAX 与 NVIDIA 的 cuSOLVERMg 库连接起来，从而将分布式 GPU 求解器暴露为 JIT 兼容的 JAX 原语。JAXMg 实现了对超出单 GPU 内存限制的矩阵进行基于 Cholesky 的线性求解和对称特征值分解。

Result: JAXMg 成功地为 JAX 提供了多 GPU 密集线性代数能力，实现了对超出单 GPU 内存限制的矩阵的基于 Cholesky 的线性求解和对称特征值分解。这种设计允许可扩展的线性代数直接嵌入到 JAX 程序中，保留了与 JAX 转换的可组合性，并支持在端到端科学工作流中进行多 GPU 执行。

Conclusion: JAXMg 通过 XLA FFI 与 NVIDIA 的 cuSOLVERMg 集成，成功地将多 GPU 分布式线性代数求解器以 JIT 兼容的 JAX 原语形式暴露出来，使得在 JAX 程序中嵌入可扩展的线性代数成为可能，保持了 JAX 转换的可组合性，并在端到端科学工作流中实现了多 GPU 执行。

Abstract: Solving large dense linear systems and eigenvalue problems is a core requirement in many areas of scientific computing, but scaling these operations beyond a single GPU remains challenging within modern programming frameworks. While highly optimized multi-GPU solver libraries exist, they are typically difficult to integrate into composable, just-in-time (JIT) compiled Python workflows. JAXMg provides multi-GPU dense linear algebra for JAX, enabling Cholesky-based linear solves and symmetric eigendecompositions for matrices that exceed single-GPU memory limits. By interfacing JAX with NVIDIA's cuSOLVERMg through an XLA Foreign Function Interface, JAXMg exposes distributed GPU solvers as JIT-compatible JAX primitives. This design allows scalable linear algebra to be embedded directly within JAX programs, preserving composability with JAX transformations and enabling multi-GPU execution in end-to-end scientific workflows.

</details>


### [4] [Exploring Performance-Productivity Trade-offs in AMT Runtimes: A Task Bench Study of Itoyori, ItoyoriFBC, HPX, and MPI](https://arxiv.org/abs/2601.14608)
*Torben R. Lahnor,Mia Reitz,Jonas Posner,Patrick Diehl*

Main category: cs.DC

TL;DR: This paper is related to **Compiler** (specifically, **Runtime Systems** for parallel programming, which fall under the scope of compilers/parallel computing infrastructure). The paper is not directly related to DSL, graph processing, or HLS, and its connection to MLIR is indirect (MLIR is a compiler infrastructure, and this work is focused on runtime systems, which are related compiler technology).
**TLDR:** This paper integrates two new cluster Asynchronous Many-Task (AMT) runtimes, Itoyori and ItoyoriFBC (an extension with future synchronization), into the Task Bench framework. It systematically compares their performance (Application Efficiency, METG) and programmer productivity (LOC, NLC) against MPI and HPX. Results show that MPI is best for regular, low-comm workloads but has low productivity; HPX is stable under load imbalance but worst in productivity; and **Itoyori achieves the highest efficiency in communication-intensive configurations while leading in programmer productivity**. ItoyoriFBC offers potential for irregular workloads via future-based synchronization.


<details>
  <summary>Details</summary>
Motivation: 尽管异步多任务 (AMT) 运行时为并行编程提供了一种富有成效的替代方案，但 AMT 领域的多样性使得公平比较具有挑战性。Task Bench 框架为解决这一挑战提供了参数化评估机制。本文的动机是利用 Task Bench 对两个新兴的集群 AMT 运行时 Itoyori 和 ItoyoriFBC 进行全面评估，并与广泛使用的 MPI 和 AMT 运行时 HPX 进行比较，以揭示不同并行编程系统在性能和编程效率上的权衡。

Method: 作者将两个基于 PGAS 模型（使用 RDMA 工作窃取）的新型集群异步多任务 (AMT) 运行时 Itoyori 和 ItoyoriFBC 集成到 Task Bench 评估框架中，并与传统的 MPI 和 AMT 运行时 HPX 进行比较。评估方法包括：1. **性能评估**：使用计算密集型内核、弱扩展性测试以及不平衡和通信密集型模式进行多配置性能测试。性能指标包括**应用效率**（Application Efficiency）和**最小有效任务粒度**（Minimum Effective Task Granularity, METG）。2. **编程效率评估**：使用**代码行数**（Lines of Code, LOC）和**库结构数量**（Number of Library Constructs, NLC）进行量化。

Result: 1. **MPI**：在常规、通信密集度低的工作负载中应用效率最高，但代码冗长且级别低。
2. **HPX**：在不同节点数量下，负载不平衡情况下效率保持稳定，但在编程效率指标（LOC 和 NLC）上表现最差，证明 AMT 不一定能保证比 MPI 提高生产力。
3. **Itoyori**：在通信密集型配置中实现了最高的应用效率，并在编程效率方面领先。
4. **ItoyoriFBC**：效率略低于 Itoyori，但其基于未来的同步机制（Future-based Synchronization）为表达不规则工作负载提供了潜力。
**总体发现**：不同的运行时系统存在明显的权衡，Itoyori 在性能（尤其是在通信密集型场景）和编程效率之间取得了良好的平衡。

Conclusion: 本文将 Itoyori 和 ItoyoriFBC 这两个新兴的集群异步多任务运行时集成到 Task Bench 框架中，并将其与 MPI 和 HPX 进行了全面的比较评估。评估结果揭示了不同系统之间的性能和编程效率权衡。MPI 在常规、通信密集度低的负载下效率最高但代码冗长；HPX 在负载不平衡下效率稳定但在编程效率方面排末尾。Itoyori 在通信密集型配置中的效率最高，并且在编程效率上具有领先优势，表明其在提供高性能的同时，也大大简化了代码的编写。ItoyoriFBC 效率略低于 Itoyori，但其基于未来的同步机制为表达不规则工作负载提供了潜力。总体而言，Itoyori 是一个很有前景的 AMT 运行时，它在性能和生产力方面提供了一个有吸引力的平衡点，特别适用于通信密集的应用场景。

Abstract: Asynchronous Many-Task (AMT) runtimes offer a productive alternative to the Message Passing Interface (MPI). However, the diverse AMT landscape makes fair comparisons challenging. Task Bench, proposed by Slaughter et al., addresses this challenge through a parameterized framework for evaluating parallel programming systems. This work integrates two recent cluster AMTs, Itoyori and ItoyoriFBC, into Task Bench for comprehensive evaluation against MPI and HPX. Itoyori employs a Partitioned Global Address Space (PGAS) model with RDMA-based work stealing, while ItoyoriFBC extends it with futurebased synchronization.
  We evaluate these systems in terms of both performance and programmer productivity. Performance is assessed across various configurations, including compute-bound kernels, weak scaling, and both imbalanced and communication-intensive patterns. Performance is quantified using application efficiency, i.e., the percentage of maximum performance achieved, and the Minimum Effective Task Granularity (METG), i.e., the smallest task duration before runtime overheads dominate. Programmer productivity is quantified using Lines of Code (LOC) and the Number of Library Constructs (NLC).
  Our results reveal distinct trade-offs. MPI achieves the highest efficiency for regular, communication-light workloads but requires verbose, lowlevel code. HPX maintains stable efficiency under load imbalance across varying node counts, yet ranks last in productivity metrics, demonstrating that AMTs do not inherently guarantee improved productivity over MPI. Itoyori achieves the highest efficiency in communication-intensive configurations while leading in programmer productivity. ItoyoriFBC exhibits slightly lower efficiency than Itoyori, though its future-based synchronization offers potential for expressing irregular workloads.

</details>


### [5] [Exploiting Spot Instances for Time-Critical Cloud Workloads Using Optimal Randomized Strategies](https://arxiv.org/abs/2601.14612)
*Neelkamal Bhuyan,Randeep Bhatia,Murali Kodialam,TV Lakshman*

Main category: cs.DC

TL;DR: 该论文与编译器、HLS、MLIR 或 DSL 无关，但与**图处理（Graph Processing）**无直接关联，而是与**调度算法**和**云计算资源管理**相关，可以归类为**优化/系统**。

**太长不看 (TL;DR):**

本文解决了混合云环境中具有硬期限的在线作业调度问题，目标是在成本效益高但不可靠的**竞价实例**和更昂贵的**按需实例**之间做出选择。作者证明了现有（主要是确定性）策略的最差竞争比界限为 $Ω(K)$（$K$ 为成本比）。他们提出了一种新型随机调度算法 **ROSS**，该算法在合理的期限内实现了可证明的**最优竞争比 $\sqrt{K}$**，显著优于现有方法。ROSS 在真实的 Azure 和 AWS 数据上进行评估，表现出高达 30% 的成本节省优势，同时有效保证了期限。


<details>
  <summary>Details</summary>
Motivation: 在混合云环境中，作业调度面临挑战：如何在利用成本低廉但不稳定的竞价实例（Spot Instances）与保证作业在硬期限内完成之间取得平衡，同时最小化成本。现有的（主要是确定性）调度策略在最坏情况下的性能表现不佳，其竞争比与成本比成正比，未能有效地解决这个问题。

Method: 作者首先在理论上分析了现有（主要是确定性）调度策略的竞争比，证明了其最坏情况下的竞争比为 $Ω(K)$。然后，他们提出了 ROSS（Randomized Online Spot Scheduling）随机调度算法，并理论证明了其在合理期限下的最优竞争比为 $\sqrt{K}$。最后，通过在 Azure 和 AWS 的真实跟踪数据上进行广泛评估，展示了 ROSS 在成本节省和期限保证方面的性能。

Result: 本文取得了以下结果：1) 理论证明了现有（主要是确定性）策略的最差竞争比为 $Ω(K)$，其中 $K$ 是按需实例和竞价实例之间的成本比。2) 提出了一种新的随机调度算法 ROSS，并在合理的期限内证明了其竞争比为最优的 $\sqrt{K}$。3) 基于 Azure 和 AWS 的真实跟踪数据的广泛评估表明，ROSS 有效地平衡了成本优化和期限保证，在成本节省方面始终优于现有最优技术高达 30%。

Conclusion: 本文解决了混合云环境中具有硬期限的作业的在线调度问题。作者证明了现有（主要是确定性）策略的最差竞争比为 $Ω(K)$，其中 $K$ 是按需实例和竞价实例之间的成本比。然后，他们提出了 ROSS 随机调度算法，在合理的期限内实现了可证明的最佳竞争比 $\sqrt{K}$，显著优于现有方法。ROSS 在实际的 Azure 和 AWS 跟踪数据上进行了评估，证明了其在平衡成本优化和期限保证方面的有效性，在各种竞价市场条件下，成本节省方面始终优于现有技术高达 30%。

Abstract: This paper addresses the challenge of deadline-aware online scheduling for jobs in hybrid cloud environments, where jobs may run on either cost-effective but unreliable spot instances or more expensive on-demand instances, under hard deadlines. We first establish a fundamental limit for existing (predominantly-) deterministic policies, proving a worst-case competitive ratio of $Ω(K)$, where $K$ is the cost ratio between on-demand and spot instances. We then present a novel randomized scheduling algorithm, ROSS, that achieves a provably optimal competitive ratio of $\sqrt{K}$ under reasonable deadlines, significantly improving upon existing approaches. Extensive evaluations on real-world trace data from Azure and AWS demonstrate that ROSS effectively balances cost optimization and deadline guarantees, consistently outperforming the state-of-the-art by up to $30\%$ in cost savings, across diverse spot market conditions.

</details>


### [6] [Specifying and Verifying RDMA Synchronisation (Extended Version)](https://arxiv.org/abs/2601.14642)
*Guillaume Ambal,Max Stupple,Brijesh Dongol,Azalea Raad*

Main category: cs.DC

TL;DR: 该论文与**编译器**（涉及程序的正确性验证、同步抽象以及形式化语义）和**图处理**（不直接相关，但RDMA通常用于高性能计算，而高性能计算中常涉及图处理，但抽象内容中未直接提及）不直接相关。论文的核心围绕**RDMA**（Remote Direct Memory Access）的**形式化语义**和**同步抽象**展开，与HLS和MLIR无关。\n\n**总结（TLDR）**: 现有RDMA形式化语义（$\text{RDMA}^\text{TSO}$）缺乏对远程同步（RMW操作）的描述，阻碍了如锁等抽象的验证。本文通过提出**$\text{RDMA}^{\text{TSO}}_{\text{RMW}}$**语义，首次形式化了TSO上的远程RMW操作，并揭示其弱原子性。基于此，构建了**$\text{RDMA}^{\text{WAIT}}_{\text{RMW}}$**等一套可组合的同步抽象，并设计和验证了**三种远程锁**。此外，还提出了强一致性模型**$\text{RDMA}^{\text{SC}}_{\text{RMW}}$**，为RDMA程序的正确性验证和高性能同步提供了形式化基础。


<details>
  <summary>Details</summary>
Motivation: 现有的$\text{RDMA}^\text{TSO}$形式化语义描述了RDMA在TSO CPU上的行为，但缺乏对**远程同步机制**的正式描述（形式化）。这意味着无法验证如锁之类常见抽象的实现，从而阻碍了RDMA程序正确性验证的进一步发展。本文旨在通过形式化远程“读-修改-写”（RMW）操作的语义来填补这一空白，并在此基础上构建和验证可组合的远程同步抽象，以支持高性能RDMA程序的正确性保证。

Method: 本文首先提出并形式化了$\text{RDMA}^{\text{TSO}}_{\text{RMW}}$语义，该语义首次将远程读-修改-写（RMW）操作纳入了对TSO CPU上的RDMA行为的描述中。通过分析该语义，发现远程RMW操作的原子性较弱，仅保证与其他远程RMW操作间的原子性。基于此，本文构建了一套可组合的同步抽象，首先是$\text{RDMA}^{\text{WAIT}}_{\text{RMW}}$库。然后，利用$\text{RDMA}^{\text{WAIT}}_{\text{RMW}}$作为基础，指定、实现并验证了三种适用于不同场景的远程锁。此外，还提出了强RDMA模型$\text{RDMA}^{\text{SC}}_{\text{RMW}}$，类似于共享内存架构中的顺序一致性。文中构建的库与现有的高性能库LOCO兼容，确保了可组合性和可验证性。

Result: 本文主要成果包括：1. **首次形式化了$\text{RDMA}^{\text{TSO}}_{\text{RMW}}$语义**，该语义涵盖了TSO上的远程RMW操作，并揭示了远程RMW操作仅保证与其他远程RMW操作间原子性的弱特性。2. **构建了$\text{RDMA}^{\text{WAIT}}_{\text{RMW}}$同步抽象库**，作为后续同步机制的基础。3. **指定、实现并验证了三种适用于不同场景的远程锁抽象**。4. **提出了强RDMA模型$\text{RDMA}^{\text{SC}}_{\text{RMW}}$**，类似于共享内存的顺序一致性。这些库与现有高性能库LOCO兼容，确保了可组合性和可验证性。

Conclusion: 本文通过提出$\text{RDMA}^{\text{TSO}}_{\text{RMW}}$语义，首次形式化了支持TSO上的远程RMW操作，揭示了远程RMW操作的弱原子性特性。在此基础上，构建了$\text{RDMA}^{\text{WAIT}}_{\text{RMW}}$同步抽象库以及三种远程锁抽象，并通过形式验证保证了其正确性。此外，还提出了强一致性模型$\text{RDMA}^{\text{SC}}_{\text{RMW}}$。这项工作填补了现有RDMA形式化语义中缺乏远程同步机制的空白，为RDMA程序的正确性验证提供了基础，并提供了一个可组合的高性能同步库。

Abstract: Remote direct memory access (RDMA) allows a machine to directly read from and write to the memory of remote machine, enabling high-throughput, low-latency data transfer. Ensuring correctness of RDMA programs has only recently become possible with the formalisation of $\text{RDMA}^\text{TSO}$ semantics (describing the behaviour of RDMA networking over a TSO CPU). However, this semantics currently lacks a formalisation of remote synchronisation, meaning that the implementations of common abstractions such as locks cannot be verified. In this paper, we close this gap by presenting $\text{RDMA}^{\text{TSO}}_{\text{RMW}}$, the first semantics for remote `read-modify-write' (RMW) instructions over TSO. It turns out that remote RMW operations are weak and only ensure atomicity against other remote RMWs. We therefore build a set of composable synchronisation abstractions starting with the $\text{RDMA}^{\text{WAIT}}_{\text{RMW}}$ library. Underpinned by $\text{RDMA}^{\text{WAIT}}_{\text{RMW}}$, we then specify, implement and verify three classes of remote locks that are suitable for different scenarios. Additionally, we develop the notion of a strong RDMA model, $\text{RDMA}^{\text{SC}}_{\text{RMW}}$, which is akin to sequential consistency in shared memory architectures. Our libraries are built to be compatible with an existing set of high-performance libraries called LOCO, which ensures compositionality and verifiability.

</details>


### [7] [Optimizing FaaS Platforms for MCP-enabled Agentic Workflows](https://arxiv.org/abs/2601.14735)
*Varad Kulkarni,Vaibhav Jha,Nikhil Reddy,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 该论文不涉及DSL、图处理、MLIR和编译器，但涉及HLS。
该论文提出了FAME，一种基于FaaS的架构，用于编排支持模型上下文协议（MCP）的自主AI智能体工作流。FAME通过将智能体模式分解为可组合的FaaS函数（Planner, Actor, Evaluator），并通过FaaS工作流进行编排来解决可扩展性和超时问题。它利用DynamoDB进行内存持久化、S3进行缓存并提出函数融合策略来优化状态管理。实验结果表明，与传统方法相比，FAME显著降低了延迟和成本，并减少了输入tokens。


<details>
  <summary>Details</summary>
Motivation: 自主AI智能体工作流正在迅速发展，但传统的虚拟机部署资源密集且缺乏弹性。无服务器函数即服务（FaaS）平台虽然提供了模块化、自动扩展和成本效益，但本质上是无状态的，给要求状态管理和复杂编排的智能体工作流带来了挑战。因此，需要一种基于FaaS的架构来可扩展地编排支持MCP的智能体工作流。

Method: FAME将ReAct等智能体模式分解为可组合的智能体组件（Planner, Actor和Evaluator），每个组件都实现为FaaS函数，并通过LangGraph构建。这些函数通过FaaS工作流（如AWS Step Functions）进行编排，以避免超时。FAME使用DynamoDB实现跨用户请求会话的智能体内存持久化和注入。它还通过AWS Lambda封装优化MCP服务器部署，利用S3缓存工具输出，并提出了函数融合策略。

Result: 在研究论文摘要和日志分析两个应用上的评估结果显示，FAME实现了高达13倍的延迟降低、88%的输入tokens减少，以及66%的成本节约，同时提高了工作流完成率。这证明了使用无服务器平台托管复杂的多智能体AI工作流的可行性。

Conclusion: FAME证明了使用FaaS平台托管复杂的多智能体AI工作流在大规模部署下的可行性。这种方法通过将智能体工作流分解为可组合的FaaS函数，并利用云服务进行状态管理和优化，实现了显著的性能提升和成本节约。

Abstract: Agentic workflows that use autonomous AI Agents powered by Large Language Models (LLMs) and Model Context Protocol (MCP) servers is rapidly rising. This introduces challenges in scalable cloud deployment and state management. Traditional hosting on Virtual Machines (VMs) is resource-intensive and lacks elasticity. Functions-as-a-Service (FaaS) platforms offer modularity, autoscaling and cost efficiency but are inherently stateless. In this paper, we present the FAME, a FaaS-based architecture for orchestrating MCP-enabled agentic workflows. FAME decomposes agentic patterns such as ReAct into composable agents: Planner, Actor and Evaluator, that are each a FaaS function built using LangGraph and are orchestrated as a FaaS workflow. This enables modular composition as AWS Step Functions and avoids function timeouts seen for monolithic agentic workflows. To address context persistence across user requests in a conversation, FAME automates agent memory persistence and injection using DynamoDB. It also optimizes MCP server deployment through AWS Lambda wrappers, caches tool outputs in S3 and proposes function fusion strategies. We evaluate FAME on two representative applications, on research paper summarization and log analytics, under diverse memory and caching configurations. Results show up to 13x latency reduction, 88% fewer input tokens and 66% in cost savings, along with improved workflow completion rates. This demonstrates the viability of serverless platforms for hosting complex, multi-agent AI workflows at scale.

</details>


### [8] [AlertGuardian: Intelligent Alert Life-Cycle Management for Large-scale Cloud Systems](https://arxiv.org/abs/2601.14912)
*Guangba Yu,Genting Mai,Rui Wang,Ruipeng Li,Pengfei Chen,Long Pan,Ruijie Xu*

Main category: cs.DC

TL;DR: 相关性分析：本文与 DSL、图处理、MLIR、编译器、HLS 均不直接相关。但由于其使用了“轻量级图模型”进行“告警去噪”，因此与**图处理**有一定的弱关联性，属于利用图模型进行异常检测和数据优化的应用范畴。
太长不读（TLDR）总结：针对大规模云系统因海量告警导致的告警疲劳问题，Company-X 提出了 AlertGuardian 框架，结合 LLM 和图模型优化告警生命周期。该框架通过图学习去噪、RAG+LLM 生成可执行摘要和多智能体规则优化三个阶段，在真实数据集上实现了 94.8% 的告警减少率和 90.5% 的故障诊断准确率，有效提升了运维效率。


<details>
  <summary>Details</summary>
Motivation: 在超大规模的云系统中，告警在检测异常、确保系统可靠性和用户体验方面至关重要。然而，当前的系统会生成海量的告警，由于告警生命周期管理效率低下，导致了“告警疲劳”（Alert Fatigue），严重降低了运维效率。因此，本文旨在解决这一问题，优化告警生命周期管理。

Method: 本文提出了 AlertGuardian 框架，它结合了大型语言模型（LLMs）和轻量级图模型来优化告警生命周期管理，具体包括三个阶段：1. **告警去噪（Alert Denoise）**：使用带有虚拟噪声的图学习模型来过滤噪声告警。2. **告警摘要（Alert Summary）**：利用检索增强生成（RAG）技术和 LLMs 来创建可操作性的告警摘要。3. **告警规则优化（Alert Rule Refinement）**：利用多智能体迭代反馈机制来提高告警规则的质量。

Result: AlertGuardian 框架在 Company-X 的四个真实世界数据集上进行了评估，结果显示：1. **显著减轻告警疲劳**：告警减少率达到 94.8%。2. **加速故障诊断**：诊断准确率达到 90.5%。3. **改进告警规则**：共改进了 1,174 条告警规则，其中 375 条被 SREs 接受（32% 的接受率）。4. **成功部署实践**：AlertGuardian 已在 Company-X 部署，并分享了相关的成功经验和教训。

Conclusion: AlertGuardian 成功部署于 Company-X 的实时服务中，显著减轻了告警疲劳（告警减少率达到 94.8%），并提高了故障诊断效率（诊断准确率达到 90.5%）。此外，它还改进了 1,174 条告警规则，其中 375 条被 SREs 采纳（采纳率为 32%）。作者分享了 AlertGuardian 部署后的成功经验和教训，证明了该框架在优化云系统的告警生命周期管理中是有效和实用的。

Abstract: Alerts are critical for detecting anomalies in large-scale cloud systems, ensuring reliability and user experience. However, current systems generate overwhelming volumes of alerts, degrading operational efficiency due to ineffective alert life-cycle management. This paper details the efforts of Company-X to optimize alert life-cycle management, addressing alert fatigue in cloud systems. We propose AlertGuardian, a framework collaborating large language models (LLMs) and lightweight graph models to optimize the alert life-cycle through three phases: Alert Denoise uses graph learning model with virtual noise to filter noise, Alert Summary employs Retrieval Augmented Generation (RAG) with LLMs to create actionable summary, and Alert Rule Refinement leverages multi-agent iterative feedbacks to improve alert rule quality. Evaluated on four real-world datasets from Company-X's services, AlertGuardian significantly mitigates alert fatigue (94.8\% alert reduction ratios) and accelerates fault diagnosis (90.5\% diagnosis accuracy). Moreover, AlertGuardian improves 1,174 alert rules, with 375 accepted by SREs (32% acceptance rate). Finally, we share success stories and lessons learned about alert life-cycle management after the deployment of AlertGuardian in Company-X.

</details>


### [9] [Application-level observability for adaptive Edge to Cloud continuum systems](https://arxiv.org/abs/2601.14923)
*Kaddour Sidi,Daniel Balouek,Baptiste Jonglez*

Main category: cs.DC

TL;DR: 该论文与编译器、HLS、MLIR、DSL或图处理不相关。
**TL;DR**：现代Edge-to-Cloud (E2C) 系统需要细粒度可观测性以确保性能目标和服务水平目标（SLO）的达标。本文介绍了一个应用级的可观测性框架，它结合了OpenTelemetry、Prometheus、K3s和Chaos Mesh，集成了开发者驱动的插桩和SLO感知的反馈机制，以实现运行时自主适应和控制。通过一个视频处理的应用案例，该框架展示了在可变负载和故障注入下的应用级指标，能够指导自动调整，从而提高了系统的可扩展性、容错性和响应能力，为构建自适应、SLO兼容的E2C应用提供了实用基础。


<details>
  <summary>Details</summary>
Motivation: 现代的Edge-to-Cloud（E2C）系统要求细粒度的可观测性，目的是在异构和动态的环境中，确保自适应行为以及遵守性能目标（SLOs）。

Method: 这项工作介绍了一个应用级别的可观测性框架，它整合了开发者驱动的插桩（instrumentation）和SLO感知的反馈机制，以实现自主适应。具体实施上，它结合了OpenTelemetry、Prometheus、K3s和Chaos Mesh，从而在Edge-to-Cloud连续体中实现了实时监控和自适应控制。

Result: 一个视频处理的用例展示了该框架如何利用应用级别的指标，在可变工作负载和注入故障的情况下，自动调整以维持目标帧率、延迟和检测精度。初步结果表明，该框架提高了系统的可扩展性、容错性和响应能力。

Conclusion: 该框架为自适应、符合SLO的E2C应用奠定了实用基础，并在视频处理用例中展示了它对可扩展性、容错性和响应能力的提升。这项工作通过在E2C系统中集成开发者驱动的插桩和SLO感知的反馈，实现了细粒度的可观测性，从而支持了异构和动态环境下的自主适应行为和性能目标的达成。

Abstract: Modern Edge-to-Cloud (E2C) systems require fine-grained observability to ensure adaptive behavior and compliance with performance objectives across heterogeneous and dynamic environments. This work introduces an application-level observability framework that integrates developer-driven instrumentation and SLO-aware feedback for autonomous adaptation. By combining OpenTelemetry, Prometheus, K3s, and Chaos Mesh, the framework enables real-time monitoring and adaptive control across the continuum. A video processing use case demonstrates how application-level metrics guide automatic adjustments to maintain target frame rate, latency, and detection accuracy under variable workloads and injected faults. Preliminary results highlight improved scalability, fault tolerance, and responsiveness, providing a practical foundation for adaptive, SLO-compliant E2C applications.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [End-to-End Transformer Acceleration Through Processing-in-Memory Architectures](https://arxiv.org/abs/2601.14260)
*Xiaoxuan Yang,Peilin Chen,Tergel Molom-Ochir,Yiran Chen*

Main category: cs.AR

TL;DR: 该论文涉及 **编译器** 和 **HLS**（硬件加速器设计，特别是内存处理）。
**太长不看（TLDR）:** Transformer模型部署面临计算开销、KV缓存内存爆炸和注意力二次复杂度三大挑战。本文提出基于内存处理（PIM）的解决方案，通过重构计算减少数据传输、动态压缩KV缓存管理内存，并将注意力视为关联记忆操作以降低复杂度。实验结果显示，PIM设计在能效和延迟上显著优于现有加速器和GPU，实现了Transformer模型的高效端到端加速。


<details>
  <summary>Details</summary>
Motivation: Transformer模型（及其在自然语言处理和大型语言模型中的应用）的部署面临三大挑战：1. **高延迟和能耗：** 注意力机制涉及大规模矩阵乘法和大量的中间结果在内存与计算单元间的移动。2. **内存和带宽瓶颈：** 在长上下文推理中，键值缓存（KV cache）的规模会不可预测地增长，甚至超过模型权重大小。3. **低效：** 注意力机制的计算复杂度是序列长度的二次方，加剧了数据传输和计算开销。本文旨在通过引入内存处理解决方案来解决这些问题。

Method: 本文提出的方法是基于内存处理（PIM）技术，具体包括：1. 重构注意力（Attention）和前馈（Feed-Forward）计算，以最小化片外数据传输。2. 动态压缩和修剪（prune）键值缓存（KV cache），以管理内存增长。3. 将注意力重新解释为关联记忆操作（associative memory operation），以降低复杂性和硬件足迹。最后，通过与现有加速器和通用GPU进行比较评估。

Result: 本文提出的内存处理（PIM）设计在能效和延迟方面比现有最先进的加速器和通用GPU有显著提高。该方法解决了计算开销、内存可伸缩性和注意力复杂性等问题，实现了Transformer模型的端到端高效加速。

Conclusion: 本文提出的内存处理（PIM）解决方案通过重构Transformer的计算、动态压缩和修剪KV缓存，以及将注意力重新解释为关联记忆操作，解决了Transformer部署中的三大难题：计算开销、内存可伸缩性和注意力复杂性。实验结果表明，该设计在能效和延迟方面优于现有的加速器和通用GPU，实现了Transformer模型的端到端高效加速。

Abstract: Transformers have become central to natural language processing and large language models, but their deployment at scale faces three major challenges. First, the attention mechanism requires massive matrix multiplications and frequent movement of intermediate results between memory and compute units, leading to high latency and energy costs. Second, in long-context inference, the key-value cache (KV cache) can grow unpredictably and even surpass the model's weight size, creating severe memory and bandwidth bottlenecks. Third, the quadratic complexity of attention with respect to sequence length amplifies both data movement and compute overhead, making large-scale inference inefficient. To address these issues, this work introduces processing-in-memory solutions that restructure attention and feed-forward computation to minimize off-chip data transfers, dynamically compress and prune the KV cache to manage memory growth, and reinterpret attention as an associative memory operation to reduce complexity and hardware footprint. Moreover, we evaluate our processing-in-memory design against state-of-the-art accelerators and general-purpose GPUs, demonstrating significant improvements in energy efficiency and latency. Together, these approaches address computation overhead, memory scalability, and attention complexity, further enabling efficient, end-to-end acceleration of Transformer models.

</details>


### [11] [Multi-Partner Project: COIN-3D -- Collaborative Innovation in 3D VLSI Reliability](https://arxiv.org/abs/2601.14347)
*George Rafael Gourdoumanis,Fotoini Oikonomou,Maria Pantazi-Kypraiou,Pavlos Stoikos,Olympia Axelou,Athanasios Tziouvaras,Georgios Karakonstantis,Tahani Aladwani,Christos Anagnostopoulos,Yixian Shen,Anuj Pathania,Alberto Garcia-Ortiz,George Floros*

Main category: cs.AR

TL;DR: 该论文与编译器、HLS、图处理、MLIR、DSL均**不直接相关**，它专注于**VLSI系统可靠性和EDA工具**。
太长不看：随着半导体工艺（如从FinFET到GAAFET，从3nm到亚纳米）复杂性增加，以及3D芯片组设计范式的兴起，2.5D/3D VLSI系统的可靠性评估成为关键挑战。本文介绍了欧盟COIN-3D项目，旨在通过欧洲领先机构合作，提供新颖的**开源EDA工具**，整合**物理和系统级可靠性算法**，以加强对3D异构系统可靠性的研究和评估。


<details>
  <summary>Details</summary>
Motivation: 随着半导体制造技术向3nm以下和亚纳米级发展，并从FinFETs过渡到GAAFETs，制造复杂性和挑战日益增加。大型单片设计的良率较低，因此，基于3D芯片组（Chiplet）的方法应运而生，它允许模块化设计和使用不同技术节点制造的异构系统（如CPU、GPU、内存），这种方法是一种有能力且具有成本效益的异构系统设计策略。然而，这种复杂性和异构性也对2.5D/3D VLSI系统的可靠性评估提出了挑战。因此，本文的动机在于提出一个旨在加强这方面研究并提供开源EDA工具的项目，以应对这些日益复杂的可靠性挑战。

Method: 本文主要通过介绍COIN-3D项目（3D超大规模集成电路可靠性协同创新）及其主要科学目标来提出解决方案。该项目的核心方法是：1. 致力于加强2.5D/3D VLSI系统可靠性方面的研究。2. 提供新颖的开源EDA工具，用于3D系统的可靠性评估。3. 整合先进的物理和系统级可靠性分析算法。这些方法旨在解决随着半导体制造（从3nm到亚纳米，从FinFET到GAAFET）的推进，以及向3D芯片组过渡所带来的可靠性和复杂性挑战。

Result: 本文的结果是介绍了COIN-3D项目，这是一个旨在加强欧洲在2.5D/3D超大规模集成电路（VLSI）系统可靠性研究方面卓越性的合作项目。它明确了项目的关键产出：提供新颖的开源电子设计自动化（EDA）工具，用于3D系统的可靠性评估，并将先进的算法整合到物理和系统级别的可靠性分析中。这标志着对先进半导体复杂性带来的可靠性挑战的积极响应和投入。

Conclusion: 本文介绍了欧盟“地平线欧洲双胞胎”项目COIN-3D（3D超大规模集成电路可靠性协同创新），该项目旨在通过欧洲领先机构之间的合作，加强2.5D/3D超大规模集成电路系统可靠性方面的研究。项目的关键科学目标是提供新颖的开源EDA工具，用于3D系统的可靠性评估，并整合先进的物理和系统级可靠性分析算法。因此，COIN-3D项目致力于解决先进半导体制造中日益复杂的可靠性挑战，为未来异构3D系统设计提供开源工具和方法。

Abstract: As semiconductor manufacturing advances from the 3-nm process toward the sub-nanometer regime and transitions from FinFETs to gate-all-around field-effect transistors (GAAFETs), the resulting complexity and manufacturing challenges continue to increase. In this context, 3D chiplet-based approaches have emerged as key enablers to address these limitations while exploiting the expanded design space. Specifically, chiplets help address the lower yields typically associated with large monolithic designs. This paradigm enables the modular design of heterogeneous systems consisting of multiple chiplets (e.g., CPUs, GPUs, memory) fabricated using different technology nodes and processes. Consequently, it offers a capable and cost-effective strategy for designing heterogeneous systems. This paper introduces the Horizon Europe Twinning project COIN-3D (Collaborative Innovation in 3D VLSI Reliability), which aims to strengthen research excellence in 2.5D/3D VLSI systems reliability through collaboration between leading European institutions. More specifically, our primary scientific goal is the provision of novel open-source Electronic Design Automation (EDA) tools for reliability assessment of 3D systems, integrating advanced algorithms for physical- and system-level reliability analysis.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [12] [DeGAS: Gradient-Based Optimization of Probabilistic Programs without Sampling](https://arxiv.org/abs/2601.15167)
*Francesca Randone,Romina Doz,Mirco Tribastone,Luca Bortolussi*

Main category: cs.PL

TL;DR: 该论文与**MLIR**、**编译器**、**HLS**、**DSL**、**图处理**无关。 它是一篇关于**概率编程**（Probabilistic Programming）中**优化方法**的论文。
**太长不看 (TL;DR)**: DeGAS提出了一种针对无循环概率程序的可微分高斯近似语义，它允许在不需要采样（sample-free）的情况下，对同时包含连续和离散组件的模型进行基于梯度的优化。DeGAS通过高斯混合模型和消失平滑技术，实现了后验和路径概率的闭合形式表达和可微分性，实验证明其在准确性和效率上与现有方法相当，并能解决采样方法在连续变量条件化下的收敛难题。


<details>
  <summary>Details</summary>
Motivation: 现有的概率程序优化方法主要依赖于采样（如MCMC和变分推断）。然而，当模型中同时存在连续和离散组件，特别是涉及到连续变量的条件化时，基于采样的基线方法往往难以收敛或效率低下。因此，需要一种**不依赖采样**、**基于梯度**的优化方法，能够有效处理具有连续和离散组件的概率程序。

Method: DeGAS的核心方法是提出一种可微分的高斯近似语义（Gaussian Approximate Semantics）来处理无循环的概率程序。它将程序评估置于高斯混合语义下，并使用一个“正在消失的平滑”（vanishing smoothing）技术来替换测度为零的谓词和离散分支。这使得后验和路径概率能够以闭合形式表达，并且这些表达式对程序参数是可微分的。这种设计允许通过标准的自动微分进行端到端优化，无需蒙特卡洛估计器。

Result: 在十三个基准程序上，DeGAS的准确性（Accuracy）和运行时间（Runtime）与变分推断（Variational Inference）和MCMC（Markov Chain Monte Carlo）具有竞争力。更重要的是，DeGAS能够可靠地解决那些由于涉及连续变量条件化而导致基于采样的基线方法无法收敛的优化问题。

Conclusion: DeGAS是一种针对无循环概率程序的、可微分的高斯近似语义。它通过高斯混合模型来评估程序，用平滑技术处理测度为零的谓词和离散分支，从而在不需要采样的情况下，对包含连续和离散组件的模型进行基于梯度的优化。实验证明，DeGAS在优化准确性和运行时间上与变分推断和MCMC具有竞争力，并且能够可靠地解决涉及连续变量条件化等采样方法难以收敛的优化问题。

Abstract: We present DeGAS, a differentiable Gaussian approximate semantics for loopless probabilistic programs that enables sample-free, gradient-based optimization in models with both continuous and discrete components. DeGAS evaluates programs under a Gaussian-mixture semantics and replaces measure-zero predicates and discrete branches with a vanishing smoothing, yielding closed-form expressions for posterior and path probabilities. We prove differentiability of these quantities with respect to program parameters, enabling end-to-end optimization via standard automatic differentiation, without Monte Carlo estimators. On thirteen benchmark programs, DeGAS achieves accuracy and runtime competitive with variational inference and MCMC. Importantly, it reliably tackles optimization problems where sampling-based baselines fail to converge due to conditioning involving continuous variables.

</details>


### [13] [Contextual Metaprogramming for Session Types](https://arxiv.org/abs/2601.15180)
*Pedro Ângelo,Atsushi Igarashi,Yuito Murase,Vasco T. Vasconcelos*

Main category: cs.PL

TL;DR: 该论文与编译器相关，涉及类型系统和元编程的集成。 太长不看：本文将阶段式元编程集成到具有会话类型消息传递的函数式语言中，基于多层上下文的上下文模态类型理论，支持代码的动态生成、传输和执行。提出了一个区分线性和非受限资源的类型系统及其类型检查器，并从理论上证明了其正确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 将阶段式元编程集成到具有会话类型消息传递的函数式语言中，以支持实现如服务器准备并通过会话类型消息按需发送代码等实际用例。

Method: 本文方法基于具有多层上下文的上下文模态类型理论模型，将阶段式元编程集成到具有会话类型消息传递的函数式语言中。提出了一个区分线性资源（使用恰好一次）和非受限资源（使用次数不限）的类型系统，并设计了一个适用于具体实现的类型检查器。理论证明了类型保持性、序贯计算的进展结果和并发运行环境中运行时错误的缺失，以及了类型检查器的正确性。

Result: 提出了一个集成阶段式元编程到会话类型消息传递函数式语言的系统，基于上下文模态类型理论和多层上下文。系统允许上下文值（在变量序列上闭合的项）被装箱、通过消息传输、接收、拆箱、本地应用和运行。设计了一个区分线性（使用一次）和非受限（使用多次）资源的类型系统和类型检查器，并证明了类型保持性、序贯计算的进展和并发运行时环境中运行时错误的缺失，以及了类型检查器的正确性。

Conclusion: 本文提出了将阶段式元编程集成到具有会话类型消息传递的函数式语言中。该系统建立在具有多层上下文的上下文模态类型理论模型上，其中上下文值（表示在变量序列上闭合的任意项）可以被装箱并通过消息传输。接收后，这些值可以被拆箱并在本地应用后运行。通过区分线性（使用恰好一次））和非受限（使用次数不限）资源，本文提出了一个类型系统及其类型检查器，并证明了类型保持性、序贯计算的进展以及并发运行环境中的运行时错误缺失，以及类型检查器的正确性。

Abstract: We propose the integration of staged metaprogramming into a session-typed message passing functional language. We build on a model of contextual modal type theory with multi-level contexts, where contextual values, closing arbitrary terms over a series of variables, may be boxed and transmitted in messages. Once received, one such value may then be unboxed and locally applied before being run. To motivate this integration, we present examples of real-world use cases, for which our system would be suitable, such as servers preparing and shipping code on demand via session typed messages. We present a type system that distinguishes linear (used exactly once) from unrestricted (used an unbounded number of times) resources, and further define a type checker, suitable for a concrete implementation. We show type preservation, a progress result for sequential computations and absence of runtime errors for the concurrent runtime environment, as well as the correctness of the type checker.

</details>
