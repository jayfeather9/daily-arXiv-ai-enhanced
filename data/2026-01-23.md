<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Remarks on Algebraic Reconstruction of Types and Effects](https://arxiv.org/abs/2601.15455)
*Patrycja Balik,Szymon Jędras,Piotr Polesiuk*

Main category: cs.PL

TL;DR: 该论文与编译器和静态分析（类型和效应系统）相关。本文分析了 Jouvelot 和 Gifford 1991 年关于代数重建类型和效应的开创性工作，指出了该原始算法在处理高阶多态性时，在变量绑定方面存在的微妙错误。


<details>
  <summary>Details</summary>
Motivation: Jouvelot 和 Gifford 在 1991 年提出的类型和效应重建算法是静态分析领域的里程碑工作，但它处理了具有挑战性的高阶多态性特性。本文的动机是希望识别并描述该原始算法中与高阶多态性相关的变量绑定处理中存在的微妙错误，以提供必要的修正和澄清。

Method: 作者重新审视了 Jouvelot 和 Gifford 1991 年论文中的类型系统和重建算法，重点分析了他们处理高阶多态性（higher-rank polymorphism）的方法以及其中涉及的变量绑定（variable binding）机制，并具体描述了发现的错误。

Result: 作者识别出并描述了 Jouvelot 和 Gifford 1991 年算法中与处理高阶多态性特性相关的变量绑定中的微妙错误（subtle bugs），这影响了该特性上的正确实现。

Conclusion: 本文重新审视了 Jouvelot 和 Gifford 1991 年关于类型和效应代数重建的开创性工作，并指出了其中与高阶多态性相关的变量绑定处理中存在的微妙错误。这证实了原始算法的复杂性，并为静态分析社区提供了必要的修正。

Abstract: In their 1991 paper "Algebraic Reconstruction of Types and Effects," Pierre Jouvelot and David Gifford presented a type-and-effect reconstruction algorithm based on an algebraic structure of effects. Their work is considered a milestone in the development of type-and-effect systems, and has inspired numerous subsequent works in the area of static analysis. However, unlike the later research it spawned, the original algorithm considered a language with higher-rank polymorphism, a feature which is challenging to implement correctly. In this note, we identify subtle bugs related to variable binding in their approach to this feature. We revisit their type system and reconstruction algorithm, and describe the discovered issues.

</details>


### [2] [Prioritizing Configuration Relevance via Compiler-Based Refined Feature Ranking](https://arxiv.org/abs/2601.16008)
*Federico Bruzzone,Walter Cazzola,Luca Favini*

Main category: cs.PL

TL;DR: 该论文与 DSL、图处理、MLIR、编译器、HLS 相关。具体来说，它与**编译器**相关，因为它是一个“编译器级方法”，通过对 Rust 编译器进行插桩实现，并涉及从编译器中提取**中间表示（IR）**；它与**图处理**相关，因为它构建了“两个互补的基于图的数据结构”，并使用“中心性度量”进行特征排序。

总结（TLDR）：现代 Rust 等语言支持高度可配置系统，但配置组合爆炸使程序分析、优化和测试面临挑战。本文提出了首个**编译器级的配置优先级排序方法**（名为 RustyEx）。该方法通过从 Rust 编译器中提取定制 IR，构建基于图的数据结构，并使用**中心性度量**来对特征进行排名，最终生成最相关的配置集，并利用 SAT 求解器保证有效性。实证结果证明了该方法能够高效且实用地探索大型配置空间，为配置感知分析和优化铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 现代编程语言（尤其是 Rust）提供的先进语言结构，允许通过聚合特征（features）构建高度可配置的软件系统，这些系统由配置（configuration）来标识。然而，配置的组合爆炸（combinatorial explosion）使得程序的穷举探索（分析、优化和测试）变得不可行，构成了巨大的挑战。因此，需要一种有效的方法来对这些配置进行优先级排序，以在有限资源内进行高效探索。

Method: 本文提出了一种编译器级的配置优先级排序方法，主要包括四个步骤：1. 从 Rust 编译器中提取定制的中间表示（IR）；2. 构建两个互补的基于图的数据结构；3. 利用中心性度量（centrality measures）对特征（features）进行排序；4. 通过考虑特征影响的代码范围来精化排序。最后，基于排序后的特征生成固定数量的最相关配置，并利用 SAT 求解器（将图的表示转化为合取范式）来保证生成配置的有效性。作者将此方法形式化并实现了一个名为 RustyEx 的原型系统。

Result: 在对高排名的开源 Rust 项目的实证评估中，RustyEx 能够有效地在限定的资源内生成用户指定数量的配置集，并能通过构造确保结果的可靠性（soundness）。结果证明，中心性引导的配置优先级排序方法能够实现对大型配置空间的有效和实用的探索，为配置感知分析和优化奠定了基础。

Conclusion: 本文提出的 RustyEx 原型系统通过在 Rust 编译器中进行插桩实现，并在高排名的开源 Rust 项目上进行了实证评估。结果表明，基于中心性度量的配置优先级排序能够有效地、实用性地探索大型配置空间，并确保了结果的可靠性，为未来配置感知分析和优化研究奠定了基础。其核心贡献在于提供了一种解决配置组合爆炸问题的编译器级方法，从而提高了程序分析、优化和测试的效率和实用性。

Abstract: Modern programming languages, most notably Rust, offer advanced linguistic constructs for building highly configurable software systems as aggregation of features -- identified by a configuration. However, they pose substantial challenges for program analysis, optimization, and testing, as the combinatorial explosion of configurations often makes exhaustive exploration infeasible. In this manuscript, we present the first compiler-based method for prioritizing configurations. Our approach consists of four main steps: 1. extracting a tailored intermediate representation from the Rust compiler, 2. constructing two complementary graph-based data structures, 3. using centrality measures to rank features, and 4. refining the ranking by considering the extent of code they impact. A fixed number of most relevant configurations are generated based on the achieved feature ranking. The validity of the generated configurations is guaranteed by using a SAT solver that takes a representation of this graph in conjunctive normal form. We formalized this approach and implemented it in a prototype, RustyEx, by instrumenting the Rust compiler. An empirical evaluation on higher-ranked open source Rust projects shows that RustyEx efficiently generates user-specified sets of configurations within bounded resources, while ensuring soundness by construction. The results demonstrate that centrality-guided configuration prioritization enables effective and practical exploration of large configuration spaces, paving the way for future research in configuration-aware analysis and optimization.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [3] [Nested and outlier embeddings into trees](https://arxiv.org/abs/2601.15470)
*Shuchi Chawla,Kristin Sheridan*

Main category: cs.DS

TL;DR: 该论文与图处理（Graph Processing，因为HST和度量空间嵌入常用于图问题的近似）、编译器、MLIR、HLS、DSL无关。
总结：本文研究了将度量空间嵌入到分层分隔树（HSTs）和超度量空间（ultrametrics）的“异常点嵌入”（outlier embeddings）问题。主要贡献是提出了一种高效的算法，能够在保证异常点集大小接近最优（$k$）且失真可控（$(32+\epsilon)c$）的情况下，将度量空间概率性地嵌入到HST中。这一新颖的概率嵌套嵌入方法能够为特定实例的“批量购买”和“预约乘车”等优化问题提供更优的近似解。


<details>
  <summary>Details</summary>
Motivation: 现有的最佳近似算法（特别是针对“批量购买”和“预约乘车”等问题）通常依赖于将度量空间嵌入到分层分隔树（HSTs）中。然而，对于某些更具挑战性的度量空间实例，直接使用传统的HST嵌入可能效果不佳。
本文的研究动机在于：
1. **提升特定实例的近似性能：** 通过允许少量“异常点”（outliers），可以实现对剩余大部分数据的更低失真嵌入，从而为那些对HST嵌入敏感的问题（如buy-at-bulk和dial-a-ride）提供更好的实例特定近似算法。
2. **理论扩展：** 扩展和发展嵌套嵌入（nested embeddings）的概念，特别是将其从确定性设置推广到更强大的概率性嵌入设置中。

Method: 本文的主要方法是：
1. **定义和研究异常点嵌入（Outlier Embeddings）：** 寻找最小的异常点集$k$，使得其余点集可概率性地低失真地嵌入到HST中。
2. **扩展嵌套嵌入（Nested Embeddings）概念：** 将[Chawla and Sheridan 2024]提出的确定性嵌套嵌入扩展到概率性嵌入的设置中。嵌套嵌入是将度量空间分解为低失真子集和较高失真其余部分，并以不显著增加整体失真的方式进行组合。
3. **算法设计和组合：** 设计了高效的算法，用于从低失真目标$c$的概率嵌入中进行采样，保证异常点数量在$O(\frac k \epsilon \log^2 k)$以内，失真控制在$(32+\epsilon)c$以内。
4. **利用现有工作：** 结合了[Munagala et al. 2023]的近似算法来实现最终结果。

Result: 主要结果是一个高效的算法：
1. **输入：** 度量空间 $(X, d)$ 和目标失真 $c$。
2. **输出：** 一个概率性嵌入的采样，其具有以下特性：
    * **异常点数量（Outliers）：** 异常点数量最多为 $O(\frac k \epsilon \log^2 k)$，其中 $k$ 是实现目标失真 $c$ 所需的最小异常点集大小。
    * **失真（Distortion）：** 嵌入失真最多为 $(32 + \epsilon)c$，其中 $\epsilon > 0$ 是任意小的正数。
3. **应用：** 这一结果直接为某些特定实例的“批量购买”（buy-at-bulk）和“预约乘车”（dial-a-ride）问题带来了比现有通过HST嵌入得到的更好的近似算法。

Conclusion: 本文的关键在于引入了“异常点嵌入”（outlier embeddings）和“嵌套嵌入”（nested embeddings）的概率版本，并证明了存在一种高效的算法，能够在可控的异常点数量和有限的失真增加下，将度量空间概率性地嵌入到HST中。该研究不仅具有理论意义（扩展了嵌套嵌入的概念），而且在实际应用中，能为某些特定实例的“批量购买”（buy-at-bulk）和“预约乘车”（dial-a-ride）问题提供更好的近似算法。

Abstract: In this paper, we consider outlier embeddings into HSTs and ultrametrics. In particular, for $(X,d)$, let $k$ be the size of the smallest subset of $X$ such that all but that subset (i.e. the ``outlier set'') can be probabilistically embedded into the space of HSTs with expected distortion at most $c$. Our primary result is showing that there exists an efficient algorithm that takes in $(X,d)$ and a target distortion $c$ and samples from a probabilistic embedding with at most $O(\frac k ε\log^2k)$ outliers and distortion at most $(32+ε)c$, for any $ε>0$. This leads to better instance-specific approximations for certain instances of the buy-at-bulk and dial-a-ride problems, whose current best approximation algorithms go through HST embeddings.
  In order to facilitate our results, we largely focus on the concept of compositions of nested embeddings introduced by [Chawla and Sheridan 2024]. A nested embedding is a composition of two embeddings of a metric space $(X,d)$ -- a low distortion embedding of a subset $S$ of nodes, and a higher distortion embedding of the entire metric. The composition is a single embedding that preserves the low distortion over $S$ and does not increase distortion over the remaining points by much. In this paper, we expand this concept from the setting of deterministic embeddings to the setting of probabilistic embeddings. We show how to find good nested compositions of embeddings into HSTs, and combine this with an approximation algorithm of [Munagala et al. 2023] to obtain our results.

</details>


### [4] [Tight Bounds for Gaussian Mean Estimation under Personalized Differential Privacy](https://arxiv.org/abs/2601.15682)
*Wei Dong,Li Ge*

Main category: cs.DS

TL;DR: 该论文与差分隐私（一种隐私保护技术，常用于数据分析和机器学习等领域，可以被认为是广义的编译器相关的技术）相关。本文研究了在高斯分布的个性化差分隐私（PDP）下的均值估计问题，针对有界和无界PDP两种变体，提出了渐进最优的估计器。首先，文章推导了相应的均值估计下界，然后设计了匹配这些下界的（对数因子内）的算法上界，解决了高斯分布无界支持带来的数据值和隐私信息需要联合考虑的挑战，以及无界PDP下隐私信息受保护导致的权重计算不清晰的问题。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是研究在高斯分布下进行均值估计的个性化差分隐私（PDP）问题。现有的工作主要集中在有界分布和有界PDP设置上，忽略了高斯分布（具有无界支持）的挑战，即数据元素贡献差异大，需要同时考虑隐私信息和数据值。此外，无界PDP设置下，隐私信息受到保护，使得问题更具挑战性，目前缺乏适用于这两种情况下的最优估计器。

Method: 本文首先为高斯分布在有界和无界PDP下的均值估计问题推导了相应的下界。然后，提出了针对这两种设置的PDP均值估计器，证明了这些算法的上界与下界在对数因子内是匹配的，从而证明了所提方法的最优性。解决高斯分布无界支持带来的挑战是通过联合考虑隐私信息和数据值。解决无界PDP下隐私信息受保护、权重计算不清晰的挑战是通过设计合适的估计器。

Result: 本文在有界和无界PDP设置下，为高斯分布的均值估计提出了最优估计器。这些估计器在各自设置下的算法上界与对应的下界在对数因子内是匹配的，表明了所提方法的渐进最优性。文章成功解决了高斯分布无界支持带来的数据值和私密信息联合考量的问题，以及无界PDP下隐私信息保护所导致的权重计算问题。

Conclusion: 本文研究了在高斯分布的个性化差分隐私（PDP）下的均值估计，并提出了在有界和无界PDP设置下的最优估计器。通过推导相应的下界，并设计算法使其上界与下界（对数因子内）匹配，证明了所提方法的有效性和最优性。

Abstract: We study mean estimation for Gaussian distributions under \textit{personalized differential privacy} (PDP), where each record has its own privacy budget. PDP is commonly considered in two variants: \textit{bounded} and \textit{unbounded} PDP. In bounded PDP, the privacy budgets are public and neighboring datasets differ by replacing one record. In unbounded PDP, neighboring datasets differ by adding or removing a record; consequently, an algorithm must additionally protect participation information, making both the dataset size and the privacy profile sensitive. Existing works have only studied mean estimation over bounded distributions under bounded PDP. Different from mean estimation for distributions with bounded range, where each element can be treated equally and we only need to consider the privacy diversity of elements, the challenge for Gaussian is that, elements can have very different contributions due to the unbounded support. we need to jointly consider the privacy information and the data values. Such a problem becomes even more challenging under unbounded PDP, where the privacy information is protected and the way to compute the weights becomes unclear. In this paper, we address these challenges by proposing optimal Gaussian mean estimators under both bounded and unbounded PDP, where in each setting we first derive lower bounds for both problems, following PDP mean estimators with the algorithmic upper bounds matching the corresponding lower bounds up to logarithmic factors.

</details>


### [5] [Improved Approximation Ratios for the Shortest Common Superstring Problem with Reverse Complements](https://arxiv.org/abs/2601.15814)
*Ryosuke Yamano,Tetsuo Shibuya*

Main category: cs.DS

TL;DR: 这不是一个与 DSL、图处理、MLIR、编译器或 HLS 相关的计算机系统领域论文。
最短公共超串问题（SCS）及其带逆补的变体（SCS-RC）在生物信息学中有重要应用。本文对用于标准 SCS 的 MGREEDY 和 TGREEDY 算法在 SCS-RC 上的扩展进行了严格的近似比分析，证明它们可以分别达到 3.75 和 2.875 近似比。2.875 近似算法是 SCS-RC 问题目前已知的最佳界限，是首次正式改进该问题的近似保证。


<details>
  <summary>Details</summary>
Motivation: 最短公共超串问题（SCS）及其带逆补的变体（SCS-RC）是生物信息学应用中的关键问题。对于 SCS-RC，先前的工作提出了 4 近似的 MGREEDY 扩展，并推测 TGREEDY 扩展可能达到 3 近似，但缺乏严格证明。本文的动机是正是要为 SCS-RC 问题的近似算法提供正式改进和更严格的近似比保证。

Method: 本文通过扩展标准最短公共超串（SCS）问题的 MGREEDY 和 TGREEDY 算法来解决带逆补的最短公共超串（SCS-RC）问题。核心方法是对这些扩展算法的近似比进行严格的数学分析和证明。特别地，分析扩展了处理逆补引入的双向重叠部分的经典证明方法。

Result: 本文证明了：1. 扩展至 SCS-RC 的 MGREEDY 算法达到了 3.75 的近似比。2. 扩展至 SCS-RC 的 TGREEDY 算法达到了 2.875 的近似比。这两种算法的结果代表了 SCS-RC 问题的首次正式近似保证改进，其中 2.875 是目前已知的最佳近似比。

Conclusion: 本文通过对 MGREEDY 和 TGREEDY 算法在 SCS-RC 问题上的扩展进行严格分析，证明了它们分别能达到 3.75 和 2.875 的近似比，首次正式改进了 SCS-RC 问题的近似保证。2.875 近似算法是目前已知该问题的最佳上界。这一研究通过处理逆补引入的双向重叠，扩展了标准 SCS 问题的经典证明方法。

Abstract: The Shortest Common Superstring (SCS) problem asks for the shortest string that contains each of a given set of strings as a substring. Its reverse-complement variant, the Shortest Common Superstring problem with Reverse Complements (SCS-RC), naturally arises in bioinformatics applications, where for each input string, either the string itself or its reverse complement must appear as a substring of the superstring. The well-known MGREEDY algorithm for the standard SCS constructs a superstring by first computing an optimal cycle cover on the overlap graph and then concatenating the strings corresponding to the cycles, while its refined variant, TGREEDY, further improves the approximation ratio. Although the original 4- and 3-approximation bounds of these algorithms have been successively improved for the standard SCS, no such progress has been made for the reverse-complement setting. A previous study extended MGREEDY to SCS-RC with a 4-approximation guarantee and briefly suggested that extending TGREEDY to the reverse-complement setting could achieve a 3-approximation. In this work, we strengthen these results by proving that the extensions of MGREEDY and TGREEDY to the reverse-complement setting achieve 3.75- and 2.875-approximation ratios, respectively. Our analysis extends the classical proofs for the standard SCS to handle the bidirectional overlaps introduced by reverse complements. These results provide the first formal improvement of approximation guarantees for SCS-RC, with the 2.875-approximate algorithm currently representing the best known bound for this problem.

</details>


### [6] [Finding large sparse induced subgraphs in graphs of small (but not very small) tree-independence number](https://arxiv.org/abs/2601.15861)
*Daniel Lokshtanov,Michał Pilipczuk,Paweł Rzążewski*

Main category: cs.DS

TL;DR: 该论文不涉及DSL、图处理、MLIR、编译器或HLS。

总结：这篇论文改进了针对树独立数有界的图上的最大权诱导子图优化问题的算法，将运行时间降低到$n^{\mathcal{O}(k)}$，其中$k$是树独立数。这使得算法对于树独立数为多对数或亚线性平衡团基分离器的几何交图类具有准多项式或亚指数时间复杂度，从而提高了其在$k$较大时的实用性。


<details>
  <summary>Details</summary>
Motivation: Lima et al.最近提出的算法虽然证明了对于树独立数有界（常数$k$）的图，一类有界树宽限制的最大权诱导子图优化问题可以在多项式时间内解决，但该算法的复杂度随$k$迅速增长，使得当树独立数$k$为超常数时算法变得不可用。因此，需要一个改进的算法来提高运行效率，使其在$k$较大时仍能保持实用性。

Method: 本文提出了一种改进的算法，用于解决在树独立数有界的图上，具有给定$CMSO_2$性质的、限制树宽的最大权诱导子图优化问题。新的算法将运行时间改进为$n^{\mathcal{O}(k)}$，其中$n$是顶点数，$k$是树独立数，$\mathcal{O}(\cdot)$中隐藏了与解的树宽限制和$CMSO_2$性质相关的因子。

Result: 一个新的、改进的算法被提出，它将解决同一族问题的时间复杂度降低到$n^{\mathcal{O}(k)}$。
这一运行时间对于树独立数为多对数级别的图类实现了准多项式时间（quasipolynomial）。
对于许多自然的几何交图类（即那些允许亚线性大小的平衡团基分离器的图），运行时间是亚指数时间（subexponential）。

Conclusion: 本文通过提出改进的算法，将Lima et al.提出的针对树独立数有界的图上的最大权诱导子图优化问题的求解时间从与$k$（树独立数）迅速增长的复杂性降低到$n^{\mathcal{O}(k)}$。这一改进使得该算法对于树独立数为多对数级别的图类以及许多具有亚线性大小的平衡团基分离器的几何交图类变得实用，实现了准多项式时间或亚指数时间，为解决这些图类上的复杂优化问题提供了高效工具。

Abstract: The independence number of a tree decomposition is the size of a largest independent set contained in a single bag. The tree-independence number of a graph $G$ is the minimum independence number of a tree decomposition of $G$. As shown recently by Lima et al. [ESA~2024], a large family of optimization problems asking for a maximum-weight induced subgraph of bounded treewidth, satisfying a given \textsf{CMSO}$_2$ property, can be solved in polynomial time in graphs whose tree-independence number is bounded by some constant~$k$.
  However, the complexity of the algorithm of Lima et al. grows rapidly with $k$, making it useless if the tree-independence number is superconstant. In this paper we present a refined version of the algorithm. We show that the same family of problems can be solved in time~$n^{\mathcal{O}(k)}$, where $n$ is the number of vertices of the instance, $k$ is the tree-independence number, and the $\mathcal{O}(\cdot)$-notation hides factors depending on the treewidth bound of the solution and the considered \textsf{CMSO}$_2$ property.
  This running time is quasipolynomial for classes of graphs with polylogarithmic tree-independence number; several such classes were recently discovered. Furthermore, the running time is subexponential for many natural classes of geometric intersection graphs -- namely, ones that admit balanced clique-based separators of sublinear size.

</details>


### [7] [Dynamic Pattern Matching with Wildcards](https://arxiv.org/abs/2601.16182)
*Arshia Ataee Naeini,Amir-Parsa Mobed,Masoud Seddighin,Saeed Seddighin*

Main category: cs.DS

TL;DR: 该论文不直接涉及 $DSL$ 或 $MLIR$ 或 $HLS$，但它与**图处理**（作为一般算法设计的一部分，尽管抽象中没有直接提及图结构，但它涉及到动态数据结构和算法设计，与图算法在思想上有共通之处）和**编译器**（更侧重于算法和数据结构的设计与分析，是计算机科学的基础部分，但并非直接针对编译器优化）相关。更准确地说，它属于**动态字符串算法**领域。；本文研究了文本和模式都可动态更新且模式含通配符的全动态模式匹配问题。对于最多 $k$ 个通配符的一般情况，提出了一种更新/查询时间为 $O(kn^{k/k+1} + k^2 \log n)$ 的算法，在 $k$ 为常数时达到真正的次线性。文章还提出了基于 $SETH$ 的条件性下界，并针对非通配符数量较少的两种特殊情况设计了更优的次线性算法，其中一种利用了 $FFT$ 技术。


<details>
  <summary>Details</summary>
Motivation: 本文研究的是在文本和模式都允许动态更新（插入、删除、修改）下的全动态模式匹配问题，其中模式中可以包含最多 $k$ 个通配符。这属于动态字符串算法领域的核心问题，其目标是设计高效（尤其是次线性时间复杂度）的算法来处理动态更新，这是实际应用中的重要需求。设计一个在 $k$ 较小时能达到真正次线性更新时间的算法是主要动机。

Method: 文章提出了一系列针对动态带通配符模式匹配问题的算法。对于包含多达 $k$ 个通配符的一般情况，设计了一个算法，其预处理时间为 $O(n\log^2 n)$，更新/查询时间为 $O(kn^{k/k+1} + k^2 \log n)$。该算法在 $k$ 为常数时是真正的次线性，在 $k=o(\log n)$ 时是次线性。此外，为了证明该问题的难度，文章基于 $SETH$ 提出了一个条件性下界。对于特殊情况，文章提出：1. 当模式包含 $w$ 个非通配符符号时，设计了一个预处理时间 $O(nw)$，更新时间 $O(w + \log n)$ 的算法。2. 当非通配符最多为两个时，结合 $FFT$ 技术和块分解，设计了一个确定性的次线性算法，其预处理时间为 $O(n^{1.8})$，更新时间为 $O(n^{0.8} \log n)$。

Result: 1. 对于最多 $k$ 个通配符的通用情况，设计了一个算法，其预处理时间为 $O(n\log^2 n)$，更新/查询时间为 $O(kn^{k/k+1} + k^2 \log n)$。该时间复杂度在 $k$ 为常数时是真正的次线性，在 $k=o(\log n)$ 时是次线性。2. 证明了一个条件性下界：假设次二次预处理时间，如果 $k = \Omega(\log n)$ 时能实现真正的次线性更新时间，将与 $SETH$ 假设矛盾。3. 对于模式中包含 $w$ 个非通配符的特殊情况，提出了一个预处理时间 $O(nw)$ 和更新时间 $O(w + \log n)$ 的算法，当 $w$ 是真正的次线性时，该算法也是真正的次线性。4. 对于非通配符最多为 $2$ 个的特殊情况，开发了一个基于 $FFT$ 和块分解的确定性次线性算法，预处理时间为 $O(n^{1.8})$，更新时间为 $O(n^{0.8} \log n)$。

Conclusion: 本文设计了一个在文本和模式都可动态更新的情况下，支持带通配符模式匹配的算法。对于一般的 $k$ 个通配符，算法实现了 $O(kn^{k/k+1} + k^2 \log n)$ 的更新/查询时间。文章还给出了一个基于 $SETH$ 假设的条件性下界，以证明在某些参数范围内，所设计算法的有效性。此外，对于两种特殊情况（非通配符数量较少），文章提出了更优的次线性算法。

Abstract: We study the fully dynamic pattern matching problem where the pattern may contain up to kwildcard symbols, each matching any symbol of the alphabet. Both the text and the pattern are subject to updates (insert, delete, change). We design an algorithm with O(nlog^2 n) preprocessing and update/query time O(knk/k+1 + k2 log n). The bound is truly sublinear for a constant k, and sublinear when k= o(log n). We further complement our results with a conditional lower bound: assuming subquadratic preprocessing time, achieving truly sublinear update time for the case k = Ω(log n) would contradict the Strong Exponential Time Hypothesis (SETH). Finally, we develop sublinear algorithms for two special cases: - If the pattern contains w non-wildcard symbols, we give an algorithm with preprocessing time O(nw) and update time O(w + log n), which is truly sublinear whenever wis truly sublinear. - Using FFT technique combined with block decomposition, we design a deterministic truly sublinear algorithm with preprocessing time O(n^1.8) and update time O(n^0.8 log n) for the case that there are at most two non-wildcards.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design](https://arxiv.org/abs/2601.15710)
*Jiahao Zhang,Zifan He,Nicholas Fraser,Michaela Blott,Yizhou Sun,Jason Cong*

Main category: cs.AR

TL;DR: 该论文与HLS和编译器、图处理（LLM推理本质上是计算图处理）、DSL（FlexLLM作为HLS库可以看作是特定领域加速器设计的抽象和DSL）相关。FlexLLM是一种可组合的高级综合（HLS）库，用于快速开发领域特定的LLM加速器。它允许对预填充和解码阶段进行定制化的混合设计，并提供全面的量化支持。利用FlexLLM，作者在Llama-3.2 1B模型上实现了加速器，在AMD U280 FPGA上相比NVIDIA A100 GPU实现了更高的端到端速度、解码吞吐量和能效，并在长上下文处理中通过集成HMT插件获得了显著的性能提升和上下文窗口扩展。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM加速器设计在算法创新与高性能硬件实现之间存在差距，缺乏一个能够快速开发特定领域LLM加速器的工具，特别是需要支持对推理的不同阶段（如预填充和解码）进行定制化优化，并提供有效的量化支持。FlexLLM的动机是提供一个可组合的高级综合（HLS）库，以弥合这一鸿沟，实现LLM加速器的快速开发和部署。

Method: FlexLLM作为一种可组合的高级综合（HLS）库，通过暴露关键的架构自由度，支持为预填充和解码阶段定制不同的时间重用和空间数据流的混合设计。它还提供了一套全面的量化工具，以支持精确的低位部署。利用该库，作者在不到两个月的时间内，用约1K行代码实现了一个完整的Llama-3.2 1B模型推理系统，包括一个阶段定制的加速器和一个用于高效长上下文处理的层次化内存Transformer (HMT) 插件。

Result: 在AMD U280 FPGA（16nm）上，FlexLLM加速器相比NVIDIA A100 GPU（7nm）的BF16推理，实现了1.29倍的端到端加速、1.64倍更高的解码吞吐量和3.14倍更高的能效。预测结果显示，在V80 FPGA（7nm）上，性能提升分别达到4.71倍、6.55倍和4.13倍。在长上下文场景中，集成HMT插件后，预填充延迟减少了23.23倍，上下文窗口扩大了64倍，在U280/V80上的端到端延迟分别降低了1.10倍/4.86倍，能效分别提高了5.21倍/6.27倍。模型的量化（WikiText-2 PPL为12.68）优于SpinQuant基线。

Conclusion: FlexLLM通过提供一种可组合的高级综合库，成功地弥合了LLM推理算法创新与高性能加速器实现之间的鸿沟，大大简化了特定领域LLM加速器的快速开发，并在性能和能效上显著超越了先进的GPU基线。

Abstract: We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\times$ end-to-end speedup, 1.64$\times$ higher decode throughput, and 3.14$\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\times$, 6.55$\times$, and 4.13$\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\times$ and extends the context window by 64$\times$, delivering 1.10$\times$/4.86$\times$ lower end-to-end latency and 5.21$\times$/6.27$\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [9] [Advancing RT Core-Accelerated Fixed-Radius Nearest Neighbor Search](https://arxiv.org/abs/2601.15633)
*Enzo Meneses,Hugo Bec,Cristóbal A. Navarroa,Benoît Crespin,Felipe A. Quezada,Nancy Hitschfeld,Heinich Porro,Maxime Maria*

Main category: cs.DC

TL;DR: 关联领域：图处理（Graph processing）（与 FRNN 和邻居搜索相关）、编译器（Compiler）（与底层优化和 RT Cores 使用相关）、高性能计算/通用 GPU 计算（与 RT Cores 和模拟加速相关）。
总结：本文提出了三种优化 RT Cores 上的快速径向近邻搜索（FRNN）粒子物理模拟的方法：实时 BVH 更新/重建优化器（提速 $\sim 3.4\times$）、消除邻居列表的新 RT Core 变体（提速可达 $\sim 2.0\times$ 并省内存）以及支持周期性边界条件的技术。这些方法显著提高了模拟性能、能效和可扩展性，同时作者也指出了 RT Cores 不如常规 GPU 计算的场景。


<details>
  <summary>Details</summary>
Motivation: 现有的基于 RT Cores 的粒子快速径向近邻搜索（FRNN）物理模拟在性能上仍有提升空间，尤其是在处理边界卷层次结构（BVH）的动态更新和重建、消除内存密集型的邻居列表，以及支持周期性边界条件（BC）等复杂模拟场景时。因此，需要开发新的技术来提高 RT Cores 在这些模拟中的速度、效率和适用性。

Method: 本文提出了三种主要方法来优化 RT Cores 上的 FRNN 仿真：
1. **BVH 结构实时更新/重建比率优化器：** 动态调整 BVH 的更新和重建策略，以适应模拟过程中不同的粒子动态。
2. **消除邻居列表的新 RT core 用法（两种变体）：** 通过新的 RT core 使用方式来避免使用耗费内存的邻居列表，从而实现更高效、内存占用更小的模拟。
3. **支持周期性边界条件（BC）的 RT Core 技术：** 使得 RT core 能够有效地处理具有周期性边界条件的 FRNN 模拟，且性能损失较小。
这些方法通过 Lennard-Jones FRNN 相互作用模型作为案例研究进行评估。

Result: 1. **BVH 优化器：** 所提出的更新/重建比率优化器能够适应模拟中的不同动态，使得基于 RT core 的管线比其他已知的 BVH 管理方法快约 $\sim 3.4$ 倍。
2. **无邻居列表的新 RT Core 变体：** 这些变体显着提高了基础 RT core 策略的速度和能效（EE）：在小半径时提速 $\sim 1.3$ 倍，对于对数正态半径分布（log normal radius distributions）提速 $\sim 2.0$ 倍。此外，这些变体还能模拟那些由于使用邻居列表而无法在内存中拟合的情况。
3. **周期性边界条件（BC）支持：** 提出的支持周期性 BC 的 RT Core 技术非常有效，因为它没有引入任何显著的性能惩罚。
4. **可扩展性：** 所提出的方法在跨代 GPU 上，其性能和能效都具有良好的可扩展性。
5. **局限性：** 实验评估还确定了仍然应优先选择常规 GPU 计算的模拟情况，有助于理解 RT cores 的优势和局限性。

Conclusion: 本文介绍了三种改进 RT Cores 上粒子 FRNN 物理模拟速度和效率的方法：BVH 结构实时更新/重建比率优化器、消除邻居列表需求的新 RT core 用法（两种变体），以及支持周期性边界条件的技术。这些方法共同提高了模拟性能、效率和可扩展性，并拓宽了 RT core 的适用范围，但同时也指出了常规 GPU 计算仍然适用的一些场景，为 RT core 的应用提供了更深入的理解。

Abstract: In this work we introduce three ideas that can further improve particle FRNN physics simulations running on RT Cores; i) a real-time update/rebuild ratio optimizer for the bounding volume hierarchy (BVH) structure, ii) a new RT core use, with two variants, that eliminates the need of a neighbor list and iii) a technique that enables RT cores for FRNN with periodic boundary conditions (BC). Experimental evaluation using the Lennard-Jones FRNN interaction model as a case study shows that the proposed update/rebuild ratio optimizer is capable of adapting to the different dynamics that emerge during a simulation, leading to a RT core pipeline up to $\sim 3.4\times$ faster than with other known approaches to manage the BVH. In terms of simulation step performance, the proposed variants can significantly improve the speedup and EE of the base RT core idea; from $\sim1.3\times$ at small radius to $\sim2.0\times$ for log normal radius distributions. Furthermore, the proposed variants manage to simulate cases that would otherwise not fit in memory because of the use of neighbor lists, such as clusters of particles with log normal radius distribution. The proposed RT Core technique to support periodic BC is indeed effective as it does not introduce any significant penalty in performance. In terms of scaling, the proposed methods scale both their performance and EE across GPU generations. Throughout the experimental evaluation, we also identify the simulation cases were regular GPU computation should still be preferred, contributing to the understanding of the strengths and limitations of RT cores.

</details>
