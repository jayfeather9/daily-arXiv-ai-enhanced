<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.DS](#cs.DS) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [AFLL: Real-time Load Stabilization for MMO Game Servers Based on Circular Causality Learning](https://arxiv.org/abs/2601.10998)
*Shinsuk Kang,Youngjae Kim*

Main category: cs.DC

TL;DR: 与 DSL、图处理、MLIR、编译器或 HLS 无关。
该论文提出了一种名为 AFLL（自适应反馈回路学习）的实时负载稳定系统，针对处理数千并发玩家的延迟关键型 MMO 游戏服务器。AFLL 使用反向传播来持续学习出站服务器消息和后续客户端请求之间的因果关系，从而实现预测性、差异化的消息类型流量控制。实验结果表明，该系统在 1,000 个并发玩家下，将平均和峰值 CPU 时间显著减少了约 50%，同时将线程竞争减少了 64.4%，并实现了零学习开销，证明了循环因果学习对实时适应性的有效性。


<details>
  <summary>Details</summary>
Motivation: 大规模多人在线（MMO）游戏服务器需要处理数千名并发玩家，同时维持低于 100 毫秒的响应时间。
当服务器负载超过容量时，传统方法会统一限制所有消息类型（损害游戏体验）或应用固定的启发式规则（无法适应动态工作负载）。
现有方法缺乏适应性，也无法区分消息的重要性，导致在负载过高时游戏体验下降。
因此，需要一个能够实时适应、学习消息因果关系并进行预测性、差异化流量控制的系统。

Method: AFLL（Adaptive Feedback Loop Learning，自适应反馈回路学习）是一种实时负载稳定系统，它通过学习出站服务器消息和后续入站客户端请求之间的因果关系来运作。
系统采用反向传播机制持续调整消息类型权重，从而实现预测性流量控制。 这种机制允许系统在过载发生前阻断低优先级消息，同时保证关键消息的交付。
实施时通过后台计算和缓存优化来实现零学习开销。
通过受控实验（1,000 个并发玩家）来评估其性能。

Result: 在 1,000 个并发玩家的受控实验中，AFLL 系统实现了显著的性能提升：
平均 CPU 时间减少了 48.3%（从 13.2 毫秒降至 6.8 毫秒）。
峰值 CPU 时间减少了 51.7%（从 54.0 毫秒降至 26.1 毫秒）。
线程竞争减少了 64.4%（从 19.6% 降至 7.0%）。
通过后台计算和缓存优化实现了零学习开销。
所有指标的变异系数（CV）低于 2%，显示了极好的重现性。
系统识别出了连接消息阻塞到负载降低的三阶段因果链。

Conclusion: AFLL 系统展示了循环因果学习能够实现延迟关键型系统的实用实时适应性。通过学习出站服务器消息和后续入站客户端请求之间的因果关系，AFLL 能够进行预测性流量控制，在不影响关键信息交付的情况下，显著降低 MMO 服务器的负载和延迟。

Abstract: Massively Multiplayer Online (MMO) game servers must handle thousands of simultaneous players while maintaining sub-100ms response times. When server load exceeds capacity, traditional approaches either uniformly throttle all message types regardless of importance (damaging gameplay) or apply fixed heuristic rules that fail to adapt to dynamic workloads. This paper presents AFLL (Adaptive Feedback Loop Learning), a real-time load stabilization system that learns the causal relationship between outgoing server messages and subsequent incoming client requests. AFLL employs backpropagation to continuously adjust message type weights, enabling predictive throttling that blocks low-priority messages before overload occurs while guaranteeing critical message delivery. Through controlled experiments with 1,000 concurrent players, AFLL reduced average CPU time by 48.3% (13.2ms to 6.8ms), peak CPU time by 51.7% (54.0ms to 26.1ms), and thread contention by 64.4% (19.6% to 7.0%), while maintaining zero learning overhead through background computation and caching optimizations. The system achieved remarkable reproducibility (CV < 2% across all metrics) and identified a three-stage causal chain linking message blocking to load reduction. AFLL demonstrates that circular causality learning enables practical real-time adaptation for latency-critical systems.

</details>


### [2] [Konflux: Optimized Function Fusion for Serverless Applications](https://arxiv.org/abs/2601.11156)
*Niklas Kowallik,Trever Schirmer,David Bermbach*

Main category: cs.DC

TL;DR: This paper is related to Compiler (Compiler optimization techniques applied to serverless functions) and potentially Graph Processing (if the function workflows are modeled as a graph). The core work is related to optimizing Function-as-a-Service (FaaS) deployments through function fusion. 
本文与编译器（将编译器优化技术应用于无服务器函数）可能相关，也可能与图处理相关（如果函数工作流被建模为图）。其核心工作是利用函数融合来优化函数即服务（FaaS）的部署。


<details>
  <summary>Details</summary>
Motivation: 函数即服务（FaaS）的部署优化极具挑战性，特别是在如何通过函数融合来降低包含多个函数的复杂无服务器应用程序的成本和延迟方面。由于可能的融合配置数量巨大，在生产环境中进行暴力基准测试既昂贵又耗时。

Method: 作者提出了一个系统，通过模拟 FaaS 平台来分析复杂应用程序的所有可能融合配置。该系统允许在本地进行实验，从而避免了在实时生产平台上重新配置和进行耗时的基准测试。

Result: 所提出的系统评估了多个示例 FaaS 应用程序和资源限制下所有的融合配置。结果表明，在分析成本和延迟权衡时，只有有限的融合配置集代表了最优解决方案，并且这些最优解受到特定定价模型的强烈影响。

Conclusion: 函数即服务（FaaS）部署的最佳配置集非常有限，并且受限于特定的定价模型。所提出的系统能够有效地本地模拟评估所有可能的融合配置，从而显著减少了生产环境中的昂贵和耗时的实验需求。

Abstract: Function-as-a-Service (FaaS) has become a central paradigm in serverless cloud computing, yet optimizing FaaS deployments remains challenging. Using function fusion, multiple functions can be combined into a single deployment unit, which can be used to reduce cost and latency of complex serverless applications comprising multiple functions. Even in small-scale applications, the number of possible fusion configurations is vast, making brute-force benchmarking in production both cost- and time-prohibitive.
  In this paper, we present a system that can analyze every possible fusion setup of complex applications. By emulating the FaaS platform, our system enables local experimentation, eliminating the need to reconfigure the live platform and significantly reducing associated cost and time. We evaluate all fusion configurations across a number of example FaaS applications and resource limits. Our results reveal that, when analyzing cost and latency trade-offs, only a limited set of fusion configurations represent optimal solutions, which are strongly influenced by the specific pricing model in use.

</details>


### [3] [Space-Optimal, Computation-Optimal, Topology-Agnostic, Throughput-Scalable Causal Delivery through Hybrid Buffering](https://arxiv.org/abs/2601.11487)
*Paulo Sérgio Almeida*

Main category: cs.DC

TL;DR: 相关领域：无（本文涉及“进程间通信”和“分布式系统”，但非“DSL”、“图处理”、“MLIR”、“编译器”或“HLS”）。/因果交付是分布式系统中通信的经典挑战，现有方法主要依赖接收方缓冲，但面临元数据开销问题。本文引入了“发送方发送许可”（SPS）策略，并提出了一个新的混合式因果交付算法（SPS + FIFO），它结合了发送方和接收方缓冲。新算法实现了几乎恒定的元数据大小和计算开销，解决了现有方法的局限性，并声称是第一个具有这些优点的、与拓扑无关的通用算法。


<details>
  <summary>Details</summary>
Motivation: 因果交付是分布式系统中进程间通信的经典且广泛有用的抽象。现有的方法主要依赖接收方缓冲，但对于大量进程，其元数据开销是不可承受的。尽管存在利用通信拓扑的特定方法，但它们缺乏通用性。纯发送方缓冲方法因其经典算法的诸多缺点而使用较少。本文的动机在于解决现有因果交付算法（尤其是流行的接收方缓冲方法和纯发送方缓冲方法）在元数据开销和可扩展性上的限制，旨在开发一种元数据开销低、性能优越、且与拓扑无关的通用因果交付算法。

Method: 本文首先讨论了纯发送方缓冲方法的局限性，并引入了“发送方发送许可”（SPS）执行策略，证明了“SPS + FIFO 蕴含 因果性”。然后，分析了一个现有的遵循SPS + FIFO的发送方缓冲算法 Cykas，并指出了其可扩展性和活性问题。最后，提出了一种新的基于SPS + FIFO的混合算法，该算法通过结合发送方缓冲（执行SPS）和接收方缓冲（执行FIFO）来强制执行因果性。

Result: 本文提出了一种新型的因果交付算法，它采用混合方法：发送方缓冲用于执行 SPS 策略，接收方缓冲用于执行 FIFO 策略。这种混合方法克服了纯发送方缓冲的局限性。新算法的关键优点包括：1. 每条消息的元数据大小几乎恒定。2. 计算上最优，通过精心选择数据结构，实现了分摊后处理开销几乎恒定。作者强调，据他们所知，这是第一个具有这些特性的、与拓扑无关的因果交付算法。

Conclusion: 本文提出了一种新的、混合式因果交付算法，结合了发送方缓冲（用于SPS）和接收方缓冲（用于FIFO）。该算法解决了纯发送方缓冲方法的局限性，实现了每条消息的元数据大小几乎恒定，并且通过精心选择数据结构，达到了分摊后处理开销几乎恒定的计算最优性。作者声称这是第一个具有这些特性的、与拓扑无关的因果交付算法。

Abstract: Message delivery respecting causal ordering (causal delivery) is one of the most classic and widely useful abstraction for inter-process communication in a distributed system. Most approaches tag messages with causality information and buffer them at the receiver until they can be safely delivered. Except for specific approaches that exploit communication topology, therefore not generally applicable, they incur a metadata overhead which is prohibitive for a large number of processes. Much less used are the approaches that enforce causal order by buffering messages at the sender, until it is safe to release them to the network, as the classic algorithm has too many drawbacks. In this paper, first we discuss the limitations of sender-only buffering approaches and introduce the Sender Permission to Send (SPS) enforcement strategy, showing that SPS + FIFO implies Causal. We analyze a recent sender-buffering algorithm, Cykas, which follows SPS + FIFO, albeit very conservatively, pointing out throughput scalability and liveness issues. Then, we introduce a novel SPS + FIFO based algorithm, which adopts a new hybrid approach: enforcing causality by combining sender-buffering to enforce SPS and receiver-buffering to enforce FIFO. The algorithm overcomes limitations of sender-only buffering, and achieves effectively constant metadata size per message. By a careful choice of data-structures, the algorithm is also computationally-optimal, with amortized effectively constant processing overhead. As far as we know, there is no other topology-agnostic causal delivery algorithm with these properties.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [4] [SwiftKV: An Edge-Oriented Attention Algorithm and Multi-Head Accelerator for Fast, Efficient LLM Decoding](https://arxiv.org/abs/2601.10953)
*Junming Zhang,Qinyan Zhang,Huajun Sun,Feiyang Gao,Sheng Hu,Rui Nie,Xiangshui Miao*

Main category: cs.AR

TL;DR: 该论文与 DSL、图处理、MLIR、编译器或 HLS **相关**，因为它提出了针对边缘加速器的、基于硬件加速的 LLM（大型语言模型）推理优化方法，这属于**编译器/硬件加速/HLS**的范畴。
太长不看版总结：本文针对边缘加速器上的 LLM 推理挑战，提出了 SwiftKV Attention 算法（一种单遍、低延迟、逐令牌流水线的注意力推理方法）和 SwiftKV-MHA 硬件加速器。实验证明，该方法在边缘加速器上实现了比原生注意力 7.16 倍的加速，显著降低了注意力延迟（总延迟降低 13.48 倍），并将生成速度和令牌效率分别提高了 17.4% 和 1.98 倍。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在边缘部署时，由于资源受限，实现快速的注意力推理和高效的解码仍然是一个挑战。作者旨在解决在边缘加速器上实现 LLM 推理效率和低延迟的问题。

Method: 本文提出了两个核心方法：
1. **SwiftKV Attention 算法：** 一种逐令牌流水线、低延迟的单遍注意力推理算法。其特点在于每个 (kt, vt) 在 KV 缓存中只被精确处理一次，采用统一的逐令牌流水线，不涉及分数具体化、分块 softmax 或第二次通过。这使得它能在单个硬件集上高效执行，无需资源密集型并行。
2. **SwiftKV-MHA 加速器：** 一种设计用于解决现有加速器对多头 LLM 解码支持有限的问题的硬件架构。它允许在高精度注意力计算和低精度 GEMV（通用矩阵向量乘法）操作在相同的处理器阵列上执行，从而实现快速高效的多头并行解码。

Result: 实验结果表明：
1. 在边缘加速器上，SwiftKV Attention 算法相对于原生注意力实现了 **7.16 倍**的加速，并且显著优于其他注意力算法。
2. SwiftKV-MHA 加速器进一步将注意力延迟降低了 **13.48 倍**。
3. 在相同设置下，SwiftKV-MHA 相较于最先进（state-of-the-art）的工作，生成速度提升了 **17.4%**，令牌效率提高了 **1.98 倍**。

Conclusion: 本文提出的 SwiftKV Attention 算法和 SwiftKV-MHA 加速器，通过创新的单遍、低延迟、逐令牌流水线注意力推理方法，在资源受限的边缘加速器上实现了大型语言模型（LLM）推理性能的显著提升，尤其在降低注意力延迟和提高解码速度方面效果显著。

Abstract: Edge acceleration for large language models is crucial for their widespread application; however, achieving fast attention inference and efficient decoding on resource-constrained edge accelerators remains challenging. This paper presents SwiftKV Attention, a per-token pipelined, low-latency single-pass attention inference algorithm, where every (kt, vt) in the KV cache is processed exactly once in a uniform per-token pipeline without score materialization, blockwise softmax, or a second pass, thereby enabling fast execution on edge accelerators with a single hardware set and no resource-intensive parallelism. Furthermore, to address the limited support for multi-head LLM decoding in existing accelerators, we design the SwiftKV-MHA accelerator, which enables high precision attention and low precision GEMV on the same processor array, achieving fast and efficient multi-head parallel decoding. Experimental results show that, on the edge accelerator, the SwiftKV Attention algorithm achieves a 7.16* speedup over native attention and significantly outperforms other attention algorithms. SwiftKV-MHA further reduces attention latency by 13.48*; under the same settings, it improves generation speed by 17.4% and increases token efficiency by 1.98* compared with state-of-the-art works.

</details>


### [5] [RidgeWalker: Perfectly Pipelined Graph Random Walks on FPGAs](https://arxiv.org/abs/2601.11057)
*Hongshi Tan,Yao Chen,Xinyu Chen,Qizhen Zhang,Cheng Chen,Weng-Fai Wong,Bingsheng He*

Main category: cs.AR

TL;DR: 是，该论文与图处理（Graph Random Walks, GRW）、编译器（HLS的语境下，虽然未直接提及，但FPGA设计通常涉及）和HLS（FPGA加速器设计）领域相关。GRW是一种图处理技术。FPGA加速器的设计和实现属于硬件描述语言（HLS）和特定领域编译器优化的范畴。
太长不看：RidgeWalker是一种高性能的图随机游走（GRW）加速器，专为数据中心FPGA设计。它利用GRW的马尔可夫特性将任务分解成可乱序执行的无状态细粒度任务，并通过异步流水线和基于排队论的反馈驱动调度器，解决了GRW加速中存在的流水线效率低和负载不均衡的问题。实验结果显示，RidgeWalker比最先进的FPGA和GPU解决方案分别平均加速7.0倍和8.1倍，具有显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 图随机游走（GRW）在许多应用中被广泛采用，但由于其强数据依赖性、不规则的内存访问模式和不平衡的执行行为，其工作负载的加速非常困难。现有的基于FPGA的加速方案因为流水线效率低下和静态调度而未能充分发挥硬件潜力。因此，需要一种高性能的GRW加速器来克服这些挑战。

Method: RidgeWalker的核心在于利用GRW的马尔可夫特性将其分解为无状态的细粒度任务，这些任务可以乱序执行。基于此，RidgeWalker引入了一个异步流水线架构，并采用了一个基于排队论的反馈驱动调度器，从而实现了完善的流水线和自适应的负载均衡。

Result: RidgeWalker在真实世界的图数据集和GRW算法上进行了评估，结果表明，它比最先进的FPGA解决方案平均加速7.0倍，比GPU解决方案平均加速8.1倍，峰值加速分别高达71.0倍和22.9倍。

Conclusion: RidgeWalker是一款高性能GRW加速器，通过利用GRW的马尔可夫特性将其分解为无状态的细粒度任务，并采用基于排队论的反馈驱动调度器和异步流水线架构，实现了完善的流水线和自适应的负载均衡。实验证明，RidgeWalker在各类GRW算法和真实图数据集上，平均速度比最先进的FPGA解决方案快7.0倍，比GPU解决方案快8.1倍，峰值加速分别高达71.0倍和22.9倍。

Abstract: Graph Random Walks (GRWs) offer efficient approximations of key graph properties and have been widely adopted in many applications. However, GRW workloads are notoriously difficult to accelerate due to their strong data dependencies, irregular memory access patterns, and imbalanced execution behavior. While recent work explores FPGA-based accelerators for GRWs, existing solutions fall far short of hardware potential due to inefficient pipelining and static scheduling. This paper presents RidgeWalker, a high-performance GRW accelerator designed for datacenter FPGAs. The key insight behind RidgeWalker is that the Markov property of GRWs allows decomposition into stateless, fine-grained tasks that can be executed out-of-order without compromising correctness. Building on this, RidgeWalker introduces an asynchronous pipeline architecture with a feedback-driven scheduler grounded in queuing theory, enabling perfect pipelining and adaptive load balancing. We prototype RidgeWalker on datacenter FPGAs and evaluated it across a range of GRW algorithms and real-world graph datasets. Experimental results demonstrate that RidgeWalker achieves an average speedup of 7.0x over state-of-the-art FPGA solutions and 8.1x over GPU solutions, with peak speedups of up to 71.0x and 22.9x, respectively. The source code is publicly available at https://github.com/Xtra-Computing/RidgeWalker.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [6] [Cutting Corners on Uncertainty: Zonotope Abstractions for Stream-based Runtime Monitoring](https://arxiv.org/abs/2601.11358)
*Bernd Finkbeiner,Martin Fränzle,Florian Kohn,Paul Kröger*

Main category: cs.PL

TL;DR: 该论文与编译器相关，体现在其使用抽象域和状态追踪的方法，常用于程序分析和验证，与编译器的优化和指令调度也有一定的关联。

太长不看：流式监控的安全关键系统中，传感器误差的传播和状态表示的无界增长是一个问题。本文引入了zonotopes作为抽象域来精确跟踪监视器的仿射状态，并使用有界内存的过近似方法解决状态增长问题。文章比较了不同的zonotope过近似策略在性能和误报率上的表现。


<details>
  <summary>Details</summary>
Motivation: 流式监控评估安全关键系统的健康状况时，传感器测量值输入流的误差会通过计算传播并影响最终的裁决。虽然仿射算术可以精确跟踪这些误差，但独立的测量噪声会导致状态表示中的松弛变量（slack variables）无限制增长，因此任何有界内存的监控算法都必须以产生可靠近似的方式在运行时统一松弛变量。

Method: 本文引入了zonotopes作为运行时逻辑语言RLola规范的在线监视器的抽象域。实验上，通过比较不同的zonotope过近似策略，评估了它们在性能和误报率方面的表现。

Result: 本文提出的基于zonotopes的有界内存监控算法能够精确捕获监视器的仿射状态，并且通过过近似产生可靠的（sound）监控结果。实验比较了不同的zonotope过近似策略在运行时监控中的性能和误报率。

Conclusion: 本文证明了在线运行时监控中，使用zonotopes可以精确捕获监视器的仿射状态并产生声音（sound）的有界内存监视器。通过比较不同的zonotope过近似策略，评估了它们在性能和误报率方面的表现。

Abstract: Stream-based monitoring assesses the health of safety-critical systems by transforming input streams of sensor measurements into output streams that determine a verdict. These inputs are often treated as accurate representations of the physical state, although real sensors introduce calibration and measurement errors. Such errors propagate through the monitor's computations and can distort the final verdict. Affine arithmetic with symbolic slack variables can track these errors precisely, but independent measurement noise introduces a fresh slack variable upon each measurement event, causing the monitor's state representation to grow without bound over time. Therefore, any bounded-memory monitoring algorithm must unify slack variables at runtime in a way that generates a sound approximation.
  This paper introduces zonotopes as an abstract domain for online monitoring of RLola specifications. We demonstrate that zonotopes precisely capture the affine state of the monitor and that their over-approximation produces a sound bounded-memory monitor. We present a comparison of different zonotope over-approximation strategies in the context of runtime monitoring, evaluating their performance and false-positive rates.

</details>


### [7] [Qihe: A General-Purpose Static Analysis Framework for Verilog](https://arxiv.org/abs/2601.11408)
*Qinlin Chen,Nairen Zhang,Jinpeng Wang,Jiacai Cui,Tian Tan,Xiaoxing Ma,Chang Xu,Jian Lu,Yue Li*

Main category: cs.PL

TL;DR: 该论文与编译器和HLS相关，因为它涉及对硬件描述语言Verilog的程序分析，这通常是硬件编译和高级综合流程的一部分，特别是IR和静态分析技术是编译器基础设施的核心；而DSL和图处理在内容中没有直接提及，MLIR是一种特定的IR，而本文中是"Verilog-specific IR"。
**TL;DR:** 软件静态分析框架的成功在硬件领域尚未复制，本文介绍了Qihe，第一个针对Verilog的通用静态分析框架。Qihe提供了一个面向分析的前端、Verilog特有的IR和一套基础分析，用于处理位向量运算、同步和并发等硬件特性。通过开发错误检测、安全分析和程序理解的客户端，Qihe展示了潜力，发现了9个未知的硬件错误，并识别了现有工具无法检测的错误和漏洞。开源Qihe（拥有超过10万行代码）旨在推动硬件静态分析领域的创新和生态系统的发展。


<details>
  <summary>Details</summary>
Motivation: 软件领域通过通用静态分析框架实现了错误检测、安全分析和程序理解等应用的蓬勃发展。然而，硬件领域缺乏这样一个通用框架，这阻碍了硬件复杂静态分析的发展，使其无法像软件那样取得显著成就。因此，作者旨在构建第一个针对Verilog的通用静态分析框架，以克服硬件静态分析中的挑战和空白。

Method: 作者开发了Qihe，这是一个通用的Verilog静态分析框架。该框架包括一个面向分析的前端、一个Verilog特有的中间表示（IR），以及一套能够捕获硬件特有特性（如位向量运算、寄存器同步和数字组件并发）的基础分析。这些基础分析旨在支持广泛的硬件分析客户端。为了验证其效用，作者进一步开发了一套客户端，涵盖错误检测、安全分析和程序理解。

Result: Qihe的初步实验结果非常有前景。例如，它在流行的真实世界硬件项目中发现了9个先前未知的错误，所有这些都得到了开发人员的确认。此外，Qihe成功识别了18个现有Verilog错误检测静态分析工具（linter）无法检测到的错误，并在真实世界硬件程序中检测了16个漏洞。Qihe的代码库超过100K行，并已开源。

Conclusion: Qihe是第一个针对Verilog的通用静态分析框架。它通过提供硬件特有的IR、面向分析的前端以及一套涵盖位向量运算、寄存器同步和并发等特性的基础分析，填补了硬件领域此类框架的空白。通过开源Qihe及其丰富的代码库，作者旨在催化硬件静态分析领域的进一步创新，并建立一个类似于软件分析的活跃生态系统。

Abstract: In the past decades, static analysis has thrived in software, facilitating applications in bug detection, security, and program understanding. These advanced analyses are largely underpinned by general-purpose static analysis frameworks, which offer essential infrastructure to streamline their development. Conversely, hardware lacks such a framework, which overshadows the promising opportunities for sophisticated static analysis in hardware, hindering achievements akin to those witnessed in software. We thus introduce Qihe, the first general-purpose static analysis framework for Verilog -- a highly challenging endeavor given the absence of precedents in hardware. Qihe features an analysis-oriented front end, a Verilog-specific IR, and a suite of diverse fundamental analyses that capture essential hardware-specific characteristics -- such as bit-vector arithmetic, register synchronization, and digital component concurrency -- and enable the examination of intricate hardware data and control flows. These fundamental analyses are designed to support a wide array of hardware analysis clients. To validate Qihe's utility, we further developed a set of clients spanning bug detection, security, and program understanding. Our preliminary experimental results are highly promising; for example, Qihe uncovered 9 previously unknown bugs in popular real-world hardware projects (averaging 1.5K+ GitHub stars), all of which were confirmed by developers; moreover, Qihe successfully identified 18 bugs beyond the capabilities of existing static analyses for Verilog bug detection (i.e., linters), and detected 16 vulnerabilities in real-world hardware programs. By open-sourcing Qihe, which comprises over 100K lines of code, we aim to inspire further innovation and applications of sophisticated static analysis for hardware, aspiring to foster a similarly vibrant ecosystem that software analysis enjoys.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [8] [Two Complexity Results on Spanning-Tree Congestion Problems](https://arxiv.org/abs/2601.10881)
*Sunny Atalig,Marek Chrobak,Christoph Dürr,Petr Kolman,Huong Luu,Jiří Sgall,Gregory Zhu*

Main category: cs.DS

TL;DR: 不相关。
生成树拥塞问题（$\\mathsf{STC}$）的目标是找到一个最小化最大边拥塞的生成树。本文解决关于 $\\mathsf{STC}$ 计算复杂性的两个开放性问题：首先，证明了对于最大度 $d \\ge 3$ 的图，$\\mathsf{STC}$ 仍然是 $\\mathbb{NP}$-hard的，完全解决了度受限图的复杂性问题。其次，证明了对于 $K$-边连通图，$\\mathsf{STC}$ 的判定版本（拥塞至多为 $K$）是多项式时间可解的。


<details>
  <summary>Details</summary>
Motivation: 生成树拥塞问题（$\\mathsf{STC}$）是一个已知的 $\\mathbb{NP}$-hard 问题，即使在某些受限图类别中也是如此。本文的动机是解决该领域中关于其计算复杂性的几个关键开放性问题。具体来说，是确定 $\\mathsf{STC}$ 在度受限图上的确切 $\\mathbb{NP}$-hard 范围，以及研究 $\\mathsf{STC}$ 判定版本在 $K$-边连通图上的复杂度。

Method: 本文通过理论证明的方法，分析了 $\\mathsf{STC}$ 问题的计算复杂度。对于度受限图，采用了归约证明来证明 $\\mathbb{NP}$ 难性。对于 $\\mathsf{STC}$ 的判定版本，则可能是通过构造性算法或利用 $K$-边连通性质进行分析，来证明它在多项式时间内可解。

Result: 本文取得了两个主要结果：（i）证明了对于最大度为 $d$ 的图，当 $d \\ge 3$ 时，$\\mathsf{STC}$ 问题仍然是 $\\mathbb{NP}$ 困难的，从而完全解决了这一变体的复杂性问题。（ii）证明了对于 $K$-边连通图，$\\mathsf{STC}$ 问题的判定版本（判断拥塞是否至多为 $K$）是可以在多项式时间内求解的。

Conclusion: 本文解决了 $\\mathsf{STC}$ 问题在不同图类别下的计算复杂性问题和相关的开放性问题。对于度受限图，证明了当最大度 $d \\ge 3$ 时，$\\mathsf{STC}$ 仍然是 $\\mathbb{NP}$ 困难的。对于 $\\mathsf{STC}$ 的判定版本，证明了对于 $K$-边连通图，该问题是可以在多项式时间内求解的。

Abstract: In the spanning-tree congestion problem ($\mathsf{STC}$), we are given a graph $G$, and the objective is to compute a spanning tree of $G$ that minimizes the maximum edge congestion. While $\mathsf{STC}$ is known to be $\mathbb{NP}$-hard, even for some restricted graph classes, several key questions regarding its computational complexity remain open, and we address some of these in our paper. (i) For graphs of degree at most $d$, it is known that $\mathsf{STC}$ is $\mathbb{NP}$-hard when $d\ge 8$. We provide a complete resolution of this variant, by showing that $\mathsf{STC}$ remains $\mathbb{NP}$-hard for each degree bound $d\ge 3$. (ii) In the decision version of $\mathsf{STC}$, given an integer $K$, the goal is to determine whether the congestion of $G$ is at most $K$. We prove that this variant is polynomial-time solvable for $K$-edge-connected graphs.

</details>


### [9] [Streaming Stochastic Submodular Maximization with On-Demand User Requests](https://arxiv.org/abs/2601.10901)
*Honglian Wang,Sijing Tu,Lutz Oettershagen,Aristides Gionis*

Main category: cs.DS

TL;DR: 该论文与图处理和编译器和HLS无关，但是与流式次模最大化（属于算法和优化范畴，与机器学习有交叉）有关。
本文探索了一个流式次模最大化新问题，其灵感来自新闻推荐平台的动态，目标是设计一个流式算法来最大化预期的总主题覆盖。作者将此问题与受拟阵约束的次模最大化相关联，并提出了一个克服现有方法在用户访问计数不确定和次线性内存预算下性能限制的新型在线流式算法，实现了$1/(8\delta)$的竞争比，且只需单次扫描和与流长度无关的内存。实验证明了所提算法的优越性。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是探索流式次模最大化中的一个新问题，该问题受新闻推荐平台动态的启发：网站需要在用户每次访问时显示最多$k$条新闻，最大化预期的总主题覆盖。特别地，现有方法在用户访问总数未知且内存预算次线性增长的更现实场景中不能保证有界性能，因此需要开发新的算法来克服这些限制。

Method: 作者首先将问题与受拟阵约束的次模最大化联系起来。针对用户访问次数已知或可使用与流长度成线性关系的内存的情况，作者表明可以有效适应现有方法。然而，针对用户访问次数未知且只能使用次线性内存的更现实场景，作者提出了一种新的在线流式算法来克服现有方法的局限性，该算法实现了$1/(8\delta)$的竞争比，并且只需进行单次扫描，内存使用与流的长度无关。

Result: 本文将所提出的问题与受拟阵约束的次模最大化联系起来。针对用户访问次数已知或可使用与流长度成线性关系的内存的情况，作者表明可以有效适应现有方法。然而，在更现实的场景下，即用户访问总数的上限已知且只能使用次线性内存时，现有算法无法保证有界性能。作者提出了一种新的在线流式算法，实现了$1/(8\delta)$的竞争比，且只需进行单次扫描，内存使用与流的长度无关。实验结果表明，该算法持续优于基线方法。

Conclusion: 本文研究了一个流式次模最大化问题，其动机来自于新闻推荐平台。针对用户访问总数未知、内存预算次线性增长的更现实场景，提出了一种新的在线流式算法，其竞争比为$1/(8\delta)$，且只进行单次扫描，内存使用与流的长度无关。实验结果表明，该算法持续优于基线方法。

Abstract: We explore a novel problem in streaming submodular maximization, inspired by the dynamics of news-recommendation platforms. We consider a setting where users can visit a news website at any time, and upon each visit, the website must display up to $k$ news items. User interactions are inherently stochastic: each news item presented to the user is consumed with a certain acceptance probability by the user, and each news item covers certain topics. Our goal is to design a streaming algorithm that maximizes the expected total topic coverage. To address this problem, we establish a connection to submodular maximization subject to a matroid constraint. We show that we can effectively adapt previous methods to address our problem when the number of user visits is known in advance or linear-size memory in the stream length is available. However, in more realistic scenarios where only an upper bound on the visits and sublinear memory is available, the algorithms fail to guarantee any bounded performance. To overcome these limitations, we introduce a new online streaming algorithm that achieves a competitive ratio of $1/(8δ)$, where $δ$ controls the approximation quality. Moreover, it requires only a single pass over the stream, and uses memory independent of the stream length. Empirically, our algorithms consistently outperform the baselines.

</details>
