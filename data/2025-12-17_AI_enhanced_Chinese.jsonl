{"id": "2512.14290", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.14290", "abs": "https://arxiv.org/abs/2512.14290", "authors": ["Suhrid Gupta", "Muhammed Tawfiqul Islam", "Rajkumar Buyya"], "title": "A Hybrid Reactive-Proactive Auto-scaling Algorithm for SLA-Constrained Edge Computing", "comment": null, "summary": "Edge computing decentralizes computing resources, allowing for novel applications in domains such as the Internet of Things (IoT) in healthcare and agriculture by reducing latency and improving performance. This decentralization is achieved through the implementation of microservice architectures, which require low latencies to meet stringent service level agreements (SLA) such as performance, reliability, and availability metrics. While cloud computing offers the large data storage and computation resources necessary to handle peak demands, a hybrid cloud and edge environment is required to ensure SLA compliance. This is achieved by sophisticated orchestration strategies such as Kubernetes, which help facilitate resource management. The orchestration strategies alone do not guarantee SLA adherence due to the inherent delay of scaling resources. Existing auto-scaling algorithms have been proposed to address these challenges, but they suffer from performance issues and configuration complexity. In this paper, a novel auto-scaling algorithm is proposed for SLA-constrained edge computing applications. This approach combines a Machine Learning (ML) based proactive auto-scaling algorithm, capable of predicting incoming resource requests to forecast demand, with a reactive autoscaler which considers current resource utilization and SLA constraints for immediate adjustments. The algorithm is integrated into Kubernetes as an extension, and its performance is evaluated through extensive experiments in an edge environment with real applications. The results demonstrate that existing solutions have an SLA violation rate of up to 23%, whereas the proposed hybrid solution outperforms the baselines with an SLA violation rate of only 6%, ensuring stable SLA compliance across various applications.", "AI": {"tldr": "\u6d89\u53ca\u8fb9\u7f18\u8ba1\u7b97\u3001\u5fae\u670d\u52a1\u3001Kubernetes\u3001\u81ea\u52a8\u4f38\u7f29\u7b97\u6cd5\u3001\u673a\u5668\u5b66\u4e60\u3002\n\u4e00\u79cd\u7528\u4e8eSLA\u53d7\u9650\u8fb9\u7f18\u8ba1\u7b97\u5e94\u7528\u7684\u65b0\u578b\u6df7\u5408\u81ea\u52a8\u4f38\u7f29\u7b97\u6cd5\u88ab\u63d0\u51fa\u3002\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u57fa\u4e8eML\u7684\u4e3b\u52a8\u9884\u6d4b\u548c\u57fa\u4e8e\u5f53\u524d\u8d44\u6e90\u5229\u7528\u7387\u7684\u88ab\u52a8\u8c03\u6574\uff0c\u5e76\u4f5c\u4e3a\u6269\u5c55\u96c6\u6210\u5230Kubernetes\u4e2d\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06SLA\u8fdd\u89c4\u7387\u4ece\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u768423%\u964d\u4f4e\u52306%\uff0c\u663e\u8457\u63d0\u9ad8\u4e86SLA\u5408\u89c4\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u5fae\u670d\u52a1\u67b6\u6784\u9700\u8981\u4f4e\u5ef6\u8fdf\u6765\u6ee1\u8db3\u4e25\u683c\u7684\u670d\u52a1\u6c34\u5e73\u534f\u8bae\uff08SLA\uff09\u3002\u867d\u7136\u6df7\u5408\u4e91\u548c\u8fb9\u7f18\u73af\u5883\u901a\u8fc7Kubernetes\u7b49\u534f\u8c03\u7b56\u7565\u5b9e\u73b0\u8d44\u6e90\u7ba1\u7406\uff0c\u4f46\u73b0\u6709\u7684\u81ea\u52a8\u4f38\u7f29\u7b97\u6cd5\u5b58\u5728\u6027\u80fd\u95ee\u9898\u548c\u914d\u7f6e\u590d\u6742\u6027\uff0c\u4e14\u65e0\u6cd5\u4fdd\u8bc1SLA\u7684\u9075\u5b88\uff0c\u5bfc\u81f4SLA\u8fdd\u89c4\u7387\u8f83\u9ad8\uff08\u6587\u4e2d\u63d0\u5230\u9ad8\u8fbe23%\uff09\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u4f38\u7f29\u7b97\u6cd5\u6765\u786e\u4fddSLA\u5408\u89c4\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u52a8\u4f38\u7f29\u7b97\u6cd5\uff0c\u7528\u4e8e\u53d7SLA\u7ea6\u675f\u7684\u8fb9\u7f18\u8ba1\u7b97\u5e94\u7528\u3002\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u7684\u9884\u6d4b\u6027\u81ea\u52a8\u4f38\u7f29\u7b97\u6cd5\uff08\u80fd\u591f\u9884\u6d4b\u4f20\u5165\u7684\u8d44\u6e90\u8bf7\u6c42\u4ee5\u9884\u6d4b\u9700\u6c42\uff09\u548c\u88ab\u52a8\u4f38\u7f29\u5668\uff08\u8003\u8651\u5f53\u524d\u7684\u8d44\u6e90\u5229\u7528\u7387\u548cSLA\u7ea6\u675f\u8fdb\u884c\u5373\u65f6\u8c03\u6574\uff09\u3002\u8be5\u7b97\u6cd5\u4f5c\u4e3a\u6269\u5c55\u96c6\u6210\u5230Kubernetes\u4e2d\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6df7\u5408\u81ea\u52a8\u4f38\u7f29\u7b97\u6cd5\u5728\u8fb9\u7f18\u73af\u5883\u4e0b\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7684SLA\u8fdd\u89c4\u7387\u9ad8\u8fbe23%\uff0c\u800c\u6240\u63d0\u51fa\u7684\u6df7\u5408\u89e3\u51b3\u65b9\u6848\u7684SLA\u8fdd\u89c4\u7387\u4ec5\u4e3a6%\u3002\u8fd9\u8868\u660e\u65b0\u65b9\u6cd5\u5728\u786e\u4fdd\u5404\u79cd\u5e94\u7528\u7684\u7a33\u5b9aSLA\u5408\u89c4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u81ea\u52a8\u4f38\u7f29\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u4f20\u7edf\u81ea\u52a8\u4f38\u7f29\u7b97\u6cd5\u5b58\u5728\u7684SLA\u8fdd\u89c4\u7387\u9ad8\u548c\u914d\u7f6e\u590d\u6742\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\uff0c\u8be5\u6df7\u5408\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u89e3\u51b3\u65b9\u6848\uff0cSLA\u8fdd\u89c4\u7387\u4ec5\u4e3a6%\uff0c\u663e\u8457\u4f4e\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u768423%\uff0c\u4ece\u800c\u786e\u4fdd\u4e86\u5404\u79cd\u5e94\u7528\u7684\u7a33\u5b9aSLA\u5408\u89c4\u6027\u3002"}}
{"id": "2512.14445", "categories": ["cs.DC", "cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.14445", "abs": "https://arxiv.org/abs/2512.14445", "authors": ["Brenton Walker", "Markus Fidler"], "title": "Performance and Stability of Barrier Mode Parallel Systems with Heterogeneous and Redundant Jobs", "comment": null, "summary": "In some models of parallel computation, jobs are split into smaller tasks and can be executed completely asynchronously. In other situations the parallel tasks have constraints that require them to synchronize their start and possibly departure times. This is true of many parallelized machine learning workloads, and the popular Apache Spark processing engine has recently added support for Barrier Execution Mode, which allows users to add such barriers to their jobs. These barriers necessarily result in idle periods on some of the workers, which reduces their stability and performance, compared to equivalent workloads with no barriers.\n  In this paper we will consider and analyze the stability and performance penalties resulting from barriers. We include an analysis of the stability of $(s,k,l)$ barrier systems that allow jobs to depart after $l$ out of $k$ of their tasks complete. We also derive and evaluate performance bounds for hybrid barrier systems servicing a mix of jobs, both with and without barriers, and with varying degrees of parallelism. For the purely 1-barrier case we compare the bounds and simulation results to benchmark data from a standalone Spark system. We study the overhead in the real system, and based on its distribution we attribute it to the dual event and polling-driven mechanism used to schedule barrier-mode jobs. We develop a model for this type of overhead and validate it against the real system through simulation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e0e\u56fe\u5904\u7406\uff08Apache Spark \u662f\u4e00\u4e2a\u6d41\u884c\u7684\u56fe\u5904\u7406\u548c\u6570\u636e\u5904\u7406\u5f15\u64ce\uff09\u3001\u7f16\u8bd1\u5668\uff08\u5e76\u884c\u8ba1\u7b97\u548c\u8c03\u5ea6\u4f18\u5316\u76f8\u5173\uff09\u3001MLIR\uff08\u4e0e\u7f16\u8bd1\u5668\u548c\u9ad8\u6027\u80fd\u8ba1\u7b97\u76f8\u5173\uff09\u6709\u5173\u3002\u5b83\u5206\u6790\u4e86\u5e76\u884c\u8ba1\u7b97\u6a21\u578b\u4e2d\u7684\u540c\u6b65\u5c4f\u969c\uff08\u4f8b\u5982 Apache Spark \u7684 Barrier Execution Mode\uff09\u5bf9\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u7684\u5f71\u54cd\u3002\u4f5c\u8005\u5206\u6790\u4e86\u4e0d\u540c\u5c4f\u969c\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\uff0c\u5bfc\u51fa\u4e86\u6df7\u5408\u5c4f\u969c\u7cfb\u7edf\u7684\u6027\u80fd\u754c\u9650\uff0c\u5e76\u5c06\u7406\u8bba\u7ed3\u679c\u4e0eSpark\u57fa\u51c6\u6570\u636e\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u8c03\u67e5\u4e86\u771f\u5b9e\u7cfb\u7edf\u4e2d\u7684\u5f00\u9500\u539f\u56e0\uff08\u5f52\u56e0\u4e8e\u8c03\u5ea6\u673a\u5236\uff09\u3002", "motivation": "\u5728\u8bb8\u591a\u5e76\u884c\u5316\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\uff0c\u5e76\u884c\u4efb\u52a1\u5b58\u5728\u7ea6\u675f\uff0c\u8981\u6c42\u5b83\u4eec\u540c\u6b65\u542f\u52a8\uff0c\u5e76\u53ef\u80fd\u540c\u6b65\u79bb\u5f00\u3002\u6d41\u884c\u7684 Apache Spark \u5904\u7406\u5f15\u64ce\u6700\u8fd1\u6dfb\u52a0\u4e86\u5bf9 Barrier Execution Mode \u7684\u652f\u6301\uff0c\u5141\u8bb8\u7528\u6237\u5411\u5176\u4f5c\u4e1a\u6dfb\u52a0\u6b64\u7c7b\u5c4f\u969c\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u5c4f\u969c\u4e0d\u53ef\u907f\u514d\u5730\u5bfc\u81f4\u90e8\u5206\u5de5\u4f5c\u8282\u70b9\u5904\u4e8e\u7a7a\u95f2\u72b6\u6001\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u8003\u8651\u548c\u5206\u6790\u7531\u540c\u6b65\u5c4f\u969c\u5bfc\u81f4\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u635f\u5931\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\u548c\u6f5c\u5728\u539f\u56e0\u3002", "method": "\u672c\u6587\u9996\u5148\u8003\u8651\u5e76\u5206\u6790\u4e86\u540c\u6b65\u5c4f\u969c\u5bf9\u5e76\u884c\u8ba1\u7b97\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u9020\u6210\u7684\u635f\u5931\u3002\u7279\u522b\u5206\u6790\u4e86 $(s,k,l)$ \u5c4f\u969c\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\uff0c\u8be5\u7cfb\u7edf\u5141\u8bb8\u4f5c\u4e1a\u5728\u5176 $k$ \u4e2a\u4efb\u52a1\u4e2d\u7684 $l$ \u4e2a\u5b8c\u6210\u540e\u79bb\u5f00\u3002\u5176\u6b21\uff0c\u63a8\u5bfc\u5e76\u8bc4\u4f30\u4e86\u6df7\u5408\u5c4f\u969c\u7cfb\u7edf\uff08\u670d\u52a1\u4e8e\u6709\u5c4f\u969c\u548c\u65e0\u5c4f\u969c\u4f5c\u4e1a\u7684\u6df7\u5408\u4f53\uff0c\u5177\u6709\u4e0d\u540c\u7a0b\u5ea6\u7684\u5e76\u884c\u6027\uff09\u7684\u6027\u80fd\u754c\u9650\u3002\u6700\u540e\uff0c\u5bf9\u4e8e\u7eaf\u7cb9\u7684 1 \u5c4f\u969c\u60c5\u51b5\uff0c\u5c06\u754c\u9650\u548c\u6a21\u62df\u7ed3\u679c\u4e0e\u72ec\u7acb\u7684 Spark \u7cfb\u7edf\u4e2d\u7684\u57fa\u51c6\u6570\u636e\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7814\u7a76\u4e86\u771f\u5b9e\u7cfb\u7edf\u4e2d\u7684\u5f00\u9500\uff0c\u5e76\u57fa\u4e8e\u5f00\u9500\u7684\u5206\u5e03\u5c06\u5176\u5f52\u56e0\u4e8e\u7528\u4e8e\u8c03\u5ea6\u5c4f\u969c\u6a21\u5f0f\u4f5c\u4e1a\u7684\u53cc\u91cd\u4e8b\u4ef6\u548c\u8f6e\u8be2\u9a71\u52a8\u673a\u5236\u3002\u5e76\u9488\u5bf9\u6027\u5730\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u578b\uff0c\u5e76\u5bf9\u7167\u771f\u5b9e\u7cfb\u7edf\u8fdb\u884c\u6a21\u62df\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u4e86\u5141\u8bb8\u90e8\u5206\u4efb\u52a1\u5b8c\u6210\u540e\u79bb\u5f00\u7684 $(s,k,l)$ \u5c4f\u969c\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u63a8\u5bfc\u5e76\u8bc4\u4f30\u4e86\u6df7\u5408\u5c4f\u969c\u7cfb\u7edf\u7684\u6027\u80fd\u754c\u9650\u3002\u5bf9\u4e8e\u7eaf\u7cb9\u7684 1 \u5c4f\u969c\u60c5\u51b5\uff0c\u6027\u80fd\u754c\u9650\u548c\u6a21\u62df\u7ed3\u679c\u4e0e\u72ec\u7acb\u7684 Spark \u7cfb\u7edf\u7684\u57fa\u51c6\u6570\u636e\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u5728\u771f\u5b9e\u7cfb\u7edf\u4e2d\uff0c\u89c2\u5bdf\u5230\u7684\u5f00\u9500\u88ab\u5f52\u56e0\u4e8e\u7528\u4e8e\u8c03\u5ea6\u5c4f\u969c\u6a21\u5f0f\u4f5c\u4e1a\u7684\u53cc\u91cd\u4e8b\u4ef6\u548c\u8f6e\u8be2\u9a71\u52a8\u673a\u5236\u3002\u4f5c\u8005\u6700\u7ec8\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u578b\u6765\u9884\u6d4b\u548c\u9a8c\u8bc1\u8fd9\u79cd\u7c7b\u578b\u7684\u5f00\u9500\u3002", "conclusion": "\u672c\u6587\u5206\u6790\u4e86\u5e76\u884c\u8ba1\u7b97\u4e2d\u540c\u6b65\u5c4f\u969c\u5bf9\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7814\u7a76\u4e86\u5141\u8bb8\u90e8\u5206\u4efb\u52a1\u5b8c\u6210\u540e\u79bb\u5f00\u7684\u5c4f\u969c\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\uff0c\u4ee5\u53ca\u6df7\u5408\u5c4f\u969c\u7cfb\u7edf\uff08\u5305\u542b\u6709\u5c4f\u969c\u548c\u65e0\u5c4f\u969c\u4f5c\u4e1a\uff09\u7684\u6027\u80fd\u754c\u9650\u3002\u5bf9\u4e8e\u7eaf\u7cb9\u7684 1 \u5c4f\u969c\u60c5\u51b5\uff0c\u5c06\u754c\u9650\u548c\u6a21\u62df\u7ed3\u679c\u4e0e\u72ec\u7acb\u7684 Spark \u7cfb\u7edf\u7684\u57fa\u51c6\u6570\u636e\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5e76\u7814\u7a76\u4e86\u771f\u5b9e\u7cfb\u7edf\u4e2d\u7684\u5f00\u9500\u3002\u6700\u540e\uff0c\u4f5c\u8005\u8ba4\u4e3a\u5f00\u9500\u6e90\u4e8e Spark \u4e2d\u7528\u4e8e\u8c03\u5ea6\u5c4f\u969c\u6a21\u5f0f\u4f5c\u4e1a\u7684\u53cc\u91cd\u4e8b\u4ef6\u548c\u8f6e\u8be2\u9a71\u52a8\u673a\u5236\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u6a21\u578b\u6765\u9a8c\u8bc1\u3002"}}
{"id": "2512.14628", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.14628", "abs": "https://arxiv.org/abs/2512.14628", "authors": ["Alireza Olama", "Andreas Lundell", "Izzat El Hajj", "Johan Lilius", "Jerker Bj\u00f6rkqvist"], "title": "PruneX: A Hierarchical Communication-Efficient System for Distributed CNN Training with Structured Pruning", "comment": null, "summary": "Inter-node communication bandwidth increasingly constrains distributed training at scale on multi-node GPU clusters. While compact models are the ultimate deployment target, conventional pruning-aware distributed training systems typically fail to reduce communication overhead because unstructured sparsity cannot be efficiently exploited by highly optimized dense collective primitives. We present PruneX, a distributed data-parallel training system that co-designs pruning algorithms with cluster hierarchy to reduce inter-node bandwidth usage. PruneX introduces the Hierarchical Structured ADMM (H-SADMM) algorithm, which enforces node-level structured sparsity before inter-node synchronization, enabling dynamic buffer compaction that eliminates both zero-valued transmissions and indexing overhead. The system adopts a leader-follower execution model with separated intra-node and inter-node process groups, performing dense collectives on compacted tensors over bandwidth-limited links while confining full synchronization to high-bandwidth intra-node interconnects. Evaluation on ResNet architectures across 64 GPUs demonstrates that PruneX reduces inter-node communication volume by approximately 60% and achieves 6.75x strong scaling speedup, outperforming the dense baseline (5.81x) and Top-K gradient compression (3.71x) on the Puhti supercomputer at CSC - IT Center for Science (Finland).", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e0e\u7f16\u8bd1\u5668\u3001DL/ML/AI \u76f8\u5173\u3002DL/ML/AI \u76f8\u5173\u90e8\u5206\u662f\u5176\u63d0\u51fa\u7684 PruneX \u7cfb\u7edf\u7528\u4e8e\u89e3\u51b3\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\u3002\n\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PruneX \u7684\u5206\u5e03\u5f0f\u6570\u636e\u5e76\u884c\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u534f\u540c\u8bbe\u8ba1\u526a\u679d\u7b97\u6cd5\u548c\u96c6\u7fa4\u5c42\u6b21\u7ed3\u6784\u6765\u51cf\u5c11\u8282\u70b9\u95f4\u901a\u4fe1\u5e26\u5bbd\u7684\u4f7f\u7528\u3002PruneX \u5f15\u5165\u4e86\u5206\u5c42\u7ed3\u6784\u5316 ADMM\uff08H-SADMM\uff09\u7b97\u6cd5\uff0c\u5728\u8fdb\u884c\u8282\u70b9\u95f4\u540c\u6b65\u524d\u5f3a\u5236\u6267\u884c\u8282\u70b9\u7ea7\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u52a8\u6001\u7f13\u51b2\u538b\u7f29\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u91cf (\u7ea6 60%)\u3002\u7cfb\u7edf\u91c7\u7528 leader-follower \u6267\u884c\u6a21\u578b\uff0c\u5c06\u5bc6\u96c6\u96c6\u5408\u64cd\u4f5c\u5e94\u7528\u4e8e\u538b\u7f29\u540e\u7684\u5f20\u91cf\uff0c\u5b9e\u73b0\u4e86\u76f8\u5bf9\u4e8e\u5bc6\u96c6\u57fa\u7ebf\u548c Top-K \u68af\u5ea6\u538b\u7f29\u66f4\u597d\u7684\u5f3a\u6269\u5c55\u6027\u52a0\u901f\u6bd4\uff086.75 \u500d\uff09\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u591a\u8282\u70b9 GPU \u96c6\u7fa4\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\u65f6\uff0c\u8282\u70b9\u95f4\u7684\u901a\u4fe1\u5e26\u5bbd\u65e5\u76ca\u6210\u4e3a\u74f6\u9888\u3002\u4f20\u7edf\u7684\u526a\u679d\u611f\u77e5\u5206\u5e03\u5f0f\u8bad\u7ec3\u7cfb\u7edf\u65e0\u6cd5\u6709\u6548\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u56e0\u4e3a\u9ad8\u5ea6\u4f18\u5316\u7684\u5bc6\u96c6\u96c6\u5408\u901a\u4fe1\u539f\u8bed\uff08collective primitives\uff09\u4e0d\u80fd\u9ad8\u6548\u5730\u5229\u7528\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u8bbe\u8ba1\u4e00\u79cd\u65b0\u7684\u7cfb\u7edf\u6765\u51cf\u5c11\u8282\u70b9\u95f4\u7684\u901a\u4fe1\u5e26\u5bbd\u4f7f\u7528\u3002", "method": "PruneX \u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u534f\u540c\u8bbe\u8ba1\u526a\u679d\u7b97\u6cd5\u548c\u96c6\u7fa4\u5c42\u6b21\u7ed3\u6784\uff0c\u4ee5\u51cf\u5c11\u8282\u70b9\u95f4\u5e26\u5bbd\u7684\u4f7f\u7528\u3002\u5177\u4f53\u5305\u62ec\uff1a1. \u5f15\u5165\u5206\u5c42\u7ed3\u6784\u5316 ADMM\uff08H-SADMM\uff09\u7b97\u6cd5\uff0c\u5728\u8282\u70b9\u95f4\u540c\u6b65\u524d\u5f3a\u5236\u6267\u884c\u8282\u70b9\u7ea7\u7ed3\u6784\u5316\u7a00\u758f\u6027\u30022. H-SADMM \u7b97\u6cd5\u5b9e\u73b0\u4e86\u52a8\u6001\u7f13\u51b2\u533a\u538b\u7f29\uff0c\u6d88\u9664\u4e86\u96f6\u503c\u4f20\u8f93\u548c\u7d22\u5f15\u5f00\u9500\u30023. \u7cfb\u7edf\u91c7\u7528\u201c\u9886\u5bfc\u8005-\u8ffd\u968f\u8005\u201d\uff08leader-follower\uff09\u6267\u884c\u6a21\u578b\uff0c\u5206\u79bb\u8282\u70b9\u5185\u548c\u8282\u70b9\u95f4\u8fdb\u7a0b\u7ec4\uff0c\u5728\u5e26\u5bbd\u53d7\u9650\u7684\u94fe\u8def\u4e0a\u5bf9\u538b\u7f29\u540e\u7684\u5f20\u91cf\u6267\u884c\u5bc6\u96c6\u96c6\u5408\u64cd\u4f5c\uff0c\u540c\u65f6\u5c06\u5b8c\u5168\u540c\u6b65\u9650\u5236\u5728\u9ad8\u5e26\u5bbd\u7684\u8282\u70b9\u5185\u4e92\u8fde\u4e0a\u3002", "result": "\u5728 64 \u4e2a GPU \u4e0a\u5bf9 ResNet \u67b6\u6784\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cPruneX \u5c06\u8282\u70b9\u95f4\u901a\u4fe1\u91cf\u51cf\u5c11\u4e86\u7ea6 60%\u3002\u5728 Puhti \u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\uff0cPruneX \u5b9e\u73b0\u4e86 6.75 \u500d\u7684\u5f3a\u6269\u5c55\u6027\uff08strong scaling\uff09\u52a0\u901f\u6bd4\uff0c\u4f18\u4e8e\u5bc6\u96c6\u57fa\u7ebf\uff085.81 \u500d\uff09\u548c Top-K \u68af\u5ea6\u538b\u7f29\uff083.71 \u500d\uff09\u3002", "conclusion": "PruneX \u901a\u8fc7\u7ed3\u5408\u526a\u679d\u7b97\u6cd5\u548c\u96c6\u7fa4\u5c42\u6b21\u7ed3\u6784\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u8282\u70b9\u95f4\u901a\u4fe1\u5e26\u5bbd\u74f6\u9888\u95ee\u9898\u3002\u5176\u6838\u5fc3 H-SADMM \u7b97\u6cd5\u5b9e\u73b0\u4e86\u8282\u70b9\u7ea7\u7684\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff0c\u4f7f\u5f97\u5728\u8282\u70b9\u95f4\u540c\u6b65\u524d\u53ef\u4ee5\u52a8\u6001\u538b\u7f29\u901a\u4fe1\u7f13\u51b2\u533a\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u91cf\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5f3a\u6269\u5c55\u6027\u52a0\u901f\u6bd4\u3002"}}
{"id": "2512.14390", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.14390", "abs": "https://arxiv.org/abs/2512.14390", "authors": ["Jakub Balab\u00e1n"], "title": "Finding $b$-colorings Using Feedback Edges", "comment": null, "summary": "A $b$-coloring of a graph is a proper vertex coloring such that each color class contains a vertex that sees all other colors in its neighborhood. The $b$-coloring problem, in which the task is to decide whether a graph admits a $b$-coloring with $k$ colors, is NP-complete in general but polytime solvable on trees. Moreover, it is known that $b$-coloring is in XP but W[$t$]-hard for all $t \\in \\mathbb{N}$ when parameterized by tree-width. In fact, only very few parameters, such as the vertex cover number, were known to admit an FPT algorithm for $b$-coloring. In this paper, we consider a more restrictive parameter measuring similarity to trees than tree-width, namely the feedback edge number, and show that $b$-coloring is fixed-parameter tractable under this parameterization. Our algorithm combines standard techniques used in parameterized algorithmics with the problem-specific ideas used in the polytime algorithm for trees. In addition, we present an FPT algorithm for $b$-coloring parameterized by distance to co-cluster, which is a parameter measuring similarity to complete multipartite graphs. Finally, we make several observations based on known results, including that $b$-coloring is W[$1$]-hard when parameterized by tree-depth.", "AI": {"tldr": "The paper is related to graph processing. The paper investigates the parameterized complexity of the $b$-coloring problem. The main findings are that $b$-coloring is Fixed-Parameter Tractable (FPT) when parameterized by the feedback edge number (a parameter more restrictive than treewidth, measuring similarity to trees) and when parameterized by the distance to co-cluster (measuring similarity to complete multipartite graphs). It also confirms that $b$-coloring is W[$1$]-hard when parameterized by tree-depth.", "motivation": "$b$-\u7740\u8272\u95ee\u9898\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u662f NP-\u5b8c\u5168\u7684\uff0c\u4f46\u5728\u6811\u4e0a\u662f\u591a\u9879\u5f0f\u65f6\u95f4\u53ef\u89e3\u7684\u3002\u6b64\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8e $b$-\u7740\u8272\u95ee\u9898\uff0c\u53ea\u6709\u5c11\u6570\u53c2\u6570\uff08\u5982\u70b9\u8986\u76d6\u6570\uff09\u53ef\u4ee5\u5f97\u5230 FPT \u7b97\u6cd5\u3002\u800c\u57fa\u4e8e\u6811\u5bbd\u7684\u53c2\u6570\u5316\u662f W[$t$]-\u96be\u7684\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u627e\u5230\u66f4\u5177\u9650\u5236\u6027\u7684\u3001\u80fd\u5bfc\u81f4 $b$-\u7740\u8272\u95ee\u9898\u5728\u56fa\u5b9a\u53c2\u6570\u53ef\u89e3\u7684\u53c2\u6570\u5316\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u5176\u590d\u6742\u6027\u3002", "method": "\u672c\u6587\u9996\u5148\u8bc1\u660e\u4e86\u57fa\u4e8e\u53cd\u9988\u8fb9\u6570\u53c2\u6570\u7684 $b$-\u7740\u8272\u95ee\u9898\u662f FPT\uff0c\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u53c2\u6570\u5316\u7b97\u6cd5\u7684\u6807\u51c6\u6280\u672f\u548c\u6811\u4e0a\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u7684\u7279\u5b9a\u60f3\u6cd5\u3002\u5176\u6b21\uff0c\u672c\u6587\u8bc1\u660e\u4e86\u57fa\u4e8e\u5230\u5171\u805a\u7c7b\u8ddd\u79bb\u53c2\u6570\u7684 $b$-\u7740\u8272\u95ee\u9898\u662f FPT\u3002\u6700\u540e\uff0c\u672c\u6587\u57fa\u4e8e\u5df2\u77e5\u7ed3\u679c\u63d0\u51fa\u4e86\u5173\u4e8e\u6811\u6df1\u53c2\u6570\u5316\u7684 $b$-\u7740\u8272\u95ee\u9898\u7684\u89c2\u5bdf\u3002", "result": "\u672c\u6587\u53d6\u5f97\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ed3\u679c\uff1a1. \u57fa\u4e8e\u53cd\u9988\u8fb9\u6570\uff08\u4e00\u4e2a\u6bd4\u6811\u5bbd\u66f4\u4e25\u683c\u7684\u3001\u8861\u91cf\u4e0e\u6811\u76f8\u4f3c\u6027\u7684\u53c2\u6570\uff09\u53c2\u6570\u5316\u7684 $b$-\u7740\u8272\u95ee\u9898\u662f\u56fa\u5b9a\u53c2\u6570\u53ef\u89e3\u7684\uff08FPT\uff09\u30022. \u57fa\u4e8e\u5230\u5171\u805a\u7c7b\u8ddd\u79bb\uff08\u4e00\u4e2a\u8861\u91cf\u4e0e\u5b8c\u5168\u591a\u90e8\u56fe\u76f8\u4f3c\u6027\u7684\u53c2\u6570\uff09\u53c2\u6570\u5316\u7684 $b$-\u7740\u8272\u95ee\u9898\u662f FPT\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u89c2\u5bdf\u5230\u57fa\u4e8e\u6811\u6df1\u53c2\u6570\u5316\u7684 $b$-\u7740\u8272\u95ee\u9898\u662f W[$1$]-\u96be\u7684\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u4e86\u56fe\u7684 $b$-\u7740\u8272\u95ee\u9898\u7684\u53c2\u6570\u5316\u590d\u6742\u5ea6\u3002\u4e3b\u8981\u7ed3\u8bba\u662f\u57fa\u4e8e\u53cd\u9988\u8fb9\u6570\u548c\u5230\u5171\u805a\u7c7b\u7684\u8ddd\u79bb\u8fd9\u4e24\u4e2a\u53c2\u6570\uff0c$b$-\u7740\u8272\u95ee\u9898\u662f\u56fa\u5b9a\u53c2\u6570\u53ef\u89e3\u7684\uff08FPT\uff09\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u89c2\u5bdf\u5230\u57fa\u4e8e\u6811\u6df1\u53c2\u6570\u7684 $b$-\u7740\u8272\u95ee\u9898\u662f W[$1$]-\u96be\u7684\u3002"}}
{"id": "2512.14172", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.14172", "abs": "https://arxiv.org/abs/2512.14172", "authors": ["Qijun Zhang", "Shang Liu", "Yao Lu", "Mengming Li", "Zhiyao Xie"], "title": "ReadyPower: A Reliable, Interpretable, and Handy Architectural Power Model Based on Analytical Framework", "comment": "Accepted by ASP-DAC'26", "summary": "Power is a primary objective in modern processor design, requiring accurate yet efficient power modeling techniques. Architecture-level power models are necessary for early power optimization and design space exploration. However, classical analytical architecture-level power models (e.g., McPAT) suffer from significant inaccuracies. Emerging machine learning (ML)-based power models, despite their superior accuracy in research papers, are not widely adopted in the industry. In this work, we point out three inherent limitations of ML-based power models: unreliability, limited interpretability, and difficulty in usage. This work proposes a new analytical power modeling framework named ReadyPower, which is ready-for-use by being reliable, interpretable, and handy. We observe that the root cause of the low accuracy of classical analytical power models is the discrepancies between the real processor implementation and the processor's analytical model. To bridge the discrepancies, we introduce architecture-level, implementation-level, and technology-level parameters into the widely adopted McPAT analytical model to build ReadyPower. The parameters at three different levels are decided in different ways. In our experiment, averaged across different training scenarios, ReadyPower achieves >20% lower mean absolute percentage error (MAPE) and >0.2 higher correlation coefficient R compared with the ML-based baselines, on both BOOM and XiangShan CPU architectures.baselines, on both BOOM and XiangShan CPU architectures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e0e\u7f16\u8bd1\u5668\u76f8\u5173\uff0c\u56e0\u4e3a\u5b83\u6d89\u53ca\u5904\u7406\u5668\u8bbe\u8ba1\u548c\u67b6\u6784\u7ea7\u5efa\u6a21\uff0c\u8fd9\u4e0e\u7f16\u8bd1\u5668\u4f18\u5316\u7684\u76ee\u6807\u7d27\u5bc6\u76f8\u5173\u3002\u8be5\u8bba\u6587\u4e0e DSL \u6216\u56fe\u5904\u7406\u6216 MLIR \u6216 HLS \u65e0\u76f4\u63a5\u5173\u7cfb\u3002\n\u73b0\u6709\u7684\u7ecf\u5178\u5206\u6790\u529f\u8017\u6a21\u578b\u4e0d\u51c6\u786e\uff0c\u800c\u57fa\u4e8e ML \u7684\u529f\u8017\u6a21\u578b\u56e0\u5176\u56fa\u6709\u7684\u5c40\u9650\u6027\uff08\u4e0d\u53ef*\u9760\u6027\u3001\u53ef\u89e3\u91ca\u6027\u6709\u9650\u548c\u4f7f\u7528\u56f0\u96be\uff09\u5728\u5de5\u4e1a\u754c\u6ca1\u6709\u88ab\u5e7f\u6cdb\u91c7\u7528\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5206\u6790\u6027\u529f\u8017\u5efa\u6a21\u6846\u67b6 ReadyPower\uff0c\u5b83\u901a\u8fc7\u5728 McPAT \u6a21\u578b\u4e2d\u5f15\u5165\u67b6\u6784\u7ea7\u3001\u5b9e\u73b0\u7ea7\u548c\u6280\u672f\u7ea7\u53c2\u6570\uff0c\u5f25\u5408\u4e86\u6a21\u578b\u4e0e\u5b9e\u9645\u5b9e\u73b0\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cReadyPower \u5728 BOOM \u548c\u9999\u5c71 CPU \u67b6\u6784\u4e0a\u5747\u4f18\u4e8e\u57fa\u4e8e ML \u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3001\u53ef\u89e3\u91ca\u6027\u4ee5\u53ca\u6613\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u7ecf\u5178\u5206\u6790\u67b6\u6784\u7ea7\u529f\u8017\u6a21\u578b\uff08\u5982 McPAT\uff09\u5b58\u5728\u4e25\u91cd\u4e0d\u51c6\u786e\u6027\uff0c\u800c\u65b0\u5174\u7684\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u7684\u529f\u8017\u6a21\u578b\u867d\u7136\u5728\u7814\u7a76\u8bba\u6587\u4e2d\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5728\u4e1a\u754c\u6ca1\u6709\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u56e0\u4e3a\u5b83\u4eec\u5b58\u5728\u56fa\u6709\u7684\u5c40\u9650\u6027\uff1a\u4e0d\u53ef\u9760\u6027\u3001\u53ef\u89e3\u91ca\u6027\u6709\u9650\u548c\u4f7f\u7528\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u51c6\u786e\u53c8\u9ad8\u6548\uff0c\u540c\u65f6\u5177\u6709\u53ef\u9760\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6613\u7528\u6027\u7684\u65b0\u7684\u529f\u8017\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86 ReadyPower \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5728\u5e7f\u6cdb\u91c7\u7528\u7684 McPAT \u5206\u6790\u6a21\u578b\u4e2d\u5f15\u5165\u67b6\u6784\u7ea7\u3001\u5b9e\u73b0\u7ea7\u548c\u6280\u672f\u7ea7\u53c2\u6570\u6765\u5f25\u5408\u7ecf\u5178\u5206\u6790\u529f\u8017\u6a21\u578b\u4e0e\u5b9e\u9645\u5904\u7406\u5668\u5b9e\u73b0\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u8fd9\u4e9b\u53c2\u6570\u901a\u8fc7\u4e0d\u540c\u7684\u65b9\u5f0f\u786e\u5b9a\u3002", "result": "ReadyPower \u5728\u4e0d\u540c\u7684\u8bad\u7ec3\u573a\u666f\u4e0b\uff0c\u5728 BOOM \u548c\u9999\u5c71 CPU \u67b6\u6784\u4e0a\uff0c\u4e0e\u57fa\u4e8e ML \u7684\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0c\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\uff08MAPE\uff09\u964d\u4f4e\u4e86 20% \u4ee5\u4e0a\uff0c\u76f8\u5173\u7cfb\u6570 R \u63d0\u9ad8\u4e86 0.2 \u4ee5\u4e0a\u3002", "conclusion": "ReadyPower \u662f\u4e00\u79cd\u65b0\u7684\u5206\u6790\u6027\u529f\u8017\u5efa\u6a21\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5f15\u5165\u67b6\u6784\u7ea7\u3001\u5b9e\u73b0\u7ea7\u548c\u6280\u672f\u7ea7\u53c2\u6570\u6765\u5f25\u5408\u7ecf\u5178\u5206\u6790\u6a21\u578b\uff08\u5982 McPAT\uff09\u4e0e\u5b9e\u9645\u5904\u7406\u5668\u5b9e\u73b0\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4ece\u800c\u514b\u670d\u4e86\u73b0\u6709 ML \u529f\u8017\u6a21\u578b\u7684\u5c40\u9650\u6027\uff08\u4e0d\u53ef\u9760\u6027\u3001\u53ef\u89e3\u91ca\u6027\u6709\u9650\u3001\u4f7f\u7528\u56f0\u96be\uff09\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cReadyPower \u5728 BOOM \u548c\u9999\u5c71 CPU \u67b6\u6784\u4e0a\u5747\u4f18\u4e8e\u57fa\u4e8e ML \u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5e73\u5747 MAPE \u4f4e\u4e8e 20%\uff0c\u76f8\u5173\u7cfb\u6570 R \u9ad8\u4e8e 0.2\u3002"}}
{"id": "2512.14409", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.14409", "abs": "https://arxiv.org/abs/2512.14409", "authors": ["Michelle D\u00f6ring", "Jannes Malanowski", "Stefan Neubert"], "title": "Cost-Free Neutrality for the River Method", "comment": "appears at AAAI 2026", "summary": "Recently, the River Method was introduced as novel refinement of the Split Cycle voting rule.\n  The decision-making process of River is closely related to the well established Ranked Pairs Method.\n  Both methods consider a margin graph computed from the voters' preferences and eliminate majority cycles in that graph to choose a winner.\n  As ties can occur in the margin graph, a tiebreaker is required along with the preferences.\n  While such a tiebreaker makes the computation efficient, it compromises the fundamental property of neutrality: the voting rule should not favor alternatives in advance.\n  One way to reintroduce neutrality is to use Parallel-Universe Tiebreaking (PUT), where each alternative is a winner if it wins according to any possible tiebreaker.\n  Unfortunately, computing the winners selected by Ranked Pairs with PUT is NP-complete.\n  Given the similarity of River to Ranked Pairs, one might expect River to suffer from the same complexity.\n  Surprisingly, we show the opposite:\n  We present a polynomial-time algorithm for computing River winners with PUT, highlighting significant structural advantages of River over Ranked Pairs.\n  Our Fused-Universe (FUN) algorithm simulates River for every possible tiebreaking in one pass.\n  From the resulting FUN diagram one can then directly read off both the set of winners and, for each winner, a certificate that explains how this alternative dominates the others.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e0eDSL\u6216\u56fe\u5904\u7406\u6216MLIR\u6216\u7f16\u8bd1\u5668\u6216HLS\u4e0d\u76f8\u5173\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u300a\u6cb3\u6d41\u65b9\u6cd5\u300b\uff0c\u5b83\u662f\u300a\u5206\u88c2\u5faa\u73af\u6295\u7968\u89c4\u5219\u300b\u7684\u4e00\u79cd\u6539\u8fdb\uff0c\u5e76\u4e0e\u300a\u6392\u540d\u5bf9\u65b9\u6cd5\u300b\u76f8\u4f3c\u3002\u5b83\u4eec\u90fd\u901a\u8fc7\u6d88\u9664\u8fb9\u7f18\u56fe\u4e2d\u7684\u591a\u6570\u5faa\u73af\u6765\u9009\u62e9\u83b7\u80dc\u8005\u3002\u4e3a\u4e86\u6062\u590d\u4e2d\u7acb\u6027\uff0c\u8bba\u6587\u8003\u8651\u4e86\u201c\u5e76\u884c\u5b87\u5b99\u7834\u5e73\u5c40\u201d\uff08PUT\uff09\uff0c\u4f46\u8ba1\u7b97\u5177\u6709PUT\u7684\u300a\u6392\u540d\u5bf9\u65b9\u6cd5\u300b\u662fNP\u5b8c\u5168\u7684\u3002\u672c\u6587\u7684\u4e3b\u8981\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\uff08\u88ab\u79f0\u4e3a\u201c\u878d\u5408\u5b87\u5b99\u201d\u6216FUN\u7b97\u6cd5\uff09\u6765\u8ba1\u7b97\u5177\u6709PUT\u7684\u300a\u6cb3\u6d41\u65b9\u6cd5\u300b\u7684\u83b7\u80dc\u8005\u3002\u8fd9\u8868\u660e\u300a\u6cb3\u6d41\u65b9\u6cd5\u300b\u5728\u7ed3\u6784\u4e0a\u4f18\u4e8e\u300a\u6392\u540d\u5bf9\u65b9\u6cd5\u300b\u3002", "motivation": "\u300a\u6cb3\u6d41\u65b9\u6cd5\u300b\u548c\u5e7f\u4e3a\u4eba\u77e5\u7684\u300a\u6392\u540d\u5bf9\u65b9\u6cd5\u300b\u90fd\u901a\u8fc7\u6d88\u9664\u6295\u7968\u8005\u504f\u597d\u8ba1\u7b97\u5f97\u51fa\u7684\u8fb9\u7f18\u56fe\u4e2d\u7684\u591a\u6570\u5faa\u73af\u6765\u9009\u51fa\u83b7\u80dc\u8005\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8fb9\u7f18\u56fe\u4e2d\u53ef\u80fd\u5b58\u5728\u5e73\u5c40\uff0c\u9700\u8981\u4e00\u4e2a\u7834\u5e73\u5c40\u89c4\u5219\u3002\u4f7f\u7528\u7834\u5e73\u5c40\u89c4\u5219\u867d\u7136\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5374\u635f\u5bb3\u4e86\u4e2d\u7acb\u6027\uff08\u5373\u4e0d\u5e94\u4e8b\u5148\u504f\u8892\u4efb\u4f55\u5019\u9009\u8005\uff09\u3002\u4e3a\u4e86\u91cd\u65b0\u5f15\u5165\u4e2d\u7acb\u6027\uff0c\u53ef\u4ee5\u4f7f\u7528\u5e76\u884c\u5b87\u5b99\u7834\u5e73\u5c40\uff08PUT\uff09\uff0c\u5176\u4e2d\u5982\u679c\u67d0\u4e2a\u9009\u9879\u5728\u4efb\u4f55\u53ef\u80fd\u7684\u7834\u5e73\u5c40\u89c4\u5219\u4e0b\u83b7\u80dc\uff0c\u5219\u5b83\u5c31\u662f\u83b7\u80dc\u8005\u3002\u7136\u800c\uff0c\u8ba1\u7b97\u5177\u6709PUT\u7684\u300a\u6392\u540d\u5bf9\u65b9\u6cd5\u300b\u83b7\u80dc\u8005\u662fNP\u5b8c\u5168\u7684\u3002\u9274\u4e8e\u300a\u6cb3\u6d41\u65b9\u6cd5\u300b\u4e0e\u300a\u6392\u540d\u5bf9\u65b9\u6cd5\u300b\u7684\u76f8\u4f3c\u6027\uff0c\u4eba\u4eec\u53ef\u80fd\u4f1a\u9884\u671f\u300a\u6cb3\u6d41\u65b9\u6cd5\u300b\u4e5f\u4f1a\u9762\u4e34\u540c\u6837\u7684\u590d\u6742\u6027\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u672c\u6587\u7684\u52a8\u673a\u662f\u63a2\u7d22\u300a\u6cb3\u6d41\u65b9\u6cd5\u300b\u662f\u5426\u5177\u6709\u76f8\u4f3c\u7684\u590d\u6742\u6027\uff0c\u5e76\u8bd5\u56fe\u627e\u5230\u4e00\u79cd\u6709\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u878d\u5408\u5b87\u5b99\u201d\uff08FUN\uff09\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u4e00\u6b21\u6027\u6a21\u62df\u6bcf\u79cd\u53ef\u80fd\u7684\u7834\u5e73\u5c40\u60c5\u51b5\u4e0b\u7684\u300a\u6cb3\u6d41\u65b9\u6cd5\u300b\u8ba1\u7b97\u3002\u901a\u8fc7\u7531\u6b64\u4ea7\u751f\u7684FUN\u56fe\uff0c\u53ef\u4ee5\u76f4\u63a5\u8bfb\u51fa\u83b7\u80dc\u8005\u96c6\u5408\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u83b7\u80dc\u8005\u63d0\u4f9b\u4e00\u4e2a\u89e3\u91ca\u5176\u5982\u4f55\u4f18\u4e8e\u5176\u4ed6\u9009\u9879\u7684\u8bc1\u660e\u3002", "result": "\u8bc1\u660e\u4e86\u4e0e\u5177\u6709PUT\u7684\u300a\u6392\u540d\u5bf9\u65b9\u6cd5\u300b\u662fNP\u5b8c\u5168\u95ee\u9898\u76f8\u53cd\uff0c\u8ba1\u7b97\u5177\u6709\u5e76\u884c\u5b87\u5b99\u7834\u5e73\u5c40\uff08PUT\uff09\u7684\u300a\u6cb3\u6d41\u65b9\u6cd5\u300b\u83b7\u80dc\u8005\u53ef\u4ee5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5b8c\u6210\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u878d\u5408\u5b87\u5b99\u201d\uff08FUN\uff09\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u6765\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u5e76\u80fd\u591f\u4e3a\u83b7\u80dc\u8005\u63d0\u4f9b\u89e3\u91ca\u5176\u4f18\u52bf\u7684\u8bc1\u660e\u3002", "conclusion": "\u300a\u6cb3\u6d41\u65b9\u6cd5\u300b\u5728\u5f15\u5165\u5e76\u884c\u5b87\u5b99\u7834\u9664\u5e73\u5c40\uff08PUT\uff09\u540e\uff0c\u8ba1\u7b97\u5176\u83b7\u80dc\u8005\u53ef\u4ee5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5b8c\u6210\uff0c\u8fd9\u4e0e\u8ba1\u7b97\u5177\u6709PUT\u7684\u300a\u6392\u540d\u5bf9\u65b9\u6cd5\u300b\u662fNP\u5b8c\u5168\u95ee\u9898\u5f62\u6210\u4e86\u9c9c\u660e\u5bf9\u6bd4\u3002\u8fd9\u8868\u660e\u300a\u6cb3\u6d41\u65b9\u6cd5\u300b\u76f8\u5bf9\u4e8e\u300a\u6392\u540d\u5bf9\u65b9\u6cd5\u300b\u5177\u6709\u663e\u8457\u7684\u7ed3\u6784\u4f18\u52bf\u3002"}}
{"id": "2512.14256", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.14256", "abs": "https://arxiv.org/abs/2512.14256", "authors": ["Huizheng Wang", "Taiquan Wei", "Zichuan Wang", "Dingcheng Jiang", "Qize Yang", "Jiaxin Liu", "Jingxiang Hou", "Chao Li", "Jinyi Deng", "Yang Hu", "Shouyi Yin"], "title": "TEMP: A Memory Efficient Physical-aware Tensor Partition-Mapping Framework on Wafer-scale Chips", "comment": "Accepted by HPCA 2026", "summary": "Large language models (LLMs) demand significant memory and computation resources. Wafer-scale chips (WSCs) provide high computation power and die-to-die (D2D) bandwidth but face a unique trade-off between on-chip memory and compute resources due to limited wafer area. Therefore, tensor parallelism strategies for wafer should leverage communication advantages while maintaining memory efficiency to maximize WSC performance. However, existing approaches fail to address these challenges.\n  To address these challenges, we propose the tensor stream partition paradigm (TSPP), which reveals an opportunity to leverage WSCs' abundant communication bandwidth to alleviate stringent on-chip memory constraints. However, the 2D mesh topology of WSCs lacks long-distance and flexible interconnects, leading to three challenges: 1) severe tail latency, 2) prohibitive D2D traffic contention, and 3) intractable search time for optimal design.\n  We present TEMP, a framework for LLM training on WSCs that leverages topology-aware tensor-stream partition, traffic-conscious mapping, and dual-level wafer solving to overcome hardware constraints and parallelism challenges. These integrated approaches optimize memory efficiency and throughput, unlocking TSPP's full potential on WSCs. Evaluations show TEMP achieves 1.7x average throughput improvement over state-of-the-art LLM training systems across various models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e0eGraph Processing\u548cCompiler\u4e0d\u76f8\u5173\uff0c\u4e0eMLIR\u548cHLS\u4e0d\u76f8\u5173\uff0c\u4e0eDSL\u4e0d\u76f8\u5173\uff0c\u4e0eCompiler\u76f8\u5173\uff08\u4e0e\u9ad8\u6027\u80fd\u8ba1\u7b97\u548c\u5e76\u884c\u5316\u76f8\u5173\uff09\u3002\n\u603b\u7ed3\uff1a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u5de8\u5927\uff0c\u6676\u5706\u7ea7\u82af\u7247\uff08WSCs\uff09\u867d\u7136\u63d0\u4f9b\u9ad8\u8ba1\u7b97\u80fd\u529b\u548cD2D\u5e26\u5bbd\uff0c\u4f46\u5728\u7247\u4e0a\u5185\u5b58\u548c\u8ba1\u7b97\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u5f20\u91cf\u6d41\u5212\u5206\u8303\u5f0f\uff08TSPP\uff09\u5229\u7528WSC\u7684\u901a\u4fe1\u4f18\u52bf\u7f13\u89e3\u5185\u5b58\u9650\u5236\u3002\u7136\u800c\uff0cWSC\u76842D\u7f51\u683c\u62d3\u6251\u5e26\u6765\u4e86\u5c3e\u5ef6\u8fdf\u3001\u6d41\u91cf\u7ade\u4e89\u548c\u4f18\u5316\u641c\u7d22\u7684\u6311\u6218\u3002\u4e3a\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86TEMP\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u62d3\u6251\u611f\u77e5\u5212\u5206\u3001\u6d41\u91cf\u611f\u77e5\u6620\u5c04\u548c\u53cc\u5c42\u6676\u5706\u6c42\u89e3\u65b9\u6cd5\uff0c\u65e8\u5728\u4f18\u5316\u5185\u5b58\u6548\u7387\u548c\u541e\u5410\u91cf\u3002\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cTEMP\u5728LLM\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709SOTA\u7cfb\u7edf\u5e73\u5747\u9ad81.7\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9700\u8981\u5927\u91cf\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u6676\u5706\u7ea7\u82af\u7247\uff08WSCs\uff09\u63d0\u4f9b\u4e86\u9ad8\u8ba1\u7b97\u80fd\u529b\u548c\u82af\u7247\u95f4\uff08D2D\uff09\u5e26\u5bbd\uff0c\u4f46\u7531\u4e8e\u6709\u9650\u7684\u6676\u5706\u9762\u79ef\uff0c\u5728\u7247\u4e0a\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u4e4b\u95f4\u5b58\u5728\u72ec\u7279\u7684\u6743\u8861\u3002\u56e0\u6b64\uff0cWSC\u7684\u5f20\u91cf\u5e76\u884c\u7b56\u7565\u9700\u8981\u5229\u7528\u901a\u4fe1\u4f18\u52bf\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5b58\u6548\u7387\uff0c\u4ee5\u6700\u5927\u5316WSC\u6027\u80fd\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002\u6b64\u5916\uff0cWSC\u76842D\u7f51\u683c\u62d3\u6251\u7f3a\u4e4f\u957f\u8ddd\u79bb\u548c\u7075\u6d3b\u7684\u4e92\u8fde\uff0c\u5bfc\u81f4\u4e86\u4e09\u4e2a\u6311\u6218\uff1a\u4e25\u91cd\u7684\u5c3e\u5ef6\u8fdf\u3001\u7981\u6b62\u7684D2D\u6d41\u91cf\u7ade\u4e89\u4ee5\u53ca\u6700\u4f73\u8bbe\u8ba1\u96be\u4ee5\u5904\u7406\u7684\u641c\u7d22\u65f6\u95f4\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u5f20\u91cf\u6d41\u5212\u5206\u8303\u5f0f\uff08TSPP\uff09\uff0c\u5b83\u5229\u7528WSC\u7684\u9ad8\u901a\u4fe1\u5e26\u5bbd\u6765\u7f13\u89e3\u7247\u4e0a\u5185\u5b58\u9650\u5236\u3002\u4e3a\u514b\u670dWSC\u76842D\u7f51\u683c\u62d3\u6251\u5e26\u6765\u7684\u6311\u6218\uff08\u5c3e\u5ef6\u8fdf\u3001D2D\u6d41\u91cf\u7ade\u4e89\u548c\u641c\u7d22\u65f6\u95f4\uff09\uff0c\u672c\u6587\u63d0\u51fa\u4e86TEMP\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u62ec\u62d3\u6251\u611f\u77e5\u5f20\u91cf\u6d41\u5212\u5206\uff08topology-aware tensor-stream partition\uff09\u3001\u6d41\u91cf\u611f\u77e5\u6620\u5c04\uff08traffic-conscious mapping\uff09\u548c\u53cc\u5c42\u6676\u5706\u6c42\u89e3\uff08dual-level wafer solving\uff09\u8fd9\u4e09\u79cd\u96c6\u6210\u65b9\u6cd5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u65e8\u5728\u4f18\u5316\u5185\u5b58\u6548\u7387\u548c\u541e\u5410\u91cf\uff0c\u5145\u5206\u53d1\u6325TSPP\u5728WSC\u4e0a\u7684\u6f5c\u529b\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684TEMP\u6846\u67b6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cTEMP\u5728\u5404\u79cd\u6a21\u578b\u4e0a\u7684\u5e73\u5747\u541e\u5410\u91cf\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684LLM\u8bad\u7ec3\u7cfb\u7edf\u63d0\u9ad8\u4e861.7\u500d\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86TEMP\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u62d3\u6251\u611f\u77e5\u5f20\u91cf\u6d41\u5212\u5206\u3001\u6d41\u91cf\u611f\u77e5\u6620\u5c04\u548c\u53cc\u5c42\u6676\u5706\u6c42\u89e3\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5728\u6676\u5706\u7ea7\u82af\u7247\u4e0a\u8bad\u7ec3LLM\u65f6\u9762\u4e34\u7684\u5185\u5b58\u9650\u5236\u548c\u5e76\u884c\u6027\u6311\u6218\u3002\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cTEMP\u7684\u5e73\u5747\u541e\u5410\u91cf\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684LLM\u8bad\u7ec3\u7cfb\u7edf\u63d0\u9ad8\u4e861.7\u500d\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6700\u5927\u5316WSC\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.14457", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.14457", "abs": "https://arxiv.org/abs/2512.14457", "authors": ["Jingyang Zhao", "Mingyu Xiao"], "title": "An Improved Approximation Algorithm for Maximum Weight 3-Path Packing", "comment": null, "summary": "Given a complete graph with $n$ vertices and non-negative edge weights, where $n$ is divisible by 3, the maximum weight 3-path packing problem is to find a set of $n/3$ vertex-disjoint 3-paths such that the total weight of the 3-paths in the packing is maximized. This problem is closely related to the classic maximum weight matching problem. In this paper, we propose a $10/17$-approximation algorithm, improving the best-known $7/12$-approximation algorithm (ESA 2015). Our result is obtained by making a trade-off among three algorithms. The first is based on the maximum weight matching of size $n/2$, the second is based on the maximum weight matching of size $n/3$, and the last is based on an approximation algorithm for star packing. Our first algorithm is the same as the previous $7/12$-approximation algorithm, but we propose a new analysis method -- a charging method -- for this problem, which is not only essential to analyze our second algorithm but also may be extended to analyze algorithms for some related problems.", "AI": {"tldr": "\u4e0e\u62bd\u8c61\u4e2d\u6240\u8ff0\u7684\u91cd\u70b9\u9886\u57df\uff08DSL\u3001\u56fe\u5904\u7406\u3001MLIR\u3001\u7f16\u8bd1\u5668\u3001HLS\uff09\u76f8\u5173\u7684\u662f\u56fe\u5904\u7406\u3002\u8be5\u8bba\u6587\u4fa7\u91cd\u4e8e\u56fe\u8bba\u4e2d\u7684\u8fd1\u4f3c\u7b97\u6cd5\u7684\u8bbe\u8ba1\u548c\u5206\u6790\uff0c\u7279\u522b\u662f\u6700\u5927\u6743 3-\u8def\u5f84\u586b\u5145\u95ee\u9898\uff0c\u8fd9\u662f\u4e00\u4e2a\u56fe\u4f18\u5316\u95ee\u9898\u3002\n\n**TL;DR:** (1) The paper is related to **graph processing** (specifically, a graph optimization problem: maximum weight 3-path packing). (2) An improved approximation algorithm with a ratio of $10/17$ is proposed for the maximum weight 3-path packing problem in complete graphs where $n$ is divisible by 3, beating the previous best ratio of $7/12$. The result is achieved by combining three distinct algorithms (based on maximum weight matchings of size $n/2$ and $n/3$, and star packing) and introducing a new analytical technique called the \"charging method.\"", "motivation": "\u89e3\u51b3\u6700\u5927\u6743 3-\u8def\u5f84\u586b\u5145\u95ee\u9898\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u6539\u8fdb\u5176\u8fd1\u4f3c\u6bd4\u3002\u8be5\u95ee\u9898\u662f\u6700\u5927\u6743\u5339\u914d\u95ee\u9898\u7684\u63a8\u5e7f\u3002\u5148\u524d\u7684\u6700\u4f73\u8fd1\u4f3c\u7b97\u6cd5\u4fdd\u6301\u7684\u8fd1\u4f3c\u6bd4\u662f $7/12$\uff0c\u672c\u6587\u65e8\u5728\u63d0\u9ad8\u8fd9\u4e00\u6bd4\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd $10/17$-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u65b9\u6cd5\uff1a1. \u57fa\u4e8e\u5927\u5c0f\u4e3a $n/2$ \u7684\u6700\u5927\u6743\u5339\u914d\uff08\u4e0e\u5148\u524d\u6700\u4f73\u7b97\u6cd5\u76f8\u540c\uff09\u30022. \u57fa\u4e8e\u5927\u5c0f\u4e3a $n/3$ \u7684\u6700\u5927\u6743\u5339\u914d\u30023. \u57fa\u4e8e\u661f\u578b\u586b\u5145\uff08star packing\uff09\u7684\u8fd1\u4f3c\u7b97\u6cd5\u3002\u5bf9\u4e8e\u7b2c\u4e00\u79cd\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6790\u6280\u672f\u2014\u2014\u201c\u5145\u7535\u65b9\u6cd5\u201d\uff0c\u8be5\u65b9\u6cd5\u5bf9\u5206\u6790\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\u3002\u901a\u8fc7\u5728\u8fd9\u4e09\u79cd\u7b97\u6cd5\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff08trade-off\uff09\u6765\u83b7\u5f97\u6700\u7ec8\u7684\u8fd1\u4f3c\u6bd4\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd $10/17$-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5c06\u6700\u5927\u6743 3-\u8def\u5f84\u586b\u5145\u95ee\u9898\u7684\u8fd1\u4f3c\u6bd4\u4ece\u5df2\u77e5\u7684\u6700\u4f73 $7/12$ \u63d0\u9ad8\u5230\u4e86 $10/17$\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6700\u5927\u6743\u5339\u914d\u7684\u5206\u6790\u65b9\u6cd5\u2014\u2014\u201c\u5145\u7535\u65b9\u6cd5\u201d\u3002", "conclusion": "\u672c\u6587\u89e3\u51b3\u4e86\u6700\u5927\u6743 3-\u8def\u5f84\u586b\u5145\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u662f\u6700\u5927\u6743\u5339\u914d\u95ee\u9898\u7684\u63a8\u5e7f\u3002\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4e86\u4e09\u79cd\u65b9\u6cd5\u7684 $10/17$-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u6539\u8fdb\u4e86\u6b64\u524d\u6700\u4f73\u7684 $7/12$-\u8fd1\u4f3c\u6bd4\u7387\u3002\u65b0\u63d0\u51fa\u7684\u57fa\u4e8e\u6700\u5927\u6743\u5339\u914d\u7684\u5206\u6790\u65b9\u6cd5\u2014\u2014\u201c\u5145\u7535\u65b9\u6cd5\u201d\u2014\u2014\u4e0d\u4ec5\u5bf9\u5206\u6790\u65b0\u7b97\u6cd5\u81f3\u5173\u91cd\u8981\uff0c\u4e5f\u53ef\u80fd\u63a8\u5e7f\u5230\u5206\u6790\u76f8\u5173\u95ee\u9898\u3002"}}
{"id": "2512.14322", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.14322", "abs": "https://arxiv.org/abs/2512.14322", "authors": ["Huizheng Wang", "Hongbin Wang", "Zichuan Wang", "Zhiheng Yue", "Yang Wang", "Chao Li", "Yang Hu", "Shouyi Yin"], "title": "PADE: A Predictor-Free Sparse Attention Accelerator via Unified Execution and Stage Fusion", "comment": "Accepted by HPCA 2026", "summary": "Attention-based models have revolutionized AI, but the quadratic cost of self-attention incurs severe computational and memory overhead. Sparse attention methods alleviate this by skipping low-relevance token pairs. However, current approaches lack practicality due to the heavy expense of added sparsity predictor, which severely drops their hardware efficiency.\n  This paper advances the state-of-the-art (SOTA) by proposing a bit-serial enable stage-fusion (BSF) mechanism, which eliminates the need for a separate predictor. However, it faces key challenges: 1) Inaccurate bit-sliced sparsity speculation leads to incorrect pruning; 2) Hardware under-utilization due to fine-grained and imbalanced bit-level workloads. 3) Tiling difficulty caused by the row-wise dependency in sparsity pruning criteria.\n  We propose PADE, a predictor-free algorithm-hardware co-design for dynamic sparse attention acceleration. PADE features three key innovations: 1) Bit-wise uncertainty interval-enabled guard filtering (BUI-GF) strategy to accurately identify trivial tokens during each bit round; 2) Bidirectional sparsity-based out-of-order execution (BS-OOE) to improve hardware utilization; 3) Interleaving-based sparsity-tiled attention (ISTA) to reduce both I/O and computational complexity. These techniques, combined with custom accelerator designs, enable practical sparsity acceleration without relying on an added sparsity predictor. Extensive experiments on 22 benchmarks show that PADE achieves 7.43x speed up and 31.1x higher energy efficiency than Nvidia H100 GPU. Compared to SOTA accelerators, PADE achieves 5.1x, 4.3x and 3.4x energy saving than Sanger, DOTA and SOFA.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e0e**MLIR**, **\u7f16\u8bd1\u5668**, **HLS**\u6216**DSL**\u65e0\u5173\uff0c\u4f46\u4e0e**\u56fe\u5904\u7406**\u76f8\u5173\u56e0\u4e3a\u5b83\u5904\u7406\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u7a00\u758f\u5316\u548c\u52a0\u901f\u3002\u5b83\u4e0e**\u7f16\u8bd1\u5668**\u548c**HLS**\u76f8\u5173\uff0c\u56e0\u4e3a\u5b83\u6d89\u53ca\u786c\u4ef6/\u7b97\u6cd5\u7684\u534f\u540c\u8bbe\u8ba1\u548c\u5b9a\u5236\u52a0\u901f\u5668\u8bbe\u8ba1\u3002\u8be5\u8bba\u6587\u63d0\u51fa\u4e86PADE\uff0c\u4e00\u4e2a\u7528\u4e8e\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u52a0\u901f\u7684\u65e0\u9884\u6d4b\u5668\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u91c7\u7528BUI-GF\u3001BS-OOE\u548cISTA\u7b49\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u4e2d\u9884\u6d4b\u5668\u5f00\u9500\u5bfc\u81f4\u7684\u5b9e\u7528\u6027\u95ee\u9898\u548cBSF\u673a\u5236\u9762\u4e34\u7684\u6311\u6218\u3002PADE\u572822\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e867.43\u500d\u4e8eNvidia H100 GPU\u7684\u52a0\u901f\u548c31.1\u500d\u66f4\u9ad8\u7684\u80fd\u6548\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684SOTA\u52a0\u901f\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u7531\u4e8e\u5f15\u5165\u4e86\u989d\u5916\u7684\u7a00\u758f\u6027\u9884\u6d4b\u5668\uff0c\u5bfc\u81f4\u786c\u4ef6\u6548\u7387\u4e25\u91cd\u4e0b\u964d\uff0c\u7f3a\u4e4f\u5b9e\u7528\u6027\u3002\u672c\u6587\u63d0\u51fa**\u4f4d\u4e32\u884c\u652f\u6301\u7684\u9636\u6bb5\u878d\u5408\uff08BSF\uff09**\u673a\u5236\uff0c\u65e8\u5728\u6d88\u9664\u5bf9\u72ec\u7acb\u9884\u6d4b\u5668\u7684\u4f9d\u8d56\uff0c\u4f46\u9762\u4e34\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u4e0d\u51c6\u786e\u7684\u4f4d\u5207\u7247\u7a00\u758f\u6027\u63a8\u6d4b\u5bfc\u81f4\u9519\u8bef\u7684\u526a\u679d\uff1b2\uff09\u7ec6\u7c92\u5ea6\u548c\u4e0d\u5e73\u8861\u7684\u4f4d\u7ea7\u5de5\u4f5c\u8d1f\u8f7d\u5bfc\u81f4\u7684\u786c\u4ef6\u5229\u7528\u7387\u4f4e\u4e0b\uff1b3\uff09\u7a00\u758f\u6027\u526a\u679d\u6807\u51c6\u4e2d\u7684\u884c\u95f4\u4f9d\u8d56\u5bfc\u81f4\u7684\u5e73\u94fa\u56f0\u96be\u3002", "method": "PADE\u63d0\u51fa\u4e86\u4e09\u9879\u5173\u952e\u521b\u65b0\uff1a1) **\u4f4d\u4e0d\u786e\u5b9a\u6027\u533a\u95f4\u652f\u6301\u7684\u4fdd\u62a4\u8fc7\u6ee4\uff08BUI-GF\uff09**\uff1a\u5728\u6bcf\u4e2a\u4f4d\u8fed\u4ee3\u4e2d\u51c6\u786e\u8bc6\u522b\u4e0d\u91cd\u8981\u7684token\uff0c\u89e3\u51b3\u4e0d\u51c6\u786e\u526a\u679d\u95ee\u9898\uff1b2) **\u57fa\u4e8e\u53cc\u5411\u7a00\u758f\u6027\u7684\u4e71\u5e8f\u6267\u884c\uff08BS-OOE\uff09**\uff1a\u63d0\u9ad8\u786c\u4ef6\u5229\u7528\u7387\uff0c\u89e3\u51b3\u786c\u4ef6\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\uff1b3) **\u57fa\u4e8e\u4ea4\u9519\u7684\u7a00\u758f\u5e73\u94fa\u6ce8\u610f\u529b\uff08ISTA\uff09**\uff1a\u51cf\u5c11I/O\u548c\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u89e3\u51b3\u5e73\u94fa\u96be\u9898\u3002\u8fd9\u4e9b\u521b\u65b0\u4e0e\u5b9a\u5236\u52a0\u901f\u5668\u8bbe\u8ba1\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u989d\u5916\u7a00\u758f\u6027\u9884\u6d4b\u5668\u7684\u7a00\u758f\u6027\u52a0\u901f\u3002", "result": "PADE\u572822\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\uff1aPADE\u5b9e\u73b0\u4e867.43\u500d\u4e8eNvidia H100 GPU\u7684\u52a0\u901f\uff0c\u5e76\u5c06\u80fd\u6548\u63d0\u9ad8\u4e8631.1\u500d\u3002\u4e0eSOTA\u52a0\u901f\u5668\u76f8\u6bd4\uff0cPADE\u5728\u8282\u80fd\u65b9\u9762\u5206\u522b\u6bd4Sanger\u3001DOTA\u548cSOFA\u63d0\u9ad8\u4e865.1\u500d\u30014.3\u500d\u548c3.4\u500d\u3002", "conclusion": "PADE\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u65e0\u9884\u6d4b\u5668\u7684\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u52a0\u901f\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u5728\u786c\u4ef6\u5b9e\u73b0\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u95ee\u9898\uff0c\u5e76\u5728\u52a0\u901f\u6bd4\u548c\u80fd\u6548\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\u3002"}}
