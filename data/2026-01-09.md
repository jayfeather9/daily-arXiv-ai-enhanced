<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [ParaCodex: A Profiling-Guided Autonomous Coding Agent for Reliable Parallel Code Generation and Translation](https://arxiv.org/abs/2601.04327)
*Erel Kaplan,Tomer Bitan,Lian Ghrayeb,Le Chen,Tom Yotam,Niranjan Hasabnis,Gal Oren*

Main category: cs.DC

TL;DR: 本论文与编译器和高性能计算（HPC）的并行编程（OpenMP GPU 卸载）相关。/太长不看：本文提出了 ParaCodex，一个基于 Codex 智能体的 HPC 工程师工作流程，通过结合分阶段热点分析、数据规划、正确性门控和性能指导优化，实现了串行 CPU 内核到高性能 OpenMP GPU 卸载代码的自动转换。实验结果显示，ParaCodex 在 HeCBench 和 Rodinia 等基准测试上，相对于参考实现分别实现了 3 倍和 5 倍的几何平均加速比，并在所有测试套件上超越了零样本 Codex 基线。


<details>
  <summary>Details</summary>
Motivation: 并行编程在高性能计算（HPC）和人工智能（AI）领域中至关重要，但生成正确且高效的代码，特别是 OpenMP GPU 卸载代码，仍然具有挑战性，其中数据移动和性能调优是主要难点。虽然自主编码智能体（如基于 Codex）可以在目标硬件上编译、测试和分析，但在缺乏领域知识指导的情况下，其输出质量不稳定。因此，有必要开发一种系统化的方法，将自主编码智能体的能力与 HPC 领域的专业知识结合起来，自动生成高性能的 OpenMP GPU 卸载代码。

Method: 本文提出了 ParaCodex，一个结合了 Codex-based 智能体、HPC 工程师工作流程和四个核心策略（分阶段的热点分析、显式数据规划、正确性门控和性能分析指导的优化）的自主 OpenMP GPU 卸载代码生成系统。该方法旨在将串行 CPU 内核自动转换为高性能的 OpenMP GPU 卸载内核。ParaCodex的工作流程包括对热点区域的识别（Stage 1/2）、明确的数据管理设计（Data Planning）、确保代码功能正确的验证机制（Correctness Gating）以及通过性能分析来指导迭代优化的流程（Profiling-Guided Refinement）。

Result: 在 HeCBench、Rodinia 和 NAS 基准测试集上的 31 个有效内核中，ParaCodex 成功地完成了所有内核的转换。与参考的 OpenMP 实现在 GPU 运行时间相比，ParaCodex 生成的内核在 25/31 的情况下实现了性能提升，在 HeCBench 上实现了 3 倍的几何平均加速比，在 Rodinia 上实现了 5 倍的几何平均加速比。它在所有测试套件上都优于零样本（zero-shot）的 Codex 基线。此外，在 CUDA 到 OpenMP 卸载的 ParEval 评估中，ParaCodex 在代码级和端到端设置中都保持了高编译和验证成功率。

Conclusion: ParaCodex 通过结合 Codex-based 智能体、定制化的 HPC 工程师工作流程、以及一系列的优化和验证策略，成功地实现了高性能 OpenMP GPU 卸载代码的自动生成和优化。它在多个基准测试中超越了人工编写的参考实现和零样本的 Codex 基线，显示了在并行编程领域，尤其是 OpenMP GPU 卸载代码生成方面的巨大潜力。未来的工作可能包括进一步扩展其应用范围和提升代码质量。

Abstract: Parallel programming is central to HPC and AI, but producing code that is correct and fast remains challenging, especially for OpenMP GPU offload, where data movement and tuning dominate. Autonomous coding agents can compile, test, and profile on target hardware, but outputs are brittle without domain scaffolding.
  We present ParaCodex, an HPC-engineer workflow that turns a Codex-based agent into an autonomous OpenMP GPU offload system using staged hotspot analysis, explicit data planning, correctness gating, and profiling-guided refinement. We evaluate translation from serial CPU kernels to OpenMP GPU offload kernels on HeCBench, Rodinia, and NAS. After excluding five kernels, ParaCodex succeeded on all 31 valid kernels. The generated kernels improved GPU time over reference OpenMP implementations in 25/31 cases, achieving geometric-mean speedups of 3x on HeCBench and 5x on Rodinia, and outperforming a zero-shot Codex baseline on all suites. We also evaluate CUDA to OpenMP offload translation on ParEval, where ParaCodex maintains high compilation and validation rates in code-only and end-to-end settings.

</details>


### [2] [Sharded Elimination and Combining for Highly-Efficient Concurrent Stacks](https://arxiv.org/abs/2601.04523)
*Ajay Singh,Nikos Metaxakis,Panagiota Fatourou*

Main category: cs.DC

TL;DR: 该论文与编译器、DSL、图处理、MLIR和HLS均不相关。本文提出了一种新型分块线性化栈（blocking linearizable stack）实现，通过利用分片和取存增值操作，并结合创新的消除机制和组合方法，显著提升了并发栈的性能；实验证明，在大多数工作负载中，性能比现有并发栈提高了多达2倍，特别适用于高并发和高竞争环境。


<details>
  <summary>Details</summary>
Motivation: 现有并发栈的性能无法满足高并发环境下的需求，尤其是在线程数量多和竞争激烈的情况下。作者旨在设计并实现一种高性能、低竞争、高并行度的线性化并发栈。

Method: 该实现基于一种新颖的消除机制（elimination mechanism）和一个新的组合方法（combining approach），并高效地将二者融合。核心技术包括利用分片（sharding）和取存增值（fetch&increment）操作，以提高性能、增强并行性并降低共享栈访问时的竞争。

Result: 实验结果表明，所提出的栈实现优于所有现有并发栈，在大多数工作负载中性能提升高达2倍。它在支持大量线程的系统和高竞争场景中尤其高效。

Conclusion: 本文提出了一种新型分块线性化栈实现，通过结合分片（sharding）和取存增值（fetch&increment）机制，并融入创新的消除机制和组合方法，显著提高了并发栈的性能。实验证明，该实现相较于现有并发栈在大多数工作负载中性能提升高达2倍，特别适用于拥有大量线程的系统和高竞争场景。

Abstract: We present a new blocking linearizable stack implementation which utilizes sharding and fetch&increment to achieve significantly better performance than all existing concurrent stacks. The proposed implementation is based on a novel elimination mechanism and a new combining approach that are efficiently blended to gain high performance. Our implementation results in enhanced parallelism and low contention when accessing the shared stack. Experiments show that the proposed stack implementation outperforms all existing concurrent stacks by up to 2X in most workloads. It is particularly efficient in systems supporting a large number of threads and in high contention scenarios.

</details>


### [3] [Cognitive Infrastructure: A Unified DCIM Framework for AI Data Centers](https://arxiv.org/abs/2601.04750)
*Krishna Chaitanya Sunkara*

Main category: cs.DC

TL;DR: 该论文与编译器和 DSL 或图处理或 MLIR 或 HLS 无关。

摘要介绍了 DCIM 3.0，这是一个统一的框架，它集成了语义推理、预测分析、自主编排和统一连接，用于下一代 AI 数据中心管理。该框架通过基于知识图谱的智能、热模型和统一设备连接协议（UDCP）等技术，解决了基础设施自动化、可持续性和数字孪生设计等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 当前数据中心在基础设施自动化、可持续性和数字孪生设计方面存在挑战，特别是在下一代 AI 数据中心的管理中。因此，需要一个统一的框架来应对这些挑战，提高数据中心运营的效率、可持续性和智能化水平。

Method: 该方法的核心是 DCIM 3.0 框架，它通过整合语义推理、预测分析、自主编排和统一连接来实现 AI 数据中心管理。具体技术包括基于知识图谱的智能以支持基础设施自动化和数字孪生设计，热模型以实现可持续性，以及统一设备连接协议（UDCP）以实现统一连接。

Result: DCIM 3.0 框架作为一个统一的解决方案被提出，它整合了语义推理、预测分析、自主编排和统一连接。通过关键技术手段，如基于知识图谱的智能、热模型和 UDCP，该框架旨在解决基础设施自动化、可持续性和数字孪生设计中的核心问题。

Conclusion: DCIM 3.0 通过整合语义推理、预测分析、自主编排和统一连接，并在知识图谱、热模型和 UDCP 的支持下，为一个解决 AI 数据中心管理挑战的统一框架打下了基础，旨在实现更高效、绿色和智能的数据中心基础设施管理。

Abstract: This work presents DCIM 3.0, a unified framework integrating semantic reasoning, predictive analytics, autonomous orchestration, and unified connectivity for next-generation AI data center management. The framework addresses critical challenges in infrastructure automation, sustainability, and digital-twin design through knowledge graph-based intelligence, thermal modeling, and the Unified Device Connectivity Protocol (UDCP).Keywords-Data Center Infrastructure Management, DCIM, AI Data Centers, Knowledge Graphs, Digital Twin, Thermal Management, Infrastructure Automation, Sustainability, GPU Computing, Data Center

</details>


### [4] [Proof of Commitment: A Human-Centric Resource for Permissionless Consensus](https://arxiv.org/abs/2601.04813)
*Homayoun Maleki,Nekane Sainz,Jon Legarda*

Main category: cs.DC

TL;DR: 该论文与 DSL、图处理、MLIR、编译器或 HLS **均不相关**。
**TL;DR: ** 本文提出了 Proof of Commitment (PoCmt)，这是一种新的无需许可的共识机制，它基于一种不可并行化的稀缺资源——实时的**人力投入**，而不是计算或资本。PoCmt 通过基于身份和时限的挑战来强制执行人的参与，从而确保了 Sybil 攻击的成本是**严格线性的**（维持多个身份需要同比增加人力）。理论分析表明，在部分同步下，PoCmt 实现了安全性、活性和与承诺成比例的公平性，提供了一种新的共识设计范式。


<details>
  <summary>Details</summary>
Motivation: 现有的无需许可共识协议（如 PoW 和 PoS）依赖于可并行化的稀缺资源（如计算或资本）来选举领导者和提供 Sybil 抵抗。这些可并行资源一旦获得，可以以可忽略不计的边际成本细分给多个身份，导致**零边际 Sybil 成本**，使得从根本上实现线性 Sybil 成本是不可行的。这激发了寻找一种基于不可并行化资源的新的共识范式，以实现严格的线性 Sybil 成本。

Method: 文章介绍了一种名为 Proof of Commitment (PoCmt) 的共识原语，其核心是一种不可并行化的稀缺资源：**实时的人力投入（real-time human engagement）**。PoCmt 通过要求验证者维持一个捕获累积人力投入、协议参与和在线可用性的“承诺状态（commitment state）”来运作。协议通过一个“人类挑战预言机（Human Challenge Oracle）”来强制执行参与度，该预言机发布针对身份、有时限的挑战，从而将每个人力时间窗口内可解决的挑战数量限制住。通过理论分析和加权骨干分析，证明了 PoCmt 在部分同步（partial synchrony）下可以实现安全性、活性和与承诺成比例的公平性。模拟实验进一步验证了其预期特性。

Result: PoCmt 实现了基于不可并行化资源（人力投入）的共识，其核心结果是：
1. **成本理论分离：** PoCmt 与基于可并行化资源的协议明确分离，后者允许**零边际 Sybil 成本**，而 PoCmt 强制执行**严格的线性成本曲线**——维持多个活跃身份需要等比例的人力时间投入。
2. **安全性和活性：** 通过加权骨干分析，证明 PoCmt 在部分同步下能够实现**安全性（safety）**、**活性（liveness）**和**与承诺成比例的公平性（commitment-proportional fairness）**。
3. **模拟验证：** 模拟实验将“人力时间容量”确定为唯一的对手瓶颈，并验证了预期的承诺漂移和公平性特性。

Conclusion: Proof of Commitment (PoCmt) 是共识设计空间中的一个新方向，它将去中心化共识的安全性建立在持续的人力投入上，而不是计算能力或资本。这为在 Sybil 成本线性增长至不可分割资源的范式下设计无需许可的共识协议提供了新的可能性。

Abstract: Permissionless consensus protocols require a scarce resource to regulate leader election and provide Sybil resistance. Existing paradigms such as Proof of Work and Proof of Stake instantiate this scarcity through parallelizable resources like computation or capital. Once acquired, these resources can be subdivided across many identities at negligible marginal cost, making linear Sybil cost fundamentally unattainable.
  We introduce Proof of Commitment (PoCmt), a consensus primitive grounded in a non-parallelizable resource: real-time human engagement. Validators maintain a commitment state capturing cumulative human effort, protocol participation, and online availability. Engagement is enforced through a Human Challenge Oracle that issues identity-bound, time-sensitive challenges, limiting the number of challenges solvable within each human window.
  Under this model, sustaining multiple active identities requires proportional human-time effort. We establish a cost-theoretic separation showing that protocols based on parallelizable resources admit zero marginal Sybil cost, whereas PoCmt enforces a strictly linear cost profile. Using a weighted-backbone analysis, we show that PoCmt achieves safety, liveness, and commitment-proportional fairness under partial synchrony.
  Simulations complement the analysis by isolating human-time capacity as the sole adversarial bottleneck and validating the predicted commitment drift and fairness properties. These results position PoCmt as a new point in the consensus design space, grounding permissionless security in sustained human effort rather than computation or capital.

</details>


### [5] [Asynchronous Secure Federated Learning with Byzantine aggregators](https://arxiv.org/abs/2601.04930)
*Antonella Del Pozzo,Achille Desreumaux,Mathieu Gestin,Alexandre Rapetti,Sara Tucci-Piergiovanni*

Main category: cs.DC

TL;DR: 该论文与**联邦学习 (Federated Learning)** 相关。
该论文提出了一种在异步通信设置下，可容忍恶意聚合器的隐私保护联邦平均新协议。协议通过聚合器复制、高斯噪声与掩码机制和引入确保均匀参与的包含机制来实现数据隐私保护、鲁棒性和活跃性，且性能与现有技术相当。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦平均（Federated Averaging）隐私保护方案大多集中在同步通信或无法应对恶意聚合器。本文的动机是解决在**异步通信**设置下，如何保护客户端数据隐私的同时，应对**恶意聚合器**可能引起的模型操纵、数据泄露或计算停止的问题。同时，异步环境下的快速客户端可能导致的隐私泄露和训练偏差问题也是其关注点。

Method: 本文通过聚合器复制（容忍部分聚合器被破坏）、客户端模型值掩码和添加高斯噪声（结合复制的聚合器进行去掩码）以及引入包含机制（确保客户端的均匀参与和平衡隐私预算）来共同实现解决方案。该方案不依赖聚合器间的共识。

Result: 提出了一种新的异步通信模型下、可容忍拜占庭聚合器的隐私保护协议。该协议通过聚合器复制和创新性的掩码/去掩码机制，实现了在恶意聚合器存在时的联邦平均和训练活性。同时，通过引入包含机制解决了异步环境中快速客户端导致的隐私和训练偏差问题。该方案在所有指标上保持了与现有技术相同的性能，且无需聚合器间的共识机制，提高了可用性。

Conclusion: 本文分析并提出了一种异步通信联邦学习环境下的隐私保护、可容忍恶意聚合器的新型安全聚合协议。该协议通过聚合器复制、高斯噪声掩蔽以及引入包含机制来保障模型的隐私和训练的活性，且无需聚合器间的共识机制，从而提高了可用性。该方法在性能上与现有技术相当，但在鲁棒性和适用性方面具有优势。

Abstract: Privacy-preserving federated averaging is a central approach for protecting client privacy in federated learning. In this paper, we study this problem in an asynchronous communications setting with malicious aggregators. We propose a new solution to provide federated averaging in this model while protecting the client's data privacy through secure aggregation and differential privacy. Our solution maintains the same performance as the state of the art across all metrics. The main contributions of this paper are threefold. First, unlike existing single- or multi-server solutions, we consider malicious aggregation servers that may manipulate the model to leak clients' data or halt computation. To tolerate this threat, we replicate the aggregators, allowing a fraction of them to be corrupted. Second, we propose a new privacy preservation protocol for protocols in asynchronous communication models with Byzantine aggregators. In this protocol, clients mask their values and add Gaussian noise to their models. In contrast with previous works, we use the replicated servers to unmask the models, while ensuring the liveness of training even if aggregators misbehave. Third, the asynchronous communication model introduces new challenges not present in existing approaches. In such a setting, faster clients may contribute more frequently, potentially reducing their privacy and biasing the training. To address this, we introduce an inclusion mechanism that ensures uniform client participation and balanced privacy budgets. Interestingly, the solution presented in this paper does not rely on agreement between aggregators. Thus, we circumvent the known impossibility of consensus in asynchronous settings where processes might crash. Additionally, this feature increases availability, as a consensus-based algorithm only progresses in periods of low latency.

</details>


### [6] [Nalar: An agent serving framework](https://arxiv.org/abs/2601.05109)
*Marco Laju,Donghyun Son,Saurabh Agarwal,Nitin Kedia,Myungjin Lee,Jayanth Srinivasa,Aditya Akella*

Main category: cs.DC

TL;DR: 该论文与编译器和图处理没有直接关系，但与**DSL（通过简洁的工作流规范）**和**MLIR（作为一种可能的低级中间表示或编译器相关概念，尽管不是直接的MLIR项目，但其底层机制可能与编译器原理有关，特别是“控制流”和“调度”）**和**HLS（作为一种可能的低级中间表示或编译器相关概念，尽管不是直接的MLIR项目，但其底层机制可能与编译器原理有关，特别是“控制流”和“调度”）**以及**编译器**和**HLS**无关，它与**编译器**和**HLS**无关，它解决的是**LLM驱动的智能体应用服务和执行效率问题**。
Nalar是一个用于智能体应用的自底向上服务框架，它将工作流规范与执行分离，通过轻量级存根、托管状态层和两级控制架构来解决组件异构性、动态控制流和不可预测延迟带来的挑战。实验结果显示，Nalar显著降低了尾部延迟（34-74%），实现了高达2.9倍的加速，并能在大规模和高吞吐量下保持高效服务。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM驱动的智能体应用越来越自动化复杂的、多步骤的任务，但由于以下几个挑战，对其进行高效服务仍然存在困难：组件的异构性、动态且由模型驱动的控制流、长运行状态以及不可预测的延迟。因此，需要一个能够解决这些挑战，并提供健壮性能的智能体服务框架。

Method: Nalar设计了一个自底向上的智能体服务框架，将工作流规范与执行清晰分离。它使用轻量级的自动生成存根（stubs）将智能体和工具调用转换为携带依赖和上下文元数据的future。同时，框架通过一个托管状态层（managed state layer）来解耦逻辑状态和物理位置，确保安全重用、迁移和一致的重试行为。此外，Nalar采用两级控制架构（two-level control architecture），将全局策略计算与局部事件驱动的执行相结合，以支持跨不断演进的工作流进行自适应路由、调度和资源管理。

Result: Nalar在三个智能体工作负载上展示了显著的性能提升。它将尾部延迟降低了34%至74%，实现了高达2.9倍的加速，能够在基线系统失败的情况下维持80 RPS（每秒请求数），并能扩展到处理13万个futures，且控制开销低于500毫秒。

Conclusion: Nalar作为一个自底向上的智能体服务框架，通过清晰地分离工作流规范与执行，并在运行时提供必要的可视性和控制，有效地提升了异构、动态和模型驱动的智能体应用的性能。其核心机制，包括轻量级的自动生成存根、托管状态层以及两级控制架构，使其能够在不增加开发者编排负担的情况下，实现可扩展、高效和策略驱动的服务。实验结果表明，Nalar在降低尾部延迟、提高速度和支持高吞吐量方面均表现出色，证明了其在解决复杂智能体应用服务挑战上的有效性。

Abstract: LLM-driven agentic applications increasingly automate complex, multi-step tasks, but serving them efficiently remains challenging due to heterogeneous components, dynamic and model-driven control flow, long-running state, and unpredictable latencies. Nalar is a ground-up agent-serving framework that cleanly separates workflow specification from execution while providing the runtime visibility and control needed for robust performance. Nalar preserves full Python expressiveness, using lightweight auto-generated stubs that turn agent and tool invocations into futures carrying dependency and context metadata. A managed state layer decouples logical state from physical placement, enabling safe reuse, migration, and consistent retry behavior. A two-level control architecture combines global policy computation with local event-driven enforcement to support adaptive routing, scheduling, and resource management across evolving workflows. Together, these mechanisms allow Nalar to deliver scalable, efficient, and policy-driven serving of heterogeneous agentic applications without burdening developers with orchestration logic. Across three agentic workloads, Nalar cuts tail latency by 34--74\%, achieves up to $2.9\times$ speedups, sustains 80 RPS where baselines fail, and scales to 130K futures with sub-500 ms control overhead.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [7] [Lenses for Partially-Specified States (Extended Version)](https://arxiv.org/abs/2601.04573)
*Kazutaka Matsuda,Minh Nguyen,Meng Wang*

Main category: cs.PL

TL;DR: 与DSL、图处理、MLIR、编译器或HLS不直接相关。

双向转换在多视图共享源时存在挑战，即难以在保持对应关系的同时保留用户更新，尤其是在组合框架下。本文提出了“偏状态透镜”（partial-state lenses），允许源和视图状态部分指定，精确表示用户的更新意图。通过对这些意图的偏序处理，该方法为合并来自多个视图的更新提供了清晰的语义，并提出了一个支持组合推理和确保更新保持的、感知偏指定性的良好行为的正式框架。


<details>
  <summary>Details</summary>
Motivation: 双向转换（bidirectional transformation）是一对满足特定良好行为的转换，但当多个视图共享一个源时，一个视图上的更新可能会影响其他视图，这使得在保持对应关系和保留用户更新的同时变得困难，尤其是在多个视图同时发生变化时。在一个组合框架内保证这些特性更具挑战性。本文的动机是解决多视图共享源时，准确表示和合并用户的更新意图，并确保更新保持和可组合性。

Method: 本文提出了“偏状态透镜”（partial-state lenses）方法。该方法允许源状态和视图状态被部分指定，从而精确地表达用户的更新意图。这些部分指定的意图是偏序的，这为合并来自多个视图的更新意图以及对更新保持性进行定义提供清晰的语义。该研究还对偏状态透镜进行了形式化，并提出了一种感知偏指定性的良好行为特性，以支持组合推理和确保更新保持性。

Result: 本文提出的偏状态透镜能够精确表示用户的更新意图，并通过偏序的意图提供了合并来自多个视图更新的清晰语义，支持了更新保持性的细化概念。此外，形式化的偏状态透镜和感知偏指定性的良好行为特性证明了该方法支持组合推理并确保更新保持。作者通过示例展示了所提出系统的实用性。

Conclusion: 我们提出了偏状态透镜，它允许源状态和视图状态被部分指定，以精确表示用户的更新意图。这种方法通过引入偏序的更新意图，为来自多个视图的更新意图的合并提供了清晰的语义，并提供了一种兼容更新合并的更新保**持的细化概念。我们还形式化了偏状态透镜，以及支持组合推理和确保更新保持的、感知偏指定性的良好行为特性。

Abstract: A bidirectional transformation is a pair of transformations satisfying certain well-behavedness properties: one maps source data into view data, and the other translates changes on the view back to the source. However, when multiple views share a source, an update on one view may affect the others, making it hard to maintain correspondence while preserving the user's update, especially when multiple views are changed at once. Ensuring these properties within a compositional framework is even more challenging. In this paper, we propose partial-state lenses, which allow source and view states to be partially specified to precisely represent the user's update intentions. These intentions are partially ordered, providing clear semantics for merging intentions of updates coming from multiple views and a refined notion of update preservation compatible with this merging. We formalize partial-state lenses, together with partial-specifiedness-aware well-behavedness that supports compositional reasoning and ensures update preservation. In addition, we demonstrate the utility of the proposed system through examples.

</details>


### [8] [The Squirrel Parser: A Linear-Time PEG Packrat Parser Capable of Left Recursion and Optimal Error Recovery](https://arxiv.org/abs/2601.05012)
*Luke A. D. Hutchison*

Main category: cs.PL

TL;DR: 该论文与编译器（解析器是编译器的前端组件）相关。它介绍了一种名为 Squirrel 的 PEG Packrat 解析器，该解析器通过基于位置的状态跟踪和迭代扩展的定点搜索等创新算法，直接处理所有形式的左递归，并设计了一套基于公理和约束的最优错误恢复机制。这使得解析器即使在存在大量错误的情况下，也能保持线性的时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统的递归下降解析器（如 Packrat 解析器）在处理所有形式的左递归时面临挑战，通常需要复杂的语法重写或定制的算法扩展。此外，已有的解析器在存在大量错误时，其错误恢复能力往往不能保证性能的线性，或缺乏系统性的最优设计。因此，需要开发一种新的 PEG Packrat 解析器，能够直接处理所有形式的左递归，并提供具有可证明性能和行为鲁棒性的最优错误恢复机制。

Method: 该方法的核心包括：1. **左递归处理**：通过每个位置的状态跟踪进行循环检测（$O(1)$复杂度），以及从后代到祖先递归帧的通信来识别和处理左递归周期。接着，通过迭代扩展（Fixed-point search via iterative expansion）来保证解析的完整性。2. **错误恢复**：基于一组（四个）公理和（十二个）约束条件，推导出并实现了一种可证明最优且鲁棒的错误恢复策略，该策略保证了解析器的线性性能不受错误数量的影响。

Result: 所呈现的 Squirrel 解析器成功实现了以下目标：1. **直接处理所有形式的左递归**：无需语法重写或复杂扩展。2. **性能优化**：即使在存在任意数量的错误的情况下，也能保持线性的时间复杂度，即 $O(\text{length of the input})$。3. **最优错误恢复**：通过严格设计的公理和约束，提供了一种可证明最优且直观的错误恢复方案。

Conclusion: Squirrel 解析器是一种高效且鲁棒的 PEG Packrat 解析器，它通过创新的算法设计（包括基于状态跟踪的循环检测和迭代扩展的定点搜索）以及一套经过严格推导的最优错误恢复策略，解决了递归下降解析器中处理左递归和错误恢复的挑战，实现了线性时间复杂度。

Abstract: We present the squirrel parser, a PEG packrat parser that directly handles all forms of left recursion with optimal error recovery, while maintaining linear time complexity in the length of the input even in the presence of an arbitrary number of errors. Traditional approaches to handling left recursion in a recursive descent parser require grammar rewriting or complex algorithmic extensions. We derive a minimal algorithm from first principles: cycle detection via per-position state tracking and $O(1)$-per-LR-cycle communication from descendant to ancestor recursion frames, and fixed-point search via iterative expansion. For error recovery, we derived a set of four axioms and twelve constraints that must be imposed upon an optimal error recovery design to ensure completeness, correctness, optimality of performance, and intuitiveness of behavior. We utilized a constraint satisfaction mechanism to search the space of all possibilities, arriving at a provably optimal and robust error recovery strategy that maintains perfect performance linearity.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [9] [Learning Multinomial Logits in $O(n \log n)$ time](https://arxiv.org/abs/2601.04423)
*Flavio Chierichetti,Mirko Giacchini,Ravi Kumar,Silvio Lattanzi,Alessandro Panconesi,Erasmo Tani,Andrew Tomkins*

Main category: cs.DS

TL;DR: 本文与图处理、MLIR、编译器、HLS 均不相关，它与 **DSL**（领域特定语言）不直接相关，但与**机器学习/推荐系统**的底层模型（MNL/Plackett-Luce）学习和计算复杂性分析相关。
总结：多项式 Logit (MNL) 学习的核心问题是，给定候选集查询访问权限，如何高效学习权重以确保选择分布与真实分布的总变异距离在 $\varepsilon$ 内。本文提出了两种算法：一种自适应算法，查询复杂度为 $O\left(\frac{n}{\varepsilon^{3}}\log n\right)$，证明其在对 $n$ 的依赖上是最优的；另一种非自适应算法，查询复杂度为 $O\left(\frac{n^{2}}{\varepsilon^{3}}\log n \log\frac{n}{\varepsilon}\right)$。作者为这两种情况提供了紧密的上界和下界，为高效 MNL 学习提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于解决一个在多项式 Logit (MNL) 模型（也称为 Plackett-Luce 模型或条件采样预言机）学习中的基本计算效率问题。尽管 MNL 模型已被广泛研究，但一个核心问题仍未解决：在给定对候选集（slate）的查询访问权的情况下，如何高效地学习权重，使得对于每一个候选集，由此产生的选择分布与真实选择分布之间的总变异距离（Total Variation Distance）在 $\varepsilon$ 以内。该问题是 MNL 学习的核心，并对现代推荐系统的界面设计和实现具有直接影响。

Method: 本文设计了两种算法来解决 MNL 模型权重学习问题，目标是使得学习到的模型在所有候选集上的选择分布与真实分布之间的总变异距离（Total Variation Distance）不大于 $\varepsilon$。第一种是自适应查询（adaptive queries）算法，其查询复杂度为 $O\left(\frac{n}{\varepsilon^{3}}\log n\right)$。第二种是非自适应查询（non-adaptive queries）算法，其查询复杂度为 $O\left(\frac{n^{2}}{\varepsilon^{3}}\log n \log\frac{n}{\varepsilon}\right)$。两种算法都只查询大小为二的候选集，并且运行时间与其查询复杂度成比例。作者还通过建立下界 $\Omega\left(\frac{n}{\varepsilon^{2}}\log n\right)$（自适应）和 $\Omega\left(\frac{n^{2}}{\varepsilon^{2}}\log n\right)$（非自适应）来证明算法的效率。

Result: 作者提出了两个算法并分析了其性能和界限。
1. **自适应查询算法**：查询复杂度为 $O\left(\frac{n}{\varepsilon^{3}}\log n\right)$。作者证明了该算法在对支持集大小 $n$ 的依赖性上达到了最优，因为它接近于下界 $\Omega\left(\frac{n}{\varepsilon^{2}}\log n\right)$。
2. **非自适应查询算法**：查询复杂度为 $O\left(\frac{n^{2}}{\varepsilon^{3}}\log n \log\frac{n}{\varepsilon}\right)$。作者证明了该算法在紧密性上只比下界 $\Omega\left(\frac{n^{2}}{\varepsilon^{2}}\log n\right)$ 多了一个 $\log n$ 因子。
**共同特性**：两种算法都只使用大小为二的候选集进行查询，并且运行时间与各自的查询复杂度成正比。

Conclusion: 本文解决了在总变异距离 $\varepsilon$ 约束下，基于查询访问来高效学习多项式 Logit (MNL) 模型权重的基本计算问题。作者提出了两种算法：自适应查询算法和非自适应查询算法，并针对性地证明了它们各自的查询复杂度，同时给出了匹配的下界。自适应算法达到了对 $n$ 依赖的最优性。这一工作为推荐系统中的高效 MNL 学习提供了理论基础和实用算法。

Abstract: A Multinomial Logit (MNL) model is composed of a finite universe of items $[n]=\{1,..., n\}$, each assigned a positive weight. A query specifies an admissible subset -- called a slate -- and the model chooses one item from that slate with probability proportional to its weight. This query model is also known as the Plackett-Luce model or conditional sampling oracle in the literature. Although MNLs have been studied extensively, a basic computational question remains open: given query access to slates, how efficiently can we learn weights so that, for every slate, the induced choice distribution is within total variation distance $\varepsilon$ of the ground truth? This question is central to MNL learning and has direct implications for modern recommender system interfaces.
  We provide two algorithms for this task, one with adaptive queries and one with non-adaptive queries. Each algorithm outputs an MNL $M'$ that induces, for each slate $S$, a distribution $M'_S$ on $S$ that is within $\varepsilon$ total variation distance of the true distribution. Our adaptive algorithm makes $O\left(\frac{n}{\varepsilon^{3}}\log n\right)$ queries, while our non-adaptive algorithm makes $O\left(\frac{n^{2}}{\varepsilon^{3}}\log n \log\frac{n}{\varepsilon}\right)$ queries. Both algorithms query only slates of size two and run in time proportional to their query complexity.
  We complement these upper bounds with lower bounds of $Ω\left(\frac{n}{\varepsilon^{2}}\log n\right)$ for adaptive queries and $Ω\left(\frac{n^{2}}{\varepsilon^{2}}\log n\right)$ for non-adaptive queries, thus proving that our adaptive algorithm is optimal in its dependence on the support size $n$, while the non-adaptive one is tight within a $\log n$ factor.

</details>


### [10] [Using Ray-shooting Queries for Sublinear Algorithms for Dominating Sets in RDV Graphs](https://arxiv.org/abs/2601.04626)
*Therese Biedl,Prashant Gokhale*

Main category: cs.DS

TL;DR: This paper is related to graph processing. The paper studies the minimum dominating set problem in RDV graphs, a class between interval and chordal graphs (defined as the vertex-intersection graphs of downward paths in a rooted tree). By leveraging the geometric representation of RDV graphs and employing a ray shooting data structure along with priority search trees, the paper shows that a minimum dominating set can be found in $O(n\log n)$ time, presuming a linear-sized representation is given. The same approach also provides a new proof for finding a minimum dominating set in an interval graph in $O(n)$ time.


<details>
  <summary>Details</summary>
Motivation: RDV 图是一种介于区间图和弦图之间的图类，其最小支配集问题尚未得到有效解决。通过利用 RDV 图的特殊结构和快速的几何查询数据结构，如优先级搜索树和射线射击数据结构，可以实现比遍历所有边更快的算法，从而提高求解效率。之前的工作已经证明了 RDV 图的邻接查询可以简化为线段相交问题，以及最大匹配可以在 $O(n\log n)$ 时间内解决。本文旨在解决 RDV 图上的最小支配集问题。

Method: 研究了 RDV 图上的支配集问题。方法是利用 RDV 图的表示特性（向下路径的顶点交集图）以及相关的几何数据结构。特别是，使用了射线射击数据结构（ray shooting data structure）来确定支配集。文章还提到，现有的 $O(n)$ 时间求解区间图最小支配集的方法也可以用相同思路得到新的证明。

Result: 证明了在给定线性大小的 RDV 图表示的情况下，可以利用射线射击数据结构，在 $O(n\log n)$ 时间内找到 RDV 图的最小支配集。同时，这个思路也为在 $O(n)$ 时间内找到区间图的最小支配集提供了一种新的证明。

Conclusion: 本文研究了 RDV 图上的最小支配集问题，并展示了如何利用射线射击数据结构和 RDV 图的特性，在 $O(n\log n)$ 时间内找到最小支配集。这种方法也为在 $O(n)$ 时间内找到区间图的最小支配集提供了一种新的证明。

Abstract: In this paper, we study the dominating set problem in \emph{RDV graphs}, a graph class that lies between interval graphs and chordal graphs and is defined as the \textbf{v}ertex-intersection graphs of \textbf{d}ownward paths in a \textbf{r}ooted tree. It was shown in a previous paper that adjacency queries in an RDV graph can be reduced to the question whether a horizontal segment intersects a vertical segment. This was then used to find a maximum matching in an $n$-vertex RDV graph, using priority search trees, in $O(n\log n)$ time, i.e., without even looking at all edges. In this paper, we show that if additionally we also use a ray shooting data structure, we can also find a minimum dominating set in an RDV graph $O(n\log n)$ time (presuming a linear-sized representation of the graph is given). The same idea can also be used for a new proof to find a minimum dominating set in an interval graph in $O(n)$ time.

</details>


### [11] [Branch-width of connectivity functions is fixed-parameter tractable](https://arxiv.org/abs/2601.04756)
*Tuukka Korhonen,Sang-il Oum*

Main category: cs.DS

TL;DR: 该论文与图处理和编译器 (计算复杂度分析)相关。这是一个关于组合优化和图算法的理论计算机科学论文。它证明了对于由预言机给定的连通性函数，寻找宽度至多为 $k$ 的分支分解是固定参数可解（FPT）的，通过提供一个运行时间为 $2^{O(k^2)} \gamma n^6 \log n$ 的算法。该结果解决了 Hliněný 提出的一个关于拟阵分支宽度的开放问题，并改进了图、拟阵、超图和刻度宽度的现有 FPT 算法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的算法（Oum 和 Seymour，2007）在寻找连通性函数的分支分解时，运行时间为 $γn^{O(k)}$，在参数 $k$ 上与 $n$ 存在紧密耦合，不是 FPT 算法。此外，对于拟阵的分支宽度，Hliněný（2005）提出了一个关于其是否固定参数可解的开放问题。本文旨在提供一个 FPT 算法来解决这些问题并改进现有算法的性能。

Method: 作者提供了一个复杂度为 $2^{O(k^2)} γn^6 \log n$ 的算法，其中 $γ$ 是计算任意集合 $X$ 的 $f(X)$ 的时间，$n$ 是集合 $V$ 的大小。这个算法被证明是固定参数可解的（FPT）。

Result: 作者提出了一个运行时间为 $2^{O(k^2)} γn^6 \log n$ 的 FPT 算法，用于找到宽度至多为 $k$ 的连通性函数的分支分解。该算法改进了 Oum 和 Seymour 之前的 $γn^{O(k)}$ 算法。该算法可以应用于图的秩宽度、拟阵的分支宽度、（超）图的分支宽度以及图的刻度宽度，并解决了 Hliněný 关于拟阵分支宽度的开放问题。此外，它改进了现有 FPT 算法中对于图分支宽度、秩宽度和刻度宽度的运行时间中对参数 $k$ 的依赖。

Conclusion: 本文证明了对于由预言机给定的连通性函数，寻找宽度至多为 $k$ 的分支分解是固定参数可解的，解决了 Hliněný 提出的关于拟阵分支宽度的开放问题，并改进了图分支宽度、秩宽度和刻度宽度现有 FPT 算法中对参数 $k$ 的依赖。

Abstract: A connectivity function on a finite set $V$ is a symmetric submodular function $f \colon 2^V \to \mathbb{Z}$ with $f(\emptyset)=0$. We prove that finding a branch-decomposition of width at most $k$ for a connectivity function given by an oracle is fixed-parameter tractable (FPT), by providing an algorithm of running time $2^{O(k^2)} γn^6 \log n$, where $γ$ is the time to compute $f(X)$ for any set $X$, and $n = |V|$. This improves the previous algorithm by Oum and Seymour [J. Combin. Theory Ser.~B, 2007], which runs in time $γn^{O(k)}$. Our algorithm can be applied to rank-width of graphs, branch-width of matroids, branch-width of (hyper)graphs, and carving-width of graphs. This resolves an open problem asked by Hliněný [SIAM J. Comput., 2005], who asked whether branch-width of matroids given by the rank oracle is fixed-parameter tractable. Furthermore, our algorithm improves the best known dependency on $k$ in the running times of FPT algorithms for graph branch-width, rank-width, and carving-width.

</details>


### [12] [An Invitation to "Fine-grained Complexity of NP-Complete Problems"](https://arxiv.org/abs/2601.05044)
*Jesper Nederlof*

Main category: cs.DS

TL;DR: 不相关。
这篇论文是对“NP-完全问题的细粒度复杂性”这一研究领域的一篇综述，探讨了在 P $\neq$ NP 假设下，NP-完全问题的最快运行时间，以及朴素的暴力算法是否已是最优的运行时间。文章回顾了该领域中跨代数、复杂性理论、组合学、密码学及算法设计等多个学科的经典及最新研究成果。


<details>
  <summary>Details</summary>
Motivation: 探讨在 P $\neq$ NP 的假设下，NP-完全问题的最快运行时间是多少，以及对于许多问题，朴素的暴力算法是否已经是运行时间最优的。

Method: 通过回顾经典和最近的研究成果，并涉及跨学科的知识，来探讨 NP-完全问题的细粒度复杂性。

Result: 该文章是关于“NP-完全问题的细粒度复杂性”研究领域的一篇综述，作者在文章中带领读者回顾了该领域中“精选的经典成果以及令人兴奋的最新发展”，涉及了代数、复杂性理论、极值和加性组合学、密码学以及算法设计等多个领域。

Conclusion: 这篇综述旨在介绍 NP-完全问题的细粒度复杂性这一研究领域，并邀请读者回顾经典结果和了解最新的发展，涉及代数、复杂性理论、组合学、密码学和算法设计等多个交叉领域。

Abstract: Assuming that P is not equal to NP, the worst-case run time of any algorithm solving an NP-complete problem must be super-polynomial. But what is the fastest run time we can get? Before one can even hope to approach this question, a more provocative question presents itself: Since for many problems the naive brute-force baseline algorithms are still the fastest ones, maybe their run times are already optimal?
  The area that we call in this survey "fine-grained complexity of NP-complete problems" studies exactly this question. We invite the reader to catch up on selected classic results as well as delve into exciting recent developments in a riveting tour through the area passing by (among others) algebra, complexity theory, extremal and additive combinatorics, cryptography, and, of course, last but not least, algorithm design.

</details>


### [13] [Learning Mixture Models via Efficient High-dimensional Sparse Fourier Transforms](https://arxiv.org/abs/2601.05157)
*Alkis Kalavasis,Pravesh K. Kothari,Shuchen Li,Manolis Zampetakis*

Main category: cs.DS

TL;DR: 该论文与**MLIR、编译器、HLS、DSL、图处理**均不相关。

**TLDR:** 本文提出了一种高效（$\rm poly(d,k)$时间/样本）的新算法，用于学习$k$个$d$维球形重尾分布的混合模型参数。与所有依赖于低阶矩的传统方法不同，该算法基于**高维稀疏傅里叶变换**，成功处理了包括洛伦兹分布在内的、甚至没有有限协方差的重尾分布。该方法绕开了传统矩方法在样本复杂度上的超多项式限制，并且无需集群均值之间存在最小分离，这在统计估计领域为重尾数据分析开辟了新的途径。


<details>
  <summary>Details</summary>
Motivation: 现有的混合模型学习方法（包括即使是对于拉普拉斯分布）都隐式或显式地依赖于**低阶矩（low-degree moments）**，这导致它们需要**超多项式数量的样本**才能运行，并且难以处理没有有限协方差的**重尾分布（heavy-tailed distributions）**，例如洛伦兹分布（Cauchy distributions）。即使是对于经典的球形高斯混合模型，也要求集群均值之间存在最小的$\ell_2$间隔。因此，本文的动机是**开发一种新的、高效的算法，能够绕过矩量法的限制，在多项式时间与样本复杂度内，成功学习重尾混合模型，并且不需要集群均值之间存在最小分离**。

Method: 本文提出了一种基于**高效高维稀疏傅里叶变换（efficient high-dimensional sparse Fourier transforms）**的新方法来学习混合模型。该方法利用了聚类分布的特征函数具有足够重尾的特性（即特征函数尾部足够重）。算法在处理$k$个$d$维球形分布混合模型的参数学习上，实现了$\rm poly(d, k)$的时间和样本复杂性。该方法克服了传统矩量法对低阶矩的依赖，从而绕过了其在样本复杂度上的超多项式下界。此外，该方法通过与现有技术相结合，可以处理更广泛的混合模型，即组件要么具有重尾特征函数，要么具有轻尾特征函数和亚高斯尾部。同时，该方法还被应用于解决对抗噪声盲（noise-oblivious adversaries）的稳健均值估计问题。

Result: 本文提出了一种在多项式时间$\rm poly(d,k)$和样本复杂度内，学习$k$个$d$维球形分布混合模型参数的算法。
1. **成功处理重尾分布：**该方法适用于具有足够重尾特征函数的重尾分布，包括洛伦兹分布（在极端情况下，它甚至没有有限协方差），但不包括高斯分布。
2. **绕过矩量法的局限：**本文证明了传统基于矩量法的算法即使对于拉普拉斯分布也需要超多项式数量的样本，而本文的新方法成功地绕开了这一局限。
3. **取消最小间隔要求：**与传统方法（如球形高斯混合模型需要最小的$\ell_2$-间隔）形成鲜明对比，本文的算法不需要集群均值之间有任何最小分离。
4. **扩展应用：**作为示例，该方法还被应用于解决了一个实际中具有动机的统计问题——针对噪声盲对抗的稳健均值估计问题。
5. **复合保证：**该方法可以与现有技术很好地结合，为组件具有重尾特征函数或具有亚高斯尾部（轻尾特征函数）的混合模型提供“两全其美”的保证。

Conclusion: 本文提出了一种新的、高效的混合模型学习算法，该算法基于高维稀疏傅里叶变换，能够处理重尾分布，包括洛伦兹分布等没有有限协方差的分布，并成功解决了以往基于矩方法的算法在样本复杂性和对分布的潜在限制。这种方法不仅在学习参数时具有多项式时间复杂度和样本效率，而且能够处理均值之间没有最小间隔的混合模型，为统计估计领域，尤其是重尾数据分析，开辟了新的道路，并展现出在稳健估计等其他统计应用中的潜力。

Abstract: In this work, we give a ${\rm poly}(d,k)$ time and sample algorithm for efficiently learning the parameters of a mixture of $k$ spherical distributions in $d$ dimensions. Unlike all previous methods, our techniques apply to heavy-tailed distributions and include examples that do not even have finite covariances. Our method succeeds whenever the cluster distributions have a characteristic function with sufficiently heavy tails. Such distributions include the Laplace distribution but crucially exclude Gaussians.
  All previous methods for learning mixture models relied implicitly or explicitly on the low-degree moments. Even for the case of Laplace distributions, we prove that any such algorithm must use super-polynomially many samples. Our method thus adds to the short list of techniques that bypass the limitations of the method of moments.
  Somewhat surprisingly, our algorithm does not require any minimum separation between the cluster means. This is in stark contrast to spherical Gaussian mixtures where a minimum $\ell_2$-separation is provably necessary even information-theoretically [Regev and Vijayaraghavan '17]. Our methods compose well with existing techniques and allow obtaining ''best of both worlds" guarantees for mixtures where every component either has a heavy-tailed characteristic function or has a sub-Gaussian tail with a light-tailed characteristic function.
  Our algorithm is based on a new approach to learning mixture models via efficient high-dimensional sparse Fourier transforms. We believe that this method will find more applications to statistical estimation. As an example, we give an algorithm for consistent robust mean estimation against noise-oblivious adversaries, a model practically motivated by the literature on multiple hypothesis testing. It was formally proposed in a recent Master's thesis by one of the authors, and has already inspired follow-up works.

</details>


### [14] [Inapproximability of Counting Permutation Patterns](https://arxiv.org/abs/2601.05166)
*Michal Opler*

Main category: cs.DS

TL;DR: 本文与DSL、图处理、MLIR、编译器或HLS无关。
摘要：排列模式计数问题的检测和计数是核心算法挑战，广泛应用于如排序分析和特性测试。尽管检测问题有$2^{\mathcal{O}(k^2)} \cdot n$的算法，但精确计数在$f(k)\cdot n^{o(k/\log k)}$时间内不可解，除非指数时间假设（ETH）失败。先前有研究曾猜想近似计数可能比精确计数更容易。本文通过利用ETH，证明了在运行时间$f(k)\cdot n^{o(k/\log k)}$内，任何算法都不能在乘法因子$n^{(1/2-\varepsilon)k}$的精度内近似计数，强烈反驳了这一猜想。这一结果表明，近似和精确计数在运行时间下界上具有相同的复杂性，并将乘法误差的下限推向基本紧密的界限。


<details>
  <summary>Details</summary>
Motivation: 排列模式的检测和计数是算法学中的核心问题，广泛应用于排序分析、非参数统计和特性测试。现有的研究发现，排列模式的精确计数和检测在复杂性上有显著差异。尽管有研究提出近似计数可能比精确计数更容易（尤其针对小长度模式），但这种猜想的普遍性尚未得到证实。本文的动机是检验这种猜想的普适性，并确定排列模式近似计数的真正复杂性边界。

Method: 这项工作主要通过理论分析方法，特别是利用指数时间假设（ETH）来建立条件性下界。通过证明在满足特定运行时间限制时，无法达到小于特定乘法误差因子的近似度，从而建立了近似计数问题的难度下界。具体来说，证明了在$f(k)\cdot n^{o(k/\log k)}$时间内，乘法误差因子不能小于$n^{(1/2-\varepsilon)k}$。

Result: 研究结果强烈反驳了排列模式近似计数在渐进意义上比精确计数更容易的猜想。在指数时间假设（ETH）下，证明了任何运行时间为$f(k)\cdot n^{o(k/\log k)}$的算法，都无法在乘法因子$n^{(1/2-\varepsilon)k}$的近似精度内计算长度为$k$的模式副本数量。这个运行时间下界与精确模式计数的条件性下界相匹配，并且所获得的乘法误差下限被认为是基本紧密的。$n^{k/2}$的近似可以通过模式检测算法在$2^{\mathcal{O}(k^2)}\cdot n$时间内实现。

Conclusion: 本文通过建立指数时间假设下的条件性下界，反驳了先前关于排列模式近似计数比精确计数更容易的猜想。研究表明，在运行时间方面，近似计数的难度与精确计数相当，并且除非指数时间假设失败，否则近似计数不能在低于特定乘法误差因子的精度下被有效地计算。这一结果揭示了排列模式计数问题的复杂性，并为该领域未来的研究设定了界限。

Abstract: Detecting and counting copies of permutation patterns are fundamental algorithmic problems, with applications in the analysis of rankings, nonparametric statistics, and property testing tasks such as independence and quasirandomness testing. From an algorithmic perspective, there is a sharp difference in complexity between detecting and counting the copies of a given length-$k$ pattern in a length-$n$ permutation. The former admits a $2^{\mathcal{O}(k^2)} \cdot n$ time algorithm (Guillemot and Marx, 2014) while the latter cannot be solved in time $f(k)\cdot n^{o(k/\log k)}$ unless the Exponential Time Hypothesis (ETH) fails (Berendsohn, Kozma, and Marx, 2021). In fact already for patterns of length 4, exact counting is unlikely to admit near-linear time algorithms under standard fine-grained complexity assumptions (Dudek and Gawrychowski, 2020).
  Recently, Ben-Eliezer, Mitrović and Sristava (2026) showed that for patterns of length up to 5, a $(1+\varepsilon)$-approximation of the pattern count can be computed in near-linear time, yielding a separation between exact and approximate counting for small patterns, and conjectured that approximate counting is asymptotically easier than exact counting in general. We strongly refute their conjecture by showing that, under ETH, no algorithm running in time $f(k)\cdot n^{o(k/\log k)}$ can approximate the number of copies of a length-$k$ pattern within a multiplicative factor $n^{(1/2-\varepsilon)k}$. The lower bound on runtime matches the conditional lower bound for exact pattern counting, and the obtained bound on the multiplicative error factor is essentially tight, as an $n^{k/2}$-approximation can be computed in $2^{\mathcal{O}(k^2)}\cdot n$ time using an algorithm for pattern detection.

</details>


### [15] [Concurrent Balanced Augmented Trees](https://arxiv.org/abs/2601.05225)
*Evan Wrench,Ajay Singh,Younghun Roh,Panagiota Fatourou,Siddhartha Jayanti,Eric Ruppert,Yuanhao Wei*

Main category: cs.DS

TL;DR: 部分关联：DSL（数据结构），编译器（并发数据结构设计），图处理（图数据结构基础）。本文首次提出了无锁的增强型平衡搜索树，并基于现有的非平衡增强树算法进行了改进。实验表明，该无锁平衡树在多线程并发环境下，比增强型非平衡树快2.2到30倍，比未增强树快几个数量级，特别地，采用委托（delegation）优化的版本在某些工作负载下性能提升超过2倍。


<details>
  <summary>Details</summary>
Motivation: 现有的搜索树通过增强（Augmentation）能够支持高效的聚合查询、顺序统计查询和范围查询等多样化操作，但缺乏无锁的增强型平衡搜索树。本文的动机在于填补这一空白，提出首个无锁的增强型平衡搜索树，以期在支持复杂查询的同时，在高并发环境下实现更好的可扩展性和性能。

Method: 本文方法基于Fatourou和Ruppert提出的增强型非平衡搜索树算法思想，并在此基础上进行了扩展和改进，设计出无锁的增强型平衡搜索树。在实现层面，解决了内存回收的挑战。此外，还提出了使用委托（delegation）机制的优化版本以提升可扩展性和性能。最后，通过实验对比了所提出的平衡树与非平衡树的性能。

Result: 本文成功提出了首个无锁的增强型平衡搜索树。实验结果显示，在120个线程上，所提出的增强型平衡树比增强型非平衡树快2.2到30倍，比未增强的树快几个数量级。特别地，使用委托机制的优化版本显著提高了可扩展性和性能，在某些工作负载下性能提升超过2倍。

Conclusion: 本文首次提出了无锁的增强型平衡搜索树数据结构，并证明了它在并发性能上显著优于增强型非平衡搜索树以及未增强的搜索树。特别是，通过引入委托（delegation）机制，该平衡树展现出优异的扩展性和性能，为并发数据结构领域提供了一个高效的解决方案。

Abstract: Augmentation makes search trees tremendously more versatile, allowing them to support efficient aggregation queries, order-statistic queries, and range queries in addition to insertion, deletion, and lookup. In this paper, we present the first lock-free augmented balanced search tree. Our algorithmic ideas build upon a recent augmented unbalanced search tree presented by Fatourou and Ruppert [DISC, 2024]. We implement both data structures, solving some memory reclamation challenges in the process, and provide an experimental performance analysis of them. We also present optimized versions of our balanced tree that use delegation to achieve better scalability and performance (by more than 2x in some workloads). Our experiments show that our augmented balanced tree is 2.2 to 30 times faster than the unbalanced augmented tree, and up to several orders of magnitude faster than unaugmented trees on 120 threads.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [16] [MPM-LLM4DSE: Reaching the Pareto Frontier in HLS with Multimodal Learning and LLM-Driven Exploration](https://arxiv.org/abs/2601.04801)
*Lei Xu,Shanshan Wang,Chenglong Xiao*

Main category: cs.AR

TL;DR: 该论文与 $\text{HLS}$（高层综合）、编译器、图处理（GNN）相关。
该论文提出了 $\text{MPM-LLM4DSE}$ 框架来加速 $\text{HLS}$ 设计空间探索 ($\text{DSE}$)。该框架使用融合了行为描述和控制与数据流图特征的**多模态预测模型** ($\text{MPM}$) 来更准确地预测 $\text{QoR}$，并使用 $\text{LLM}$ 作为优化器 ($\text{LLM4DSE}$)，结合定制的提示工程方法，将 $\text{pragma}$ 影响分析等领域知识融入优化过程，以生成高质量的配置。实验结果表明，$\text{MPM}$ 在预测性能上显著优于现有方法，而 $\text{LLM4DSE}$ 在 $\text{DSE}$ 任务中取得了显著的性能增益。


<details>
  <summary>Details</summary>
Motivation: 现有的 $\text{HLS}$ 设计空间探索（DSE）的加速方法主要依赖 GNN 作为 QoR 预测的替身模型，以及多目标优化算法。然而，这些方法存在两个局限性：1. 基于 $\text{GNN}$ 的预测方法可能无法充分捕获行为描述中固有的丰富语义特征。2. 传统的**多目标优化算法**通常没有明确考虑领域特定知识，例如 $\text{pragma}$ 指令如何影响 $\text{QoR}$ 的信息。本文旨在解决上述限制。

Method: 本文提出了 MPM-LLM4DSE 框架，用于加速 HLS 设计空间探索（DSE）。该方法包括两个主要部分：1. **多模态预测模型（MPM）**：融合了行为描述的特征和控制与数据流图（CDFG）的特征，用于预测 QoR 指标。2. **基于大语言模型（LLM）的优化器（LLM4DSE）**：使用 LLM 作为优化器来生成高质量的配置，并采用定制的提示工程方法，将 $\text{pragma}$ 对 $\text{QoR}$ 的影响分析等领域特定知识纳入其中，以指导 LLM 进行探索。

Result: 1. **预测模型性能**：本文提出的多模态预测模型（$\text{MPM}$）相比最先进的工作 $\text{ProgSG}$，性能提升高达 $10.25$ 倍。2. **DSE 任务性能**：在 DSE 任务中，本文提出的 $\text{LLM4DSE}$ 框架相比先前的方法，平均性能增益达到了 $39.90\%$。这验证了所提出的提示工程方法的有效性。

Conclusion: 本文提出的 MPM-LLM4DSE 框架通过结合多模态预测模型和基于 LLM 的优化器，并在 DSE 任务中取得了显著的性能提升。这证明了融合行为描述和图结构特征的预测模型，以及将领域知识（如 pragma 影响分析）融入到 LLM 优化过程中的有效性。

Abstract: High-Level Synthesis (HLS) design space exploration (DSE) seeks Pareto-optimal designs within expansive pragma configuration spaces. To accelerate HLS DSE, graph neural networks (GNNs) are commonly employed as surrogates for HLS tools to predict quality of results (QoR) metrics, while multi-objective optimization algorithms expedite the exploration. However, GNN-based prediction methods may not fully capture the rich semantic features inherent in behavioral descriptions, and conventional multi-objective optimization algorithms often do not explicitly account for the domain-specific knowledge regarding how pragma directives influence QoR. To address these limitations, this paper proposes the MPM-LLM4DSE framework, which incorporates a multimodal prediction model (MPM) that simultaneously fuses features from behavioral descriptions and control and data flow graphs. Furthermore, the framework employs a large language model (LLM) as an optimizer, accompanied by a tailored prompt engineering methodology. This methodology incorporates pragma impact analysis on QoR to guide the LLM in generating high-quality configurations (LLM4DSE). Experimental results demonstrate that our multimodal predictive model significantly outperforms state-of-the-art work ProgSG by up to 10.25$\times$. Furthermore, in DSE tasks, the proposed LLM4DSE achieves an average performance gain of 39.90\% over prior methods, validating the effectiveness of our prompting methodology. Code and models are available at https://github.com/wslcccc/MPM-LLM4DSE.

</details>


### [17] [Challenges and Research Directions for Large Language Model Inference Hardware](https://arxiv.org/abs/2601.05047)
*Xiaoyu Ma,David Patterson*

Main category: cs.AR

TL;DR: 该论文与编译器、HLS、DSL或MLIR无关。它与**图处理**没有直接关系，但考虑到LLM基础设施通常在分布式系统上运行，并可能涉及数据流图优化，因此在广义的硬件架构层面有间接联系（但核心主题是硬件架构而非图算法）。它与**LLM**相关，因为它的主题是改善大语言模型（LLM）的推理性能。

总结：这篇论文关注LLM推理的内存和互连瓶颈。它提出了四种主要的架构创新方向：高带宽闪存提供大容量高带宽、近内存处理、3D内存-逻辑堆叠以提高带宽，以及低延迟互连来加速通信。这些创新主要针对数据中心AI的需求，并探讨了在移动设备上的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）推理面临的挑战，特别是其自回归解码阶段与训练阶段的根本性不同，使得内存和互连成为主要的瓶颈，而不是计算能力。当前的架构难以高效支持大规模LLM的推理需求，因此需要新的硬件和系统架构来解决这些挑战，实现更高效、更大规模的LLM部署，这是论文的核心动机。

Method: 这篇论文采用了对当前大语言模型（LLM）推理架构的挑战进行分析，并提出解决这些挑战的未来研究方向或架构机遇的方法。具体来说，它识别了瓶颈（内存和互连），然后提出了四种主要的架构创新作为解决方案。

Result: 论文提出了四种主要的架构研究机遇，这些机遇被认为能够有效缓解LLM推理中的内存和互连瓶颈：
1. **高带宽闪存（High Bandwidth Flash）：** 提供10倍的内存容量，同时保持接近于HBM的带宽。
2. **近内存处理（Processing-Near-Memory）：** 减少数据移动和提升内存带宽。
3. **3D内存-逻辑堆叠（3D memory-logic stacking）：** 实现更高的内存带宽和集成度。
4. **低延迟互连（low-latency interconnect）：** 加快通信速度。
这些架构创新有望解决数据中心AI中的LLM推理性能问题，并具有向移动设备部署的潜力。

Conclusion: 这篇论文的结论是，解决大语言模型（LLM）推理中的内存和互连瓶颈，需要通过架构创新，例如采用高带宽闪存（High Bandwidth Flash）、近内存处理（Processing-Near-Memory）、3D内存-逻辑堆叠以及低延迟互连。这些创新有望显著提升LLM的推理性能，尤其是在数据中心AI领域，同时也具有向移动设备推广的潜力。

Abstract: Large Language Model (LLM) inference is hard. The autoregressive Decode phase of the underlying Transformer model makes LLM inference fundamentally different from training. Exacerbated by recent AI trends, the primary challenges are memory and interconnect rather than compute. To address these challenges, we highlight four architecture research opportunities: High Bandwidth Flash for 10X memory capacity with HBM-like bandwidth; Processing-Near-Memory and 3D memory-logic stacking for high memory bandwidth; and low-latency interconnect to speedup communication. While our focus is datacenter AI, we also review their applicability for mobile devices.

</details>
